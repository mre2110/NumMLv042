
<!DOCTYPE html>

<html lang="de">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Subgradient Descent &#8212; Numerische Algorithmen für Maschinelles Lernen WS21/22 (Version 0.42)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Stichwortverzeichnis" href="genindex.html" />
    <link rel="search" title="Suche" href="search.html" />
    <link rel="next" title="Proximal Gradient Descent" href="13_Proximal_Gradient_Descent.html" />
    <link rel="prev" title="Projected Gradient-Descent" href="11_Projected_Gradient_Descent.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="de">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Numerische Algorithmen für Maschinelles Lernen WS21/22 (Version 0.42)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Dieses Buch durchsuchen ..." aria-label="Dieses Buch durchsuchen ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="00_Vorwort.html">
   Numerische Algorithmen für Maschinelles Lernen (Version 0.422)
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_Regression.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_Dimensionsreduktion.html">
   Dimensionsreduktion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_Regularisierung.html">
   Regularisierung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_Background_Removal_QR.html">
   Background Removal mit TSVD
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_Klassifikation_mit_SVM.html">
   Support-Vector Klassifikation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_Neuronale_Netze.html">
   Neuronale Netze
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_Topic_Extraction.html">
   Topic Extraction, NMF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_Grundlagen_Optimierung.html">
   Grundlagen der Optimierung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_Konvexitaet.html">
   Konvexität
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_Gradient_Descent.html">
   Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_Projected_Gradient_Descent.html">
   Projected Gradient-Descent
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Subgradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13_Proximal_Gradient_Descent.html">
   Proximal Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14_Stochastic_Gradient_Descent.html">
   Stochastic Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15_Probabilistische_Lineare_Algebra.html">
   Probabilistische Lineare Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="99_Literatur.html">
   Weiterführende Links
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Navigation umschalten" aria-controls="site-navigation"
                title="Navigation umschalten" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Laden Sie diese Seite herunter"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/12_Subgradient_Descent.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Quelldatei herunterladen" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="In PDF drucken"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Vollbildmodus"
        title="Vollbildmodus"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/mre2110/NumMLv042/master?urlpath=tree/12_Subgradient_Descent.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Starten Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Inhalt
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uberblick">
   Überblick
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#grundlagen">
   Grundlagen
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#voruberlegungen">
   Vorüberlegungen
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lipschitz-stetigkeit">
   Lipschitz-Stetigkeit
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#l-glattheit">
   <span class="math notranslate nohighlight">
    \(L\)
   </span>
   -Glattheit
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mu-konvexitat">
   <span class="math notranslate nohighlight">
    \(\mu\)
   </span>
   -Konvexität
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#zusammenfassung">
   Zusammenfassung
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Subgradient Descent</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Inhalt </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uberblick">
   Überblick
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#grundlagen">
   Grundlagen
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#voruberlegungen">
   Vorüberlegungen
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lipschitz-stetigkeit">
   Lipschitz-Stetigkeit
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#l-glattheit">
   <span class="math notranslate nohighlight">
    \(L\)
   </span>
   -Glattheit
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mu-konvexitat">
   <span class="math notranslate nohighlight">
    \(\mu\)
   </span>
   -Konvexität
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#zusammenfassung">
   Zusammenfassung
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="subgradient-descent">
<h1>Subgradient Descent<a class="headerlink" href="#subgradient-descent" title="Link zu dieser Überschrift">¶</a></h1>
<div class="section" id="uberblick">
<h2>Überblick<a class="headerlink" href="#uberblick" title="Link zu dieser Überschrift">¶</a></h2>
<p>Bisher haben wir immer <span class="math notranslate nohighlight">\(f\in C^1(\mathbb{R}^d)\)</span> vorausgesetzt.
In der Praxis trifft das nicht immer zu (Lasso, RELU-MLP).</p>
<p>Ist <span class="math notranslate nohighlight">\(f\)</span> konvex, dann existiert in jedem Punkt der Subgradient.
Wir benutzen jetzt bei Gradient Descent statt des
(eventuell nicht existierenden) Gradienten einen Subgradienten.</p>
<p>Die fehlende Glattheit von <span class="math notranslate nohighlight">\(f\)</span> wird sich negativ auf die Konvergenzgeschwindigkeit auswirken.</p>
</div>
<div class="section" id="grundlagen">
<h2>Grundlagen<a class="headerlink" href="#grundlagen" title="Link zu dieser Überschrift">¶</a></h2>
<p><strong>Definition</strong>:</p>
<ul>
<li><p>ist <span class="math notranslate nohighlight">\(f:\mathbb{R}^d \supset \mathrm{dom}(f) \rightarrow \mathbb{R}\)</span>,
dann ist <span class="math notranslate nohighlight">\(g\in \mathbb{R}^d\)</span> <em>Subgradient</em> von <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(x\in \mathrm{dom}(f)\)</span>, falls</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f(y) \geq f(x) + g^T(y-x) \quad \forall y\in\mathrm{dom}(f)
    \end{equation*}\]</div>
</li>
<li><p>die Menge aller Subgradienten von <span class="math notranslate nohighlight">\(f\)</span> an der Stelle <span class="math notranslate nohighlight">\(x\)</span> bezeichnen wir mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    \partial f(x) = \big\{g\ |\ g\ \text{ist Subgradient von}\ f\ \text{in}\ x\big\}
    \end{equation*}\]</div>
</li>
</ul>
<p><strong>Beispiel:</strong> Ist <span class="math notranslate nohighlight">\(f(x)=|x|\)</span>, dann ist <span class="math notranslate nohighlight">\(\partial f(0) = [-1,1]\)</span>, denn</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
|y| = f(y) \geq f(0) + g(y-0) = gy \quad \forall y\in\mathbb{R}
\end{equation*}\]</div>
<p>genau dann wenn</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
g \in [-1,1].
\end{equation*}\]</div>
<p><strong>Lemma</strong>: Ist <span class="math notranslate nohighlight">\(f\)</span> differenzierbar in <span class="math notranslate nohighlight">\(x\)</span>, dann gilt <span class="math notranslate nohighlight">\(\partial f(x) \subset \{f'(x)\}\)</span>, also
<span class="math notranslate nohighlight">\(\partial f(x) = \{f'(x)\}\)</span> oder <span class="math notranslate nohighlight">\(\partial f(x) = \emptyset\)</span>.</p>
<p>Die Ungleichung <span class="math notranslate nohighlight">\(f(y) \geq f(x) + g^T(y-x)\)</span> sieht aus wie die Konvexitätsbedingung für <span class="math notranslate nohighlight">\(C^1\)</span>-Funktionen,
nur dass <span class="math notranslate nohighlight">\(f'(x)\)</span> durch <span class="math notranslate nohighlight">\(g\)</span> ersetzt wurde.
Dies ist kein Zufall.</p>
<p><strong>Lemma</strong>: <span class="math notranslate nohighlight">\(f:\mathbb{R}^d \supset \mathrm{dom}(f) \rightarrow \mathbb{R}\)</span> ist konvex
genau dann, wenn <span class="math notranslate nohighlight">\(\mathrm{dom}(f)\)</span> konvex ist und</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \partial f(x) \neq \emptyset
  \quad
  \forall x\in\mathrm{dom}(f).
  \end{equation*}\]</div>
<p><strong>Beweis:</strong></p>
<p>„<span class="math notranslate nohighlight">\(\Rightarrow\)</span>“</p>
<ul class="simple">
<li><p>ist <span class="math notranslate nohighlight">\(f\)</span> konvex, so ist <span class="math notranslate nohighlight">\(\mathrm{epi}(f)\)</span> konvex</p></li>
<li><p><span class="math notranslate nohighlight">\(\partial f(x) \neq \emptyset\)</span> folgt aus dem
<a class="reference external" href="https://de.wikipedia.org/wiki/St%C3%BCtzhyperebene">Existenzsatz für Stützhyperebenen</a> für konvexe Mengen angewandt auf <span class="math notranslate nohighlight">\(\mathrm{epi}(f)\)</span></p></li>
</ul>
<p>„<span class="math notranslate nohighlight">\(\Leftarrow\)</span>“</p>
<ul class="simple">
<li><p>nach Voraussetzung ist <span class="math notranslate nohighlight">\(\mathrm{dom}(f)\)</span> konvex
und <span class="math notranslate nohighlight">\(\partial f(x) \neq \emptyset\)</span>, d.h.
es existiert <span class="math notranslate nohighlight">\(g(x) \in \partial f(x)\)</span> mit</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
f(y) \geq f(x) +g(x) (y-x) \quad \forall x,y\in \mathrm{dom}(f)
\end{equation*}\]</div>
<ul class="simple">
<li><p>für <span class="math notranslate nohighlight">\(t\in \left[ 0,1\right]\)</span> setzen wir</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
z= (1-t) x+ty
\end{equation*}\]</div>
<ul class="simple">
<li><p>dann ist <span class="math notranslate nohighlight">\(z\in \mathrm{dom}(f)\)</span> und</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\begin{aligned}
f(x) 
&amp;\geq f(z) + g(z) (x-z) 
\\ 
f(y) 
&amp;\geq f(z) + g(z) (y-z)
\end{aligned}
\end{equation*}\]</div>
<ul class="simple">
<li><p>multiplizieren wir die erste Ungleichung mit <span class="math notranslate nohighlight">\(1-t\)</span>, die zweite mit <span class="math notranslate nohighlight">\(t\)</span> und addieren die Ergebnisse, dann erhalten wir</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
 (1-t) f(x) + t f(y) 
&amp;\geq (1-t)\big(f(z) +g(z)  (x-z) \big) 
\\
&amp; \quad + t \big(f(z) +g(z) (y-z)\big)\\
&amp;= f(z) +g(z) \big( \underbrace{( 1-t) x+ty}_z -z\big)\\
&amp;= f(z) \\
&amp;= f\big(( 1-t) x+ty \big)
\end{align*}\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Existenz von Subgradienten und Konvexität sind also äquivalent.</p>
<p>Einige weitere Eigenschaften des Gradienten lassen sich auch auf Subgradienten übertragen.
Für Lipschitz-Stetigkeit bei <span class="math notranslate nohighlight">\(C^1\)</span>-Funktionen <span class="math notranslate nohighlight">\(f\)</span> hatten wir <span class="math notranslate nohighlight">\(\|f'(x)\|\leq L_f\)</span>.
Dies gilt analog auch für Subgradienten.</p>
<p><strong>Lemma</strong>: <span class="math notranslate nohighlight">\(f:\mathbb{R}^d \supset \mathrm{dom}(f) \rightarrow \mathbb{R}\)</span> konvex, <span class="math notranslate nohighlight">\(\mathrm{dom}(f)\)</span> offen.
Dann ist</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|f(x) - f(y)\| \leq L_f \|x - y\| \quad \forall x,y\in \mathrm{dom}(f)
  \end{equation*}\]</div>
<p>äquivalent zu</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|g\| \leq L_f \quad \forall g \in \partial f(x) \quad \forall x\in \mathrm{dom}(f).
  \end{equation*}\]</div>
<p><strong>Beweis</strong>:</p>
<p>„<span class="math notranslate nohighlight">\(\Rightarrow\)</span>“</p>
<ul class="simple">
<li><p>für <span class="math notranslate nohighlight">\(x \in \mathrm{dom}(f)\)</span>, <span class="math notranslate nohighlight">\(g \in \partial f(x)\)</span>, <span class="math notranslate nohighlight">\(\varepsilon&gt;0\)</span> definieren wir</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    y = 
    \begin{cases}
    x + \varepsilon \frac{g}{\|g\|} &amp; g \neq 0\\
    x &amp; g=0
    \end{cases}
    \end{equation*}\]</div>
<ul>
<li><p>damit folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    y -x = 
    \begin{cases}
    \varepsilon \frac{g}{\|g\|} &amp; g \neq 0\\
    0 &amp; g=0
    \end{cases}
    \end{equation*}\]</div>
<p>bzw.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    \|y-x\|
    = 
    \begin{cases}
    \varepsilon &amp; g \neq 0\\
    0 &amp; g=0 
    \end{cases}
    \leq \varepsilon
    \end{equation*}\]</div>
</li>
<li><p>wegen <span class="math notranslate nohighlight">\(\mathrm{dom}(f)\)</span> offen ist für <span class="math notranslate nohighlight">\(\varepsilon&gt;0\)</span> hinreichend klein
auch <span class="math notranslate nohighlight">\(y\in\mathrm{dom}(f)\)</span></p></li>
<li><p>außerdem gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    g^T(y-x)
    = 
    \begin{cases}
    \varepsilon\frac{g^T g}{\|g\|} &amp; g \neq 0\\
    0 &amp; g=0 
    \end{cases}
    = \varepsilon\|g\|
    \end{equation*}\]</div>
</li>
<li><p>wegen <span class="math notranslate nohighlight">\(g \in \partial f(x)\)</span> ist dann</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f(y) \geq f(x) + g^T(y-x) = f(x) + \varepsilon\|g\| 
    \end{equation*}\]</div>
</li>
<li><p>da <span class="math notranslate nohighlight">\(f\)</span> Lipschitz-stetig ist folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    L_f \varepsilon \geq L_f \|y-x\| \geq f(y) - f(x) \geq \varepsilon\|g\|, 
    \end{equation*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    \|g\| \leq L_f
    \end{equation*}\]</div>
</li>
</ul>
<p>„<span class="math notranslate nohighlight">\(\Leftarrow\)</span>“</p>
<ul>
<li><p>für <span class="math notranslate nohighlight">\(x,y \in \mathrm{dom}(f)\)</span> und <span class="math notranslate nohighlight">\(g \in \partial f(x)\)</span> gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f(y) \geq f(x) + g^T(y-x)
    \end{equation*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f(x) - f(y) \leq  g^T(x-y)
    \end{equation*}\]</div>
</li>
<li><p>mit Cauchy-Schwartz und <span class="math notranslate nohighlight">\(\|g\| \leq L_f\)</span> folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f(x) - f(y) \leq  \|g\| \|x-y\| \leq L_f \|x -y\| 
    \end{equation*}\]</div>
</li>
<li><p>Vertauschen von <span class="math notranslate nohighlight">\(x\)</span> und <span class="math notranslate nohighlight">\(y\)</span> liefert schließlich</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    |f(x) - f(y)| \leq L_f \|x -y\| \quad \forall x,y \in \mathrm{dom}(f)
    \end{equation*}\]</div>
</li>
</ul>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>Satz</strong>: <span class="math notranslate nohighlight">\(f:\mathbb{R}^d \supset \mathrm{dom}(f) \rightarrow \mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(x \in \mathrm{dom}(f)\)</span>, dann gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  0 \in \partial f(x) 
  \quad\Leftrightarrow\quad
  x\ \text{ist globales Minimum von}\ f.
  \end{equation*}\]</div>
<p><strong>Beweis</strong>:</p>
<p>„<span class="math notranslate nohighlight">\(\Rightarrow\)</span>“</p>
<ul>
<li><p>ist <span class="math notranslate nohighlight">\(0 \in \partial f(x)\)</span>, dann folgt aus der Definition des Subgradienten</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f(y) \geq f(x) + 0^T(y-x) =  f(x) \quad \forall y\in\mathrm{dom}(f)
    \end{equation*}\]</div>
</li>
</ul>
<p>„<span class="math notranslate nohighlight">\(\Leftarrow\)</span>“</p>
<ul>
<li><p>ist <span class="math notranslate nohighlight">\(x \in\mathrm{dom}(f)\)</span> globales Minimum, dann gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f(y) \geq f(x) = f(x) + 0^T(y-x)  \quad \forall y\in\mathrm{dom}(f)
    \end{equation*}\]</div>
</li>
</ul>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>Bemerkung:</strong></p>
<ul class="simple">
<li><p>die Bedingung <span class="math notranslate nohighlight">\(0 \in \partial f(x)\)</span> ist stärker als <span class="math notranslate nohighlight">\(f'(x)=0\)</span> bei <span class="math notranslate nohighlight">\(C^1\)</span>-Funktionen</p></li>
<li><p><span class="math notranslate nohighlight">\(f'(x)=0\)</span> ist nur eine <em>notwendige Bedingung</em> für <em>lokale</em> Minima, d.h.
aus <span class="math notranslate nohighlight">\(f'(x)=0\)</span> alleine kann man i.A. nicht folgern, dass <span class="math notranslate nohighlight">\(x\)</span> ein lokales
oder gar globales Minimum ist</p></li>
<li><p>dagegen garantiert <span class="math notranslate nohighlight">\(0 \in \partial f(x)\)</span> immer, dass <span class="math notranslate nohighlight">\(x\)</span> globales Minimum ist</p></li>
</ul>
</div>
<div class="section" id="voruberlegungen">
<h2>Vorüberlegungen<a class="headerlink" href="#voruberlegungen" title="Link zu dieser Überschrift">¶</a></h2>
<p>Wir betrachten nun <em>Subgradient-Descent</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  x_{t+1} = x_t - \gamma_t g_t, \quad g_t \in \partial f(x_t).
  \end{equation*}\]</div>
<p>Dabei ist <span class="math notranslate nohighlight">\(g_t\)</span> <em>irgendein</em> Element aus <span class="math notranslate nohighlight">\(\partial f(x_t)\)</span>.</p>
<p>Ist <span class="math notranslate nohighlight">\(f\)</span> konvex, dann ist <span class="math notranslate nohighlight">\(\partial f(x) \neq \emptyset\)</span> <span class="math notranslate nohighlight">\(\forall x\)</span>,
so dass immer mindestens ein <span class="math notranslate nohighlight">\(g_t\)</span> existiert.</p>
<p>Ist <span class="math notranslate nohighlight">\(f\)</span> konvex und in <span class="math notranslate nohighlight">\(x_t\)</span> differenzierbar, dann gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \partial f(x_t) = \{ f'(x_t)\}
  \end{equation*}\]</div>
<p>und somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  g_t = f'(x_t).
  \end{equation*}\]</div>
<p>Wir lassen auch variable Schrittweite <span class="math notranslate nohighlight">\(\gamma_t\)</span> zu.</p>
<p>Unter diesen Voraussetzungen wollen wir analog zu Gradient-Descent die Konvergenzeigenschaften untersuchen.
Viele Ergebnisse lassen sich direkt übertragen, indem man einfach <span class="math notranslate nohighlight">\(f'_t\)</span> durch <span class="math notranslate nohighlight">\(g_t\)</span> ersetzt.</p>
<p>Für Schrittweite <span class="math notranslate nohighlight">\(\gamma\)</span> erhalten wir damit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  g_{t}^T\left(x_{t}-x_{*}\right) 
  =\frac{1}{2 \gamma}\big(\gamma^{2}\|g_{t}\|_{2}^{2}+\|x_{t}-x_{*}\|_{2}^{2}-\|x_{t+1}-x_{*}\|_{2}^{2}\big).
  \end{equation*}\]</div>
<p>Da <span class="math notranslate nohighlight">\(g_t\)</span> Subgradient in <span class="math notranslate nohighlight">\(x_t\)</span> ist, gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_\ast \geq f_t + g_t^T (x_\ast - x_t)
  \end{equation*}\]</div>
<p>und somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
  f_t - f_\ast 
  &amp;\leq g_t^T (x_t - x_\ast)
  \\
  &amp;=\frac{1}{2 \gamma}\big(\gamma^{2}\|g_{t}\|_{2}^{2}+\|x_{t}-x_{*}\|_{2}^{2}-\|x_{t+1}-x_{*}\|_{2}^{2}\big).
  \end{align*}\]</div>
</div>
<div class="section" id="lipschitz-stetigkeit">
<h2>Lipschitz-Stetigkeit<a class="headerlink" href="#lipschitz-stetigkeit" title="Link zu dieser Überschrift">¶</a></h2>
<p>Lipschitz-Stetigkeit von <span class="math notranslate nohighlight">\(f\)</span>, d.h.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|f(y) - f(x)\| \leq L_f \|y - x\| \quad \forall x,y\in\mathbb{R}^d
  \end{equation*}\]</div>
<p>ist äquivalent zu</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|g\| \leq L_f  \quad \forall g\in \partial f(x) \quad \forall x\in\mathbb{R}^d.
  \end{equation*}\]</div>
<p>Damit erhalten wir aus der letzten Ungleichung im vorherigen Abschnitt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_t - f_\ast 
  \leq \frac{1}{2 \gamma}\big(\gamma^{2}L_f^{2}+\|x_{t}-x_{*}\|_{2}^{2}-\|x_{t+1}-x_{*}\|_{2}^{2}\big)
  \end{equation*}\]</div>
<p>und somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
  \sum_{t=0}^{T-1} (f_t - f_\ast)
  &amp; \leq 
  \frac{\gamma}{2}T L_f^2 
  + \frac{1}{2\gamma} 
  \big( 
  \underbrace{\|x_{0}-x_{*}\|_{2}^{2}}_{e_0^2}
  -
  \underbrace{\|x_{T}-x_{*}\|_{2}^{2}}_{\geq 0}
  \big)
  \\
  &amp; \leq \frac{\gamma T L_f^2}{2} + \frac{e_0^2}{2\gamma},
  \end{align*}\]</div>
<p>was identisch ist mit der Abschätzung bei Gradient-Descent.</p>
<p>Somit erhalten wir in diesem Fall auch die selbe Konvergenzaussage
wie bei Gradient-Descent.</p>
<p><strong>Satz:</strong> <span class="math notranslate nohighlight">\(f:\mathbb{R}^d\to \mathbb{R}\)</span>, konvex, L-stetig mit Konstante <span class="math notranslate nohighlight">\(L_f\)</span>
und es existiere <span class="math notranslate nohighlight">\(x_\ast = \mathrm{argmin}_{x\in\mathbb{R}^d}f(x)\)</span>.
Mit <span class="math notranslate nohighlight">\(\gamma = \frac{c}{T^\omega}\)</span>, <span class="math notranslate nohighlight">\(\omega\in(0,1)\)</span>, gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
  \min_{t=0,\ldots,T-1}(f_t - f_\ast)
  &amp;\leq \frac{1}{T} \sum_{t=0}^{T-1} (f_t - f_\ast)\\
  &amp;= \mathcal{O}\Big(\big(\frac{1}{T}\big)^{\min(\omega,1-\omega)}\Big)
  \quad 
  \text{für}
  \quad
  T\to\infty.
  \end{align*}\]</div>
<p>Die optimale Ordnung ist <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> für <span class="math notranslate nohighlight">\(\omega=\frac{1}{2}\)</span>.
Mit <span class="math notranslate nohighlight">\(e_0 = \|x_0 - x_\ast\|\)</span>, <span class="math notranslate nohighlight">\(\gamma = \frac{e_0}{L_f\sqrt{T}}\)</span> gilt außerdem</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \min_{t=0,\ldots,T-1}(f_t - f_\ast)
  \leq \frac{1}{T} \sum_{t=0}^{T-1} (f_t - f_\ast)
  \leq \frac{L_f e_0}{\sqrt{T}}.
  \end{equation*}\]</div>
</div>
<div class="section" id="l-glattheit">
<h2><span class="math notranslate nohighlight">\(L\)</span>-Glattheit<a class="headerlink" href="#l-glattheit" title="Link zu dieser Überschrift">¶</a></h2>
<p>Bei differenzierbarem <span class="math notranslate nohighlight">\(f\)</span> hatten wir unter zusätzlichen   Voraussetzungen (<span class="math notranslate nohighlight">\(L\)</span>-Glattheit, <span class="math notranslate nohighlight">\(\mu\)</span>-Konvexität) höhere Konvergenzraten nachweisen können.</p>
<p>Bei <span class="math notranslate nohighlight">\(L\)</span>-Glattheit stoßen wir hier auf ein Problem.
Wie das folgende Lemma zeigt, sind <span class="math notranslate nohighlight">\(L\)</span>-glatte Funktionen mit existierenden Subgradienten automatisch differenzierbar.</p>
<p><strong>Lemma:</strong>
<span class="math notranslate nohighlight">\(f:\mathbb{R}^d \supset \mathrm{dom}(f) \to \mathbb{R}\)</span>,
<span class="math notranslate nohighlight">\(\mathrm{dom}(f)\)</span> offen und in <span class="math notranslate nohighlight">\(x\in \mathrm{dom}(f)\)</span>
gelte <span class="math notranslate nohighlight">\(\partial f(x)\neq\emptyset\)</span>.
Gibt es ein <span class="math notranslate nohighlight">\(L\ge 0\)</span> so dass für <span class="math notranslate nohighlight">\(g_x \in \partial f(x)\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f(y) \leq f(x) + g_x^T (y-x) + \frac{L}{2}\|y-x\|_2^2
  \quad \forall y\in\mathrm{dom}(f)
  \end{equation*}\]</div>
<p>gilt, dann ist <span class="math notranslate nohighlight">\(f\)</span> differenzierbar in <span class="math notranslate nohighlight">\(x\)</span> und <span class="math notranslate nohighlight">\(f'(x)=g_x\)</span>.</p>
<p><strong>Beweis:</strong></p>
<ul>
<li><p>wegen <span class="math notranslate nohighlight">\(g_x \in \partial f(x)\)</span> gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f(y) \geq f(x) + g_x^T(y-x) \quad \forall y\in\mathrm{dom}(f)
    \end{equation*}\]</div>
<p>und es folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    0 \leq f(y) - \big(f(x) + g_x^T (y-x)\big) \leq \frac{L}{2} \|y-x\|_2^2
    \end{equation*}\]</div>
</li>
<li><p>also ist für <span class="math notranslate nohighlight">\(y \to x\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    \big| f(y) - \big(f(x) + g_x^T (y-x)\big) \big|= {\scriptstyle \mathcal{O}}(\|y-x\|)
    \end{equation*}\]</div>
<p>und somit <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(x\)</span> differenzierbar mit Ableitung <span class="math notranslate nohighlight">\(f'(x)=g_x\)</span></p>
</li>
</ul>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Verlangen wir also <span class="math notranslate nohighlight">\(L\)</span>-Glattheit für subdifferenzierbare
Funktionen (z.B. <span class="math notranslate nohighlight">\(f\)</span> konvex), so landen wir wieder beim differenzierbarem Fall.</p>
</div>
<div class="section" id="mu-konvexitat">
<h2><span class="math notranslate nohighlight">\(\mu\)</span>-Konvexität<a class="headerlink" href="#mu-konvexitat" title="Link zu dieser Überschrift">¶</a></h2>
<p><span class="math notranslate nohighlight">\(\mu\)</span>-Konvexität lässt sich einfach auf den subdifferenzierbare Funktionen verallgemeinern.</p>
<p><strong>Definition:</strong>
<span class="math notranslate nohighlight">\(f:\mathbb{R}^d \supset \mathrm{dom}(f) \to \mathbb{R}\)</span>
heißt <span class="math notranslate nohighlight">\(\mu\)</span>-konvex, falls</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f(y) \geq f(x) + g^T (y-x) + \frac{\mu}{2}\|y-x\|_2^2
  \quad \forall g \in \partial f(x)
  \quad \forall x,y\in\mathrm{dom}(f).
  \end{equation*}\]</div>
<p>Wie im letzten Kapitel erklärt, müssen wir bei nicht differenzierbarem <span class="math notranslate nohighlight">\(f\)</span> auf <span class="math notranslate nohighlight">\(L\)</span>-Glattheit verzichten, was einige Schwierigkeiten verursachen wird.</p>
<p>Die Probleme die sich dabei ergeben werden wir am folgenden Beispiel näher untersuchen.</p>
<p><strong>Beispiel:</strong></p>
<ul>
<li><p>für die Funktion <span class="math notranslate nohighlight">\(f:\mathbb{R}\to\mathbb{R}\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f(x) 
  = e^{|x|}
  = \begin{cases}
    e^{-x} &amp; x&lt;0\\
    e^{x} &amp; 0 \leq x
    \end{cases}
  \end{equation*}\]</div>
<p>erhalten wir als Subgradient</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \partial f(x) 
  = \begin{cases}
    \mathrm{sign}(x)\ e^{|x|} &amp; x \neq 0\\
    [-1,1] &amp; x = 0
    \end{cases}
  \end{equation*}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(f\)</span> ist <span class="math notranslate nohighlight">\(\mu\)</span>-konvex mit <span class="math notranslate nohighlight">\(\mu=1\)</span> (Übung)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu\)</span>-Konvexität liefert nur eine Schranke nach unten, d.h. <span class="math notranslate nohighlight">\(f\)</span>
kann sehr stark wachsen und somit kann <span class="math notranslate nohighlight">\(\partial f\)</span> sehr große
Werte annehmen</p></li>
<li><p>ist dies der Fall, dann können bei Subgradient-Descent
„Overshoots“ auftreten</p></li>
<li><p>um dies zu kompensieren und Konvergenz zu sichern, muss
die Schrittweite <span class="math notranslate nohighlight">\(\gamma\)</span> sehr klein gewählt werden</p></li>
<li><p>für „brave“ Funktionen <span class="math notranslate nohighlight">\(f\)</span> verursacht das aber unnötigen Aufwand</p></li>
<li><p>als Konsequenz muss die Schrittweite entsprechend an <span class="math notranslate nohighlight">\(f\)</span> angepasst
werden</p></li>
</ul>
<p>Beim einfachen Gradient-Descent hat die <span class="math notranslate nohighlight">\(L\)</span>-Glattheit (als Beschränkung nach oben) dieses Problem beseitigt.
Da  <span class="math notranslate nohighlight">\(L\)</span>-Glattheit hier nicht zur Verfügung steht, werden wir
unter der zusätzlichen Annahme</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|g_t\| \leq B \quad \forall t
  \end{equation*}\]</div>
<p>und <span class="math notranslate nohighlight">\(t\)</span>-abhängiger Schrittweite <span class="math notranslate nohighlight">\(\gamma_t\)</span>
das folgende Konvergenz-Resultat für Subgradient-Descent
beweisen.</p>
<p><strong>Satz:</strong> <span class="math notranslate nohighlight">\(f\)</span> sei <span class="math notranslate nohighlight">\(\mu\)</span>-konvex und es existiere <span class="math notranslate nohighlight">\(x_\ast\)</span>.
Für</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \gamma_t = \frac{2}{\mu(t+1)}
  \end{equation*}\]</div>
<p>erhalten wir für Subgradient-Descent</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f\Big(
  \underbrace{\frac{2}{T(T+1)} \sum_{t=1}^T t x_t}
   _{\text{Konvexkombination der }x_t}
  \Big) - f_\ast
  \leq
  \frac{2 B^2}{\mu(T+1)}
  \end{equation*}\]</div>
<p>mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  B = \max_{t=1,\ldots,T}\|g_t\|_2.
  \end{equation*}\]</div>
<p><strong>Beweis:</strong></p>
<ul>
<li><p>aus den Vorüberlegungen wissen wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    g_{t}^T(x_{t}-x_{*}) 
    =\frac{1}{2 \gamma_t}
    \big(
    \gamma_t^{2}\|g_{t}\|_{2}^{2}
    +\|x_{t}- x_{*}\|_{2}^{2}
    -\|x_{t+1}-x_{*}\|_{2}^{2}
    \big)
    \quad 
    \end{equation*}\]</div>
</li>
<li><p>aus der <span class="math notranslate nohighlight">\(\mu\)</span>-Konvexität von <span class="math notranslate nohighlight">\(f\)</span> folgt mit <span class="math notranslate nohighlight">\(y=x_\ast\)</span>, <span class="math notranslate nohighlight">\(x=x_t\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f_\ast 
    \geq 
    f_t + g_t^T (x_\ast-x_t) + \frac{\mu}{2}\|x_\ast-x_t\|_2^2
    \end{equation*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f_t - f_\ast  
    \leq 
    g_t^T (x_t-x_\ast)
    - \frac{\mu}{2}\|x_t-x_\ast\|_2^2
    \end{equation*}\]</div>
<p>und somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    f_t - f_\ast
    &amp;\leq
     \frac{1}{2 \gamma_t}
     \big(
     \gamma_t^{2}\|g_{t}\|_{2}^{2}
     +\|x_{t}- x_{*}\|_{2}^{2}
     -\|x_{t+1}-x_{*}\|_{2}^{2}
     \big)
     - \frac{\mu}{2}\|x_t-x_\ast\|_2^2 \\
    &amp;\leq
     \frac{B^2}{2}\gamma_t
     + \frac{1}{2}
       \big(\frac{1}{\gamma_t} - \mu\big) \|x_{t}- x_{*}\|_{2}^{2}
     - \frac{1}{2 \gamma_t} \|x_{t+1}-x_{*}\|_{2}^{2}
    \end{align*}\]</div>
</li>
<li><p>multiplizieren wir mit <span class="math notranslate nohighlight">\(t\)</span> und setzen wir wieder
<span class="math notranslate nohighlight">\(e_t = \|x_{t}- x_{*}\|_{2}\)</span>, so erhalten wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    t(f_t - f_\ast)
    \leq
    \frac{t}{2} 
    \Big(
    B^2 \gamma_t
    + \underbrace{\big(\frac{1}{\gamma_t} - \mu\big)}_{\alpha_t} \ e_t^2
    - \underbrace{\frac{1}{\gamma_t}}_{\beta_t} \ e_{t+1}^2
    \Big)
    \end{equation*}\]</div>
</li>
<li><p>Summation über <span class="math notranslate nohighlight">\(t\)</span> liefert</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \sum_{t=1}^T t(f_t - f_\ast)
    &amp; = 
    \sum_{t=0}^T t(f_t - f_\ast)\\
    &amp; \leq
    \sum_{t=0}^T
    \frac{t}{2} 
    \Big(B^2 \gamma_t + \alpha_t e_t^2 - \beta_t e_{t+1}^2\Big)\\
    &amp; = 
    \frac{B^2}{2} \sum_{t=1}^T t \gamma_t
    + \frac{1}{2}
    \Big(
    \sum_{t=0}^T t \alpha_t e_t^2  - \sum_{t=0}^T t \beta_t e_{t+1}^2
    \Big)\\
    &amp; = 
    \frac{B^2}{2} \sum_{t=1}^T t \gamma_t
    + \frac{1}{2}
    \Big(
      \sum_{t=0}^{T-1} (t+1) \alpha_{t+1} e_{t+1}^2  
    - \sum_{t=0}^T t \beta_t e_{t+1}^2
    \Big)\\
    &amp; = 
    \frac{B^2}{2} \sum_{t=1}^T t \gamma_t
    + \frac{1}{2}
    \Big(
      \sum_{t=0}^{T-1} \big((t+1) \alpha_{t+1} - t \beta_t\big) e_{t+1}^2  
    - \beta_T e_{T+1}^2   
    \Big)\\
    &amp; \leq 
    \frac{B^2}{2} \sum_{t=1}^T t \gamma_t
    + \frac{1}{2}
    \Big(
      \sum_{t=0}^{T-1} \big((t+1) \alpha_{t+1} - t \beta_t\big) e_{t+1}^2  
    \Big)
    \end{align*}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(\alpha_t\)</span> und <span class="math notranslate nohighlight">\(\beta_t\)</span> hängen nur von <span class="math notranslate nohighlight">\(\gamma_t\)</span> und <span class="math notranslate nohighlight">\(\mu\)</span> ab</p></li>
<li><p>kann man <span class="math notranslate nohighlight">\(\gamma_t\)</span> so wählen, dass</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  (t+1) \alpha_{t+1} - t \beta_t \leq 0
  \quad \forall t \geq 0
  \end{equation*}\]</div>
<p>gilt, dann kann der zweite Summand auf der rechten Seite mit <span class="math notranslate nohighlight">\(0\)</span>
nach oben abgeschätzt werden</p>
</li>
<li><p>für</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \gamma_t = \frac{2}{\mu(t+1)}
  \end{equation*}\]</div>
<p>ist</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
  (t+1) \alpha_{t+1} - t \beta_t 
  &amp;= (t+1) \big(\frac{1}{\gamma_{t+1}} - \mu\big) - t \frac{1}
     {\gamma_t}\\
  &amp;= (t+1) \big(\frac{\mu(t+2)}{2} - \mu\big) - t \frac{\mu(t+1)}{2}\\
  &amp;= \frac{\mu}{2} (t+1)t - t\frac{\mu}{2} (t+1)\\
  &amp;= 0
  \end{align*}\]</div>
<p>und damit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \sum_{t=1}^T t(f_t - f_\ast) 
  \leq 
  \frac{B^2}{2} \sum_{t=1}^T t \gamma_t
  \end{equation*}\]</div>
</li>
<li><p>wegen</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \frac{2}{T(T+1)}\sum_{t=1}^T t = 1,
  \end{equation*}\]</div>
<p>der Konvexität von <span class="math notranslate nohighlight">\(f\)</span> und der Jensen-Ungleichung
erhalten wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
  f\Big(\frac{2}{T(T+1)}\sum_{t=1}^T t x_t \Big) - f_\ast
  &amp;\leq \Big(\frac{2}{T(T+1)}\sum_{t=1}^T t f_t \Big) - f_\ast\\
  &amp;= \frac{2}{T(T+1)}\sum_{t=1}^T t (f_t - f_\ast)\\
  &amp;\leq \frac{B^2}{T(T+1)} \sum_{t=1}^T t \gamma_t
  \end{align*}\]</div>
</li>
<li><p>mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \gamma_t = \frac{2}{\mu(t+1)}
  \end{equation*}\]</div>
<p>folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \sum_{t=1}^T t \gamma_t
  = \frac{2}{\mu} \sum_{t=1}^T \frac{t}{t+1} \leq \frac{2}{\mu} T
  \end{equation*}\]</div>
<p>und somit schließlich</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f\Big(\frac{2}{T(T+1)}\sum_{t=1}^T t x_t \Big) - f_\ast
  \leq \frac{2B^2}{\mu(T+1)}
  \end{equation*}\]</div>
</li>
</ul>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>Bemerkung:</strong></p>
<ul class="simple">
<li><p>in der oberen Schranke steckt <span class="math notranslate nohighlight">\(x_0\)</span> nicht explizit drin, geht aber
implizit über <span class="math notranslate nohighlight">\(B\)</span> ein (<span class="math notranslate nohighlight">\(\|g_t\|_2 \leq B\)</span> <span class="math notranslate nohighlight">\(\forall t\)</span>)</p></li>
<li><p>für <span class="math notranslate nohighlight">\(f\)</span> differenzierbar, <span class="math notranslate nohighlight">\(\mu\)</span>-konvex und <span class="math notranslate nohighlight">\(L\)</span>-glatt hatten wir bei
Gradient-Descent die Komplexität <span class="math notranslate nohighlight">\(\mathcal{O}\big(\log(\frac{1}{\varepsilon})\big)\)</span></p></li>
<li><p>für <span class="math notranslate nohighlight">\(f\)</span> <span class="math notranslate nohighlight">\(\mu\)</span>-konvex, <span class="math notranslate nohighlight">\(\|g_t\|_2 \leq B\)</span> <span class="math notranslate nohighlight">\(\forall t\)</span> bekommen wir
bei Subgradient-Descent nur die Komplexität <span class="math notranslate nohighlight">\(\mathcal{O}\big(\frac{1}{\varepsilon}\big)\)</span>,
d.h. aus fehlender Glattheit von <span class="math notranslate nohighlight">\(f\)</span> kann wieder langsamere Konvergenz folgen</p></li>
</ul>
</div>
<div class="section" id="zusammenfassung">
<h2>Zusammenfassung<a class="headerlink" href="#zusammenfassung" title="Link zu dieser Überschrift">¶</a></h2>
<p>Ist <span class="math notranslate nohighlight">\(f\)</span> konvex und nicht differenzierbar, dann benutzt man bei Gradient-Descent statt des
(eventuell nicht  existierenden) Gradienten einen Subgradienten.</p>
<p>Für das <em>nicht restringierte Optimierungsproblem</em> haben wir folgendes
Konvergenzverhalten nachgewiesen:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(f\)</span> konvex und Lipschitz-stetig, <span class="math notranslate nohighlight">\(\gamma = \frac{c}{\sqrt{T}}\)</span>, <span class="math notranslate nohighlight">\(c&gt;0\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    \min_{t=0,\ldots,T-1}(f_t - f_\ast)
    \leq \frac{L_f e_0}{\sqrt{T}}
    \end{equation*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    \min_{t=0,\ldots,T-1}(f_t - f_\ast) \leq \varepsilon
    \quad \Rightarrow\quad
    T = \mathcal{O}\big(\frac{1}{\varepsilon^2}\big)
    \end{equation*}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(f\)</span> <span class="math notranslate nohighlight">\(\mu\)</span>-konvex mit <span class="math notranslate nohighlight">\(\mu&gt;0\)</span> und <span class="math notranslate nohighlight">\(\|g_t\|\leq B\)</span>,
<span class="math notranslate nohighlight">\(\gamma_t = \frac{2}{\mu(t+1)}\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f\Big(\frac{2}{T(T+1)} \sum_{t=1}^T t x_t\Big) - f_\ast
    \leq
    \frac{2 B^2}{\mu(T+1)},
    \end{equation*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f\Big(\frac{2}{T(T+1)} \sum_{t=1}^T t x_t\Big) - f_\ast
    \leq \varepsilon
    \quad \Rightarrow\quad
    T = \mathcal{O}\Big( \frac{1}{\varepsilon} \Big)
    \end{equation*}\]</div>
</li>
</ul>
<p>Im ersten Fall benötigen wir für Genauigkeit <span class="math notranslate nohighlight">\(\varepsilon\)</span>
den selben asymptotischen Aufwand wie bei
differenzierbarem <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>Im zweiten Fall steigt der
Aufwand von <span class="math notranslate nohighlight">\(\mathcal{O}\Big( \log\big(\frac{1}{\varepsilon}\big) \Big)\)</span>
auf <span class="math notranslate nohighlight">\(\mathcal{O}\Big( \frac{1}{\varepsilon} \Big)\)</span>, d.h. durch die
reduzierten Glattheitsanforderungen an <span class="math notranslate nohighlight">\(f\)</span> reduziert sich hier
die Konvergenzgeschwindigkeit.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="11_Projected_Gradient_Descent.html" title="zurück Seite">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">zurück</p>
            <p class="prev-next-title">Projected Gradient-Descent</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="13_Proximal_Gradient_Descent.html" title="weiter Seite">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">weiter</p>
        <p class="prev-next-title">Proximal Gradient Descent</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      Durch Martin Reißel<br/>
    
        &copy; Urheberrechte © 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>