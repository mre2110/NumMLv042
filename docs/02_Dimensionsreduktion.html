
<!DOCTYPE html>

<html lang="de">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Dimensionsreduktion &#8212; Numerische Algorithmen für Maschinelles Lernen WS21/22 (Version 0.42)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Stichwortverzeichnis" href="genindex.html" />
    <link rel="search" title="Suche" href="search.html" />
    <link rel="next" title="Regularisierung" href="03_Regularisierung.html" />
    <link rel="prev" title="Regression" href="01_Regression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="de">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Numerische Algorithmen für Maschinelles Lernen WS21/22 (Version 0.42)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Dieses Buch durchsuchen ..." aria-label="Dieses Buch durchsuchen ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="00_Vorwort.html">
   Numerische Algorithmen für Maschinelles Lernen (Version 0.422)
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_Regression.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Dimensionsreduktion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_Regularisierung.html">
   Regularisierung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_Background_Removal_QR.html">
   Background Removal mit TSVD
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_Klassifikation_mit_SVM.html">
   Support-Vector Klassifikation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_Neuronale_Netze.html">
   Neuronale Netze
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_Topic_Extraction.html">
   Topic Extraction, NMF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_Grundlagen_Optimierung.html">
   Grundlagen der Optimierung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_Konvexitaet.html">
   Konvexität
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_Gradient_Descent.html">
   Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_Projected_Gradient_Descent.html">
   Projected Gradient-Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12_Subgradient_Descent.html">
   Subgradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13_Proximal_Gradient_Descent.html">
   Proximal Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14_Stochastic_Gradient_Descent.html">
   Stochastic Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15_Probabilistische_Lineare_Algebra.html">
   Probabilistische Lineare Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="99_Literatur.html">
   Weiterführende Links
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Navigation umschalten" aria-controls="site-navigation"
                title="Navigation umschalten" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Laden Sie diese Seite herunter"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/02_Dimensionsreduktion.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Quelldatei herunterladen" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="In PDF drucken"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Vollbildmodus"
        title="Vollbildmodus"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/mre2110/NumMLv042/master?urlpath=tree/02_Dimensionsreduktion.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Starten Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Inhalt
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uberblick">
   Überblick
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modellvereinfachung">
   Modellvereinfachung
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regressionsproblem">
     Regressionsproblem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gd">
     GD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#newton">
     Newton
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normalgleichungen">
     Normalgleichungen
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pseudoinverse-und-singularwertzerlegung-svd">
     Pseudoinverse und Singulärwertzerlegung (SVD)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#abgeschnittene-singularwertzerlegung-tsvd">
     Abgeschnittene Singulärwertzerlegung (TSVD)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dimensionsreduktion-uber-principle-component-analysis-pca">
   Dimensionsreduktion über Principle Component Analysis (PCA)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#motivation">
     Motivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mathematische-formulierung">
     Mathematische Formulierung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#beispiel">
     Beispiel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pca-bei-klassifikation-von-ziffern">
     PCA bei Klassifikation von Ziffern
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#klassifikation-mit-k-means-clustering-ohne-pca">
     Klassifikation mit k-Means-Clustering ohne PCA
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#klassifikation-mit-pca-und-k-means-clustering">
       Klassifikation mit PCA und k-Means-Clustering
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#zusammenfassung">
   Zusammenfassung
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Dimensionsreduktion</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Inhalt </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uberblick">
   Überblick
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modellvereinfachung">
   Modellvereinfachung
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regressionsproblem">
     Regressionsproblem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gd">
     GD
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#newton">
     Newton
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normalgleichungen">
     Normalgleichungen
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pseudoinverse-und-singularwertzerlegung-svd">
     Pseudoinverse und Singulärwertzerlegung (SVD)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#abgeschnittene-singularwertzerlegung-tsvd">
     Abgeschnittene Singulärwertzerlegung (TSVD)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dimensionsreduktion-uber-principle-component-analysis-pca">
   Dimensionsreduktion über Principle Component Analysis (PCA)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#motivation">
     Motivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mathematische-formulierung">
     Mathematische Formulierung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#beispiel">
     Beispiel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pca-bei-klassifikation-von-ziffern">
     PCA bei Klassifikation von Ziffern
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#klassifikation-mit-k-means-clustering-ohne-pca">
     Klassifikation mit k-Means-Clustering ohne PCA
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#klassifikation-mit-pca-und-k-means-clustering">
       Klassifikation mit PCA und k-Means-Clustering
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#zusammenfassung">
   Zusammenfassung
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="dimensionsreduktion">
<h1>Dimensionsreduktion<a class="headerlink" href="#dimensionsreduktion" title="Link zu dieser Überschrift">¶</a></h1>
<div class="section" id="uberblick">
<h2>Überblick<a class="headerlink" href="#uberblick" title="Link zu dieser Überschrift">¶</a></h2>
<p>Wir betrachten Methoden, um lineare Modelle bei Regressionsproblemen zu vereinfachen
bzw. bei hochdimensionalen Datensätzen Dimensionsreduktionen durchzuführen.
Das wesentliche Werkzeug dazu ist die <strong>Singulärwertzerlegung</strong></p>
<p>Die Singulärwertzerlegung kommt ursprünglich aus der linearen Algebra.
Dort zeigt man, dass jede Matrix <span class="math notranslate nohighlight">\(A\in\mathbb{R}^{m\times n}\)</span> sich in</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
A = U\Sigma V^T
\end{equation*}\]</div>
<p>mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\quad
U\in\mathbb{R}^{m\times m}, 
\quad
V\in\mathbb{R}^{n\times n},  
\quad
\Sigma = \text{diag}\big(\sigma_1, \ldots, \sigma_r, 0, \ldots,0 \big)\in\mathbb{R}^{m\times n}
\end{equation*}\]</div>
<p>und
<span class="math notranslate nohighlight">\(\sigma_1 \ge \ldots \ge \sigma_r &gt; 0\)</span>,
<span class="math notranslate nohighlight">\(r \le \min(m,n)\)</span>,
zerlegen lässt, wobei <span class="math notranslate nohighlight">\(U,V\)</span> orthonormal sind.</p>
<p>Geometrisch bedeutet das, dass man immer orthonormale Basen
<span class="math notranslate nohighlight">\(V\)</span> im Urbild-Raum und <span class="math notranslate nohighlight">\(U\)</span> im Bildraum finden kann, so dass sich die zu <span class="math notranslate nohighlight">\(A\)</span> gehörige
lineare Abbildung bezüglich dieser
Basen als Diagonalmatrix <span class="math notranslate nohighlight">\(\Sigma\)</span> darstellen lässt.</p>
<p>Die Singulärwertzerlegung stellt somit eine Verallgemeinerung der Diagonalisierbarkeit
quadratischer Matrizen (und damit der Eigenwert- und Eigenvektor-Berechnung) dar.</p>
<p>Andererseits kann man die Berechnung von Singulärwerten und Singulärvektoren aber auch
als restringiertes konvexes Optimierungsproblem beschreiben. Für den ersten Spaltenvektor <span class="math notranslate nohighlight">\(v_1\)</span>  von <span class="math notranslate nohighlight">\(V\)</span> gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
v_1 = \text{argmax}_{\|v\|_2^2=1}f(v) = \text{argmax}_{\|v\|_2^2 \leq 1}f(v),
\quad
f(v) = \|A v\|_2^2,
\quad
f(v_1) = \sigma_1^2.
\end{equation*}\]</div>
<p>Allgemein gilt mit <span class="math notranslate nohighlight">\(V_k = \text{span}(v_1,\ldots,v_k)\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
v_k = \text{argmax}_{\|v\|_2^2=1, v\bot V_{k-1}}f(v) = \text{argmax}_{\|v\|_2^2\leq 1, v\bot V_{k-1}}f(v),
\quad
f(v) = \|Av\|_2^2,
\quad
f(v_k) = \sigma_{k}^2.
\end{equation*}\]</div>
<p>Für die numerische Berechnung von Singulärwerten und -vektoren haben
wir damit die Möglichkeit, Algorithmen aus beiden Bereichen anzuwenden.
Welche Verfahren dabei effizienter sind, hängt stark von der konkreten Aufgabenstellung
ab.</p>
</div>
<div class="section" id="modellvereinfachung">
<h2>Modellvereinfachung<a class="headerlink" href="#modellvereinfachung" title="Link zu dieser Überschrift">¶</a></h2>
<div class="section" id="regressionsproblem">
<h3>Regressionsproblem<a class="headerlink" href="#regressionsproblem" title="Link zu dieser Überschrift">¶</a></h3>
<p>Wir betrachten wieder das lineares Regressionsproblem aus dem letzten Abschnitt</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">autograd.numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">jacobian</span>

<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sympy</span> <span class="k">as</span> <span class="nn">sy</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">17</span>

<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Math</span>
<span class="o">%</span><span class="k">precision</span> 5
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>


<span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-1</span>

<span class="n">h</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="mi">5</span> <span class="o">-</span> <span class="mi">7</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;b.&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/02_Dimensionsreduktion_6_0.png" src="_images/02_Dimensionsreduktion_6_0.png" />
</div>
</div>
<p>benutzen aber als Modellfunktion</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
g(x,w) = w_1 + w_2\,(x-1) + w_3(x + 1)
\end{equation*}\]</div>
<p>bei unverändertem Loss</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
l(w) = \frac{1}{m}\sum_{i=1}^m l_i(w),
\quad
l_i(w) = \frac{1}{2} \big( g(x_i,w) - y_i \big)^2. 
\end{equation*}\]</div>
<p>Die Modellfunktion ist „überparametriert“:</p>
<ul class="simple">
<li><p>mit <span class="math notranslate nohighlight">\(g\)</span> können beliebige Geraden erzeugt werden</p></li>
<li><p>dafür reichen aber bereits zwei Parameter</p></li>
</ul>
<p>Mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
X = 
\begin{pmatrix}
1 &amp; x_1 - 1 &amp; x_1 + 1\\
\vdots &amp; \vdots &amp; \vdots\\
1 &amp; x_m - 1 &amp; x_m + 1
\end{pmatrix}
\in \mathbb{R}^{m\times 3},
\quad
\end{equation*}\]</div>
<p>und</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
w = 
\begin{pmatrix}
w_1 \\ w_2 \\w_3
\end{pmatrix},
\quad
y = 
\begin{pmatrix}
y_1\\
\vdots\\
y_m
\end{pmatrix}
\in \mathbb{R}^m
\end{equation*}\]</div>
<p>können wir <span class="math notranslate nohighlight">\(l\)</span> umschreiben zu</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
l(x)=\frac{1}{2m} \|Xw - y\|_2^2 = \frac{1}{2m}(Xw - y)^T (Xw - y)
\end{equation*}\]</div>
<p>und erhalten</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
l'(w) = \frac{1}{m}X^T(Xw - y),
\quad
l''(w) = \frac{1}{m}X^T X.
\end{equation*}\]</div>
<p>Unsere Zielfunktion ist also nach wie vor quadratisch und konvex.</p>
<p>Wir wenden jetzt nacheinander die Algorithmen aus dem letzten Abschnitt an.</p>
</div>
<div class="section" id="gd">
<h3>GD<a class="headerlink" href="#gd" title="Link zu dieser Überschrift">¶</a></h3>
<p>Direkte Anwendung von GD liefert folgende Ergebnisse.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">npar</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> 
    <span class="c1">#return w[0] + w[1] * x + w[2] * x**2 + w[3] * x**3</span>

<span class="k">def</span> <span class="nf">l</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">l1</span> <span class="o">=</span> <span class="n">jacobian</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">GD</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">nit</span> <span class="o">=</span> <span class="mi">20</span><span class="p">):</span>
    <span class="n">ww</span> <span class="o">=</span> <span class="p">[</span><span class="n">w0</span><span class="p">]</span>
    <span class="n">w</span>  <span class="o">=</span> <span class="n">w0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nit</span><span class="p">):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">l1</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">ww</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">ww</span>


<span class="k">def</span> <span class="nf">evalw</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># Fit</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">);</span>
    <span class="n">xp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">g</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="s1">&#39;r&#39;</span><span class="p">);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">evalwl</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># Konvergenzgeschwindigkeit</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">w</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$k$&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$l(w^{(k)})$&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    

<span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">npar</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">nit</span> <span class="o">=</span> <span class="mi">300</span><span class="p">)</span>
<span class="n">evalw</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">evalwl</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w = </span><span class="si">{}</span><span class="s2">, l(w) = </span><span class="si">{:f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">l</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w = [-0.9954 -2.5079 -1.5126], l(w) = 0.055993
</pre></div>
</div>
<img alt="_images/02_Dimensionsreduktion_11_1.png" src="_images/02_Dimensionsreduktion_11_1.png" />
<img alt="_images/02_Dimensionsreduktion_11_2.png" src="_images/02_Dimensionsreduktion_11_2.png" />
</div>
</div>
<p>Ändern wir den Startwert so erhalten wir zwar einen ähnlichen Wert für <span class="math notranslate nohighlight">\(l\)</span>, aber
einen anderen Parametersatz <span class="math notranslate nohighlight">\(w\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">npar</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">nit</span> <span class="o">=</span> <span class="mi">300</span><span class="p">)</span>
<span class="n">evalw</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">evalwl</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w = </span><span class="si">{}</span><span class="s2">, l(w) = </span><span class="si">{:f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">l</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w = [-0.6512 -2.828  -1.1768], l(w) = 0.056096
</pre></div>
</div>
<img alt="_images/02_Dimensionsreduktion_13_1.png" src="_images/02_Dimensionsreduktion_13_1.png" />
<img alt="_images/02_Dimensionsreduktion_13_2.png" src="_images/02_Dimensionsreduktion_13_2.png" />
</div>
</div>
<p>GD zeigt dabei unverändertes Konvergenzverhalten.</p>
</div>
<div class="section" id="newton">
<h3>Newton<a class="headerlink" href="#newton" title="Link zu dieser Überschrift">¶</a></h3>
<p>Die Iterationsvorschrift für das einfache Newton-Verfahren lautet</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
w^{(k+1)} = w^{(k)} -  l''\big( w^{(k)} \big)^{-1} l'\big( w^{(k)} \big).
\end{equation*}\]</div>
<p>In unserem Beispiel ist</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
l''(w) = \frac{1}{m}X^T X, 
\quad
X = 
\begin{pmatrix}
1 &amp; x_1 - 1 &amp; x_1 + 1\\
\vdots &amp; \vdots &amp; \vdots\\
1 &amp; x_m - 1 &amp; x_m + 1
\end{pmatrix}.
\end{equation*}\]</div>
<p>Da die Spalten von <span class="math notranslate nohighlight">\(X\)</span> linear abhängig sind, hat <span class="math notranslate nohighlight">\(X\)</span> keinen vollen Rang so dass die Hessematrix <span class="math notranslate nohighlight">\(l''\)</span> singulär (nur positiv semidefinit)
ist. Somit ist das einfache Newton-Verfahren nicht anwendbar.</p>
<p>Singuläre Hesse-Matrizen treten bei Optimierungsproblemen häufig auf, weshalb es eine ganze Reihe von modifizierten Newton-Varianten gibt (gedämpfte Newton-Verfahren, Quasi-Newton, Gauß-Newton, Trust-Region, siehe auch <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html">scipy.optimize.minimize</a>), die
dieses Problem auf unterschiedliche Art und Weise umschiffen.</p>
<p>Wir wenden BFGS an, ein Quasi-Newton-Verfahren dass adaptiv
spd-Näherungen <span class="math notranslate nohighlight">\(B^{(k)}\)</span> der Hesse-Matrix benutzt. Für
<span class="math notranslate nohighlight">\(w^{(0)}, B^{(0)}\)</span> gegeben berechnet man</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
B^{(k)} p^{(k)} 
&amp;= -l'(w^{(k)}) \\
w^{(k+1)} 
&amp;= w^{(k)} + \gamma^{(k)} p^{(k)} \\
B^{(k+1)} 
&amp;= B^{(k)} + \frac{ y^{(k)} {y^{(k)}}^T }{ {h^{(k)}}^T y^{(k)} } 
- \frac{ u^{(k)}{u^{(k)}}^T }{ {h^{(k)}}^T u^{(k)} }
\end{align*}\]</div>
<p>mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
h^{(k)} = w^{(k+1)} - w^{(k)},
\quad
y^{(k)}= l'(w^{(k+1)}) - l'(w^{(k)}),
\quad
u^{(k)} = B^{(k)} h^{(k)}.
\end{equation*}\]</div>
<p><span class="math notranslate nohighlight">\(\gamma^{(k)}\)</span> wird dabei durch Liniensuche bestimmt.</p>
<p>Wir benutzen die BFGS Implementierung von <a class="reference external" href="https://www.scipy.org/">SciPy</a> und erhalten für zwei unterschiedliche Startwerte folgende Ergebnisse.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>

<span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">npar</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">method</span> <span class="o">=</span> <span class="s1">&#39;BFGS&#39;</span><span class="p">,</span> <span class="n">jac</span> <span class="o">=</span> <span class="n">l1</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;disp&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span> <span class="p">)</span><span class="o">.</span><span class="n">x</span>
<span class="n">evalw</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w = </span><span class="si">{}</span><span class="s2">, l(w) = </span><span class="si">{:f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">l</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.055920
         Iterations: 8
         Function evaluations: 9
         Gradient evaluations: 9
w = [-1.0151 -2.532  -1.5169], l(w) = 0.055920
</pre></div>
</div>
<img alt="_images/02_Dimensionsreduktion_19_1.png" src="_images/02_Dimensionsreduktion_19_1.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">npar</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">method</span> <span class="o">=</span> <span class="s1">&#39;BFGS&#39;</span><span class="p">,</span> <span class="n">jac</span> <span class="o">=</span> <span class="n">l1</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;disp&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span> <span class="p">)</span><span class="o">.</span><span class="n">x</span>
<span class="n">evalw</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w = </span><span class="si">{}</span><span class="s2">, l(w) = </span><span class="si">{:f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">l</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Optimization terminated successfully.
         Current function value: 0.055920
         Iterations: 7
         Function evaluations: 8
         Gradient evaluations: 8
w = [-0.6817 -2.8653 -1.1836], l(w) = 0.055920
</pre></div>
</div>
<img alt="_images/02_Dimensionsreduktion_20_1.png" src="_images/02_Dimensionsreduktion_20_1.png" />
</div>
</div>
<p>Die Ergebnisse sind vergleichbar mit denen von GD, allerdings ist
die Anzahl der Iterationen (und damit der Aufwand) deutlich geringer.</p>
</div>
<div class="section" id="normalgleichungen">
<h3>Normalgleichungen<a class="headerlink" href="#normalgleichungen" title="Link zu dieser Überschrift">¶</a></h3>
<p>Wir betrachten jetzt das Normalgleichungssystem</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
X^TXw = X^Ty.
\end{equation*}\]</div>
<p>Da <span class="math notranslate nohighlight">\(X\)</span> linear abhängige Spalten hat, ist <span class="math notranslate nohighlight">\(X^TX\)</span>
positiv semidefinit, also nicht regulär.
Wie wir oben gesehen haben, existieren aber immer Lösungen dieses
Systems.</p>
<p>Wie lösen wir jetzt numerisch dieses singuläre Gleichungssystem?</p>
<p>Direkte Verfahren (LU, Cholesky, QR) sind prinzipiell anwendbar, leiden aber oft an Stabilitätsproblemen.</p>
<p>Bei den iterativen Verfahren betrachten wir wieder CGLS, wenden es auf unser Beispielproblem an
und erhalten für zwei unterschiedliche Startwerte die folgenden Ergebnisse.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">CGLS</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">nit</span> <span class="o">=</span> <span class="mi">20</span><span class="p">):</span>
    <span class="n">ww</span> <span class="o">=</span> <span class="p">[</span><span class="n">w0</span><span class="p">]</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    
    <span class="n">rr</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nit</span><span class="p">):</span>
        <span class="n">Xp</span>  <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
        <span class="n">al</span>  <span class="o">=</span> <span class="n">rr</span> <span class="o">/</span> <span class="n">Xp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xp</span><span class="p">)</span>
        <span class="n">w</span>   <span class="o">=</span> <span class="n">w</span> <span class="o">+</span> <span class="n">al</span> <span class="o">*</span> <span class="n">p</span>
        <span class="n">s</span>   <span class="o">=</span> <span class="n">s</span> <span class="o">-</span> <span class="n">al</span> <span class="o">*</span> <span class="n">Xp</span>
        <span class="n">r</span>   <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">rrn</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
        <span class="n">be</span>  <span class="o">=</span> <span class="n">rrn</span> <span class="o">/</span> <span class="n">rr</span>
        <span class="n">p</span>   <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">be</span> <span class="o">*</span> <span class="n">p</span>
        
        <span class="n">rr</span> <span class="o">=</span> <span class="n">rrn</span>
        <span class="n">ww</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        
    <span class="k">return</span><span class="p">(</span><span class="n">ww</span><span class="p">)</span>


<span class="n">wX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">npar</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">wX</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">npar</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span>

<span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">npar</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">CGLS</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">evalw</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w = </span><span class="si">{}</span><span class="s2">, l(w) = </span><span class="si">{:f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">l</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w = [-1.0151 -2.532  -1.5169], l(w) = 0.055920
</pre></div>
</div>
<img alt="_images/02_Dimensionsreduktion_26_1.png" src="_images/02_Dimensionsreduktion_26_1.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">npar</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">CGLS</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">evalw</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w = </span><span class="si">{}</span><span class="s2">, l(w) = </span><span class="si">{:f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">l</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w = [-0.6817 -2.8653 -1.1836], l(w) = 0.055920
</pre></div>
</div>
<img alt="_images/02_Dimensionsreduktion_27_1.png" src="_images/02_Dimensionsreduktion_27_1.png" />
</div>
</div>
<p>CGLS erreicht bereits im zweiten Schritt wieder (eine) exakte Lösung der Normalgleichungen. Je nach Startwert erhält man unterschiedliche Parameter <span class="math notranslate nohighlight">\(w\)</span>, die aber die selbe Modellfunktion parametrieren.</p>
</div>
<div class="section" id="pseudoinverse-und-singularwertzerlegung-svd">
<h3>Pseudoinverse und Singulärwertzerlegung (SVD)<a class="headerlink" href="#pseudoinverse-und-singularwertzerlegung-svd" title="Link zu dieser Überschrift">¶</a></h3>
<p>Um diese Mehrdeutigkeiten zu beseitigen suchen wir jetzt eine Lösung <span class="math notranslate nohighlight">\(w^+\)</span> der Normalgleichungen, die selbst möglichst kleine Norm <span class="math notranslate nohighlight">\(\|w\|_2\)</span> hat, also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\|X w -y \|_2^2 \to \min
\quad\text{und}\quad
\|w \|_2^2 \to \min.
\end{equation*}\]</div>
<p>Dieses Problem hat immer eine eindeutige Lösung <span class="math notranslate nohighlight">\(w^+\)</span>. Sie hängt linear von <span class="math notranslate nohighlight">\(y\)</span> ab,
d.h. zu <span class="math notranslate nohighlight">\(X\in\mathbb{R}^{m\times n}\)</span> gibt es eine eindeutige Matrix <span class="math notranslate nohighlight">\(X^+\in\mathbb{R}^{n\times m}\)</span>,
die <strong>Pseudoinverse</strong>, mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
w^+ = X^+ y.
\end{equation*}\]</div>
<p>Es gibt direkte Verfahren (über QR-Zerlegungen) um <span class="math notranslate nohighlight">\(X^+\)</span> zu berechnen, in der Praxis
wird <span class="math notranslate nohighlight">\(X^+\)</span> in der Regel über die <strong>Singulärwertzerlegung</strong> bestimmt.</p>
<p>Jede Matrix <span class="math notranslate nohighlight">\(X\in\mathbb{R}^{m\times n}\)</span> lässt sich zerlegen
in</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
X = U\Sigma V^T, 
\end{equation*}\]</div>
<p>wobei
<span class="math notranslate nohighlight">\(U\in\mathbb{R}^{m\times m}\)</span>,
<span class="math notranslate nohighlight">\(V\in\mathbb{R}^{n\times n}\)</span> orthonormal sind und</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\Sigma = \text{diag}\big(\sigma_1, \ldots, \sigma_r, 0, \ldots,0 \big)\in\mathbb{R}^{m\times n},
\quad r \le \min(m,n).
\end{equation*}\]</div>
<p>Dabei sind</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\sigma_1 \ge \ldots \ge \sigma_r &gt; 0
\end{equation*}\]</div>
<p>die <strong>Singulärwerte</strong> und <span class="math notranslate nohighlight">\(r\)</span> der Rang von <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Damit können wir jetzt das Normalgleichungssystem mit Zusatzbedingung (also <span class="math notranslate nohighlight">\(w^+ = X^+ y\)</span>)
über das dazu äquivalente Optimierungsproblem</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\|X w -y \|_2^2 \to \min
\quad\text{und}\quad
\|w \|_2^2 \to \min
\end{equation*}\]</div>
<p>ganz einfach lösen.
Für orthonormale Matrizen <span class="math notranslate nohighlight">\(Q\)</span> gilt <span class="math notranslate nohighlight">\(\|Qw \|_2 = \|w \|_2\)</span>, <span class="math notranslate nohighlight">\(Q^{-1}=Q^T\)</span>, und somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
\|X w -y \|_2^2 
&amp;= \|U\Sigma V^T w -y \|_2^2\\
&amp;= \|\Sigma V^T w - U^Ty \|_2^2\\
&amp;= \|\Sigma \tilde{w} - \tilde{y} \|_2^2
\end{align*}\]</div>
<p>mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\tilde{w} = V^T w, \quad \|\tilde{w}\|_2^2 = \|w\|_2^2
\end{equation*}\]</div>
<p>und <span class="math notranslate nohighlight">\(\tilde{y} = U^Ty\)</span>. Wir müssen jetzt also die Lösung <span class="math notranslate nohighlight">\(\tilde{w}^+\)</span> von</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\|\Sigma \tilde{w} - \tilde{y} \|_2^2 \to \min
\quad\text{und}\quad
\|\tilde{w} \|_2^2 \to \min
\end{equation*}\]</div>
<p>bestimmen und erhalten <span class="math notranslate nohighlight">\(w^+ =V\tilde{w}^+\)</span>.</p>
<p>Nun ist aber</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\|\Sigma \tilde{w} - \tilde{y} \|_2^2 = \sum_{i=1}^r (\sigma_i \tilde{w}_i - \tilde{y}_i\big)^2 + \sum_{i=r+1}^m\tilde{y}_i^2,
\end{equation*}\]</div>
<p>so dass <span class="math notranslate nohighlight">\(\|\Sigma \tilde{w} - \tilde{y} \|_2^2\)</span> minimal wird für</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\tilde{w}^+_i = \frac{\tilde{y}_i}{\sigma_i}, \quad i=1,\ldots,r.
\end{equation*}\]</div>
<p>Damit sind die ersten <span class="math notranslate nohighlight">\(r\)</span> Komponenten von <span class="math notranslate nohighlight">\(\tilde{w}^+\)</span> bestimmt.
Wegen</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\|\tilde{w}\|_2^2 = \sum_{i=1}^n \tilde{w}_i^2,
\end{equation*}\]</div>
<p>wird <span class="math notranslate nohighlight">\(\|\tilde{w}\|_2^2\)</span> minimal für</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\tilde{w}^+_i = 0, \quad i=r+1,\ldots,n.
\end{equation*}\]</div>
<p>Somit ist</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\tilde{w}^+ = \Big(\frac{\tilde{y}_1}{\sigma_1}, \ldots, \frac{\tilde{y}_r}{\sigma_r}, 0, \ldots, 0 \Big)^T
\end{equation*}\]</div>
<p>und <span class="math notranslate nohighlight">\(w^+ =V\tilde{w}^+\)</span>. Wegen <span class="math notranslate nohighlight">\(\tilde{y} = U^Ty\)</span> kann man <span class="math notranslate nohighlight">\(w^+\)</span> jetzt schreiben als</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
w^+ = V\Sigma^+ U^T y, 
\quad
\Sigma^+ = \text{diag}\Big(\frac{1}{\sigma_1}, \ldots, \frac{1}{\sigma_r}, 0, \ldots, 0 \Big)\in\mathbb{R}^{n\times m}.
\end{equation*}\]</div>
<p>Ist also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
X = U\Sigma V^T
\end{equation*}\]</div>
<p>eine Singulärwertzerlegung von <span class="math notranslate nohighlight">\(X\)</span>, dann ist die Pseudoinverse</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
X^+ = V\Sigma^+ U^T
\end{equation*}\]</div>
<p>und <span class="math notranslate nohighlight">\(\Sigma^+\)</span> ist die Pseudoinverse von <span class="math notranslate nohighlight">\(\Sigma\)</span>.</p>
<p>Wie berechnet man nun numerisch eine Singulärwertzerlegung?
Einen ersten Hinweis bekommt man durch folgende Beziehungen</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
X = U \Sigma V^T
\quad\Rightarrow\quad
X^T X = V \Sigma^T \Sigma V^T,
\quad
X X^T = U \Sigma \Sigma^T U^T.
\end{equation*}\]</div>
<p>Da <span class="math notranslate nohighlight">\(U,V\)</span> orthonormal und quadratisch und <span class="math notranslate nohighlight">\(\Sigma^T \Sigma\)</span>, <span class="math notranslate nohighlight">\(\Sigma \Sigma^T\)</span>  Diagonalmatrizen sind,
haben wir damit eine Eigenwert-/Eigenvektorzerlegung der beiden quadratischen
symmetrischen Matrizen <span class="math notranslate nohighlight">\(X^T X\)</span>, <span class="math notranslate nohighlight">\(X X^T\)</span>,
d.h.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sigma_i^2\)</span>, <span class="math notranslate nohighlight">\(i = 1,\ldots,r\)</span>, sind die positiven Eigenwerte von <span class="math notranslate nohighlight">\(X^T X\)</span>, <span class="math notranslate nohighlight">\(X X^T\)</span>, alle weiteren Eigenwerte sind <span class="math notranslate nohighlight">\(0\)</span></p></li>
<li><p>die Spalten von <span class="math notranslate nohighlight">\(V\)</span> sind die Eigenvektoren von <span class="math notranslate nohighlight">\(X^T X\)</span></p></li>
<li><p>die Spalten von <span class="math notranslate nohighlight">\(U\)</span> sind die Eigenvektoren von <span class="math notranslate nohighlight">\(X X^T\)</span></p></li>
</ul>
<p>Damit könnte man numerisch wie folgt vorgehen:</p>
<ul>
<li><p>berechne (z.B. mit QR-Iteration mit Shift) alle Eigenwerte <span class="math notranslate nohighlight">\(\lambda_i\)</span> und Eigenvektoren <span class="math notranslate nohighlight">\(v_i\)</span> von <span class="math notranslate nohighlight">\(X^T X\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \lambda_1 \ge \ldots \ge \lambda_r &gt;0, 
  \quad 
  \lambda_{r+1} = \ldots = \lambda_n = 0
  \end{equation*}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(\sigma_i = \sqrt{\lambda_i},\quad i = 1,\ldots,r\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(V = \big(v_1,\ldots,v_n \big)\)</span></p></li>
<li><p>aus <span class="math notranslate nohighlight">\(U \Sigma = XV\)</span> folgt für die <span class="math notranslate nohighlight">\(i\)</span>-te Spalte der Matrizen</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  (U \Sigma)_i = u_i \sigma_i,
  \quad 
  (XV)_i = Xv_i
  \end{equation*}\]</div>
<p>somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  u_i  
  = \frac{1}{\sigma_i}Xv_i,
  \quad i = 1,\ldots,r
  \end{equation*}\]</div>
</li>
<li><p>ergänze <span class="math notranslate nohighlight">\(u_1,\ldots,u_r\)</span> zu einer Orthonormalbasis <span class="math notranslate nohighlight">\(u_1,\ldots,u_m\)</span> (z.B. mit modifiziertem Gram-Schmidt)
und setze</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  U = \big(u_1,\ldots,u_m \big)
  \end{equation*}\]</div>
</li>
</ul>
<p>Alternativ könnte man natürlich auch über die Eigenwerte/Eigenvektoren von <span class="math notranslate nohighlight">\(XX^T\)</span> die SVD berechnen.
Beide Varianten sind nicht optimal (Aufwand, Kondition).</p>
<p>Der heute gebräuchlichste Algorithmus vermeidet
die explizite Berechnung von <span class="math notranslate nohighlight">\(X^TX\)</span>. Er basiert zunächst auf der Beobachtung, dass die Singulärwerte
von <span class="math notranslate nohighlight">\(X\)</span> identisch sind mit den Singulärwerten von</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
TXS, \quad T,S \text{ orthonormal.}
\end{equation*}\]</div>
<p>Im ersten Schritt wird mit einer Folge von Householder-Transformationen von links und rechts <span class="math notranslate nohighlight">\(T,S\)</span> so bestimmt, dass</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
TXS = R, \quad R \text{ bidiagonal,}
\end{equation*}\]</div>
<p>was das Analogon zur Hessenbergtransformation bei der Eigenwertberechnung ist.</p>
<p>Der zweite Schritt beruht auf einer angepassten QR-Iteration zur Berechnung der
Singulärwerte (Wurzel der Eigenwerte von <span class="math notranslate nohighlight">\(R^TR\)</span>), ohne <span class="math notranslate nohighlight">\(R^TR\)</span> explizit zu berechnen.</p>
<p>Wir wenden nun die Pseudoinverse an, um unser unterbestimmtes Regressionsproblem zu lösen.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">evalw</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w = </span><span class="si">{}</span><span class="s2">, l(w) = </span><span class="si">{:f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">l</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w = [-1.0151 -2.532  -1.5169], l(w) = 0.055920
</pre></div>
</div>
<img alt="_images/02_Dimensionsreduktion_36_1.png" src="_images/02_Dimensionsreduktion_36_1.png" />
</div>
</div>
</div>
<div class="section" id="abgeschnittene-singularwertzerlegung-tsvd">
<h3>Abgeschnittene Singulärwertzerlegung (TSVD)<a class="headerlink" href="#abgeschnittene-singularwertzerlegung-tsvd" title="Link zu dieser Überschrift">¶</a></h3>
<p>Jede Matrix <span class="math notranslate nohighlight">\(X\in\mathbb{R}^{m\times n}\)</span> kann
in</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
X = U\Sigma V^T, 
\end{equation*}\]</div>
<p><span class="math notranslate nohighlight">\(U\in\mathbb{R}^{m\times m}\)</span>,
<span class="math notranslate nohighlight">\(V\in\mathbb{R}^{n\times n}\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\Sigma = \text{diag}\big(\sigma_1, \ldots, \sigma_r, 0, \ldots,0 \big)\in\mathbb{R}^{m\times n},
\quad
r \le \min(m,n)
\end{equation*}\]</div>
<p>zerlegt werden.</p>
<p>Da in <span class="math notranslate nohighlight">\(\Sigma\)</span> nur die ersten <span class="math notranslate nohighlight">\(r\)</span> Diagonalelemente ungleich <span class="math notranslate nohighlight">\(0\)</span> sind
folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
X 
= U\Sigma V^T
= \sum_{i=1}^r u_i \sigma_i v_i^T
= U_r\Sigma_r V_r^T
\end{equation*}\]</div>
<p>mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
U_r 
&amp;= \big(u_1,\ldots,u_r \big) \in \mathbb{R}^{m\times r}, \\
\Sigma_r 
&amp;= \text{diag}\big(\sigma_1,\ldots,\sigma_r \big) \in \mathbb{R}^{r\times r}, \\
V_r 
&amp;= \big(v_1,\ldots,v_r \big) \in \mathbb{R}^{n\times r}
\end{align*}\]</div>
<p>sowie</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
X^+ = V_r \Sigma_r^{-1} U_r^T.
\end{equation*}\]</div>
<p>Ist <span class="math notranslate nohighlight">\(r \ll \min(m,n)\)</span>, so sind Matrixprodukte über <span class="math notranslate nohighlight">\(U_r\Sigma_r V_r^T\)</span>,
<span class="math notranslate nohighlight">\(V_r \Sigma_r^{-1} U_r^T\)</span> deutlich günstiger zu Berechnen als Produkte die direkt <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(X^+\)</span> benutzen.</p>
<p>Nehmen wir nun an, dass <span class="math notranslate nohighlight">\(X\in\mathbb{R}^{m\times n}\)</span> Singulärwerte
<span class="math notranslate nohighlight">\(\sigma_1 \ge \ldots \ge \sigma_k \ge \sigma_{k+1} \ge \ldots \ge \sigma_r &gt; 0\)</span> mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\sigma_1  \gg \sigma_{k+1}
\end{equation*}\]</div>
<p>hat. Damit erhalten wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
X 
= U\Sigma V^T
= \sum_{i=1}^r u_i \sigma_i v_i^T
\approx \sum_{i=1}^k u_i \sigma_i v_i^T
= U_k\Sigma_k V_k^T
= X_k
\end{equation*}\]</div>
<p><span class="math notranslate nohighlight">\(X_k\)</span> ist eine Rang-<span class="math notranslate nohighlight">\(k\)</span>-Approximation der Rang-<span class="math notranslate nohighlight">\(r\)</span>-Matrix <span class="math notranslate nohighlight">\(X\)</span> (abgeschnittene Singulärwertzerlegung, TSVD), also
eine vereinfachte Version unseres ursprünglichen linearen
Modells, das durch <span class="math notranslate nohighlight">\(X\)</span> repräsentiert wird.
Wegen</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
X_k = U_k\Sigma_k V_k^T
\end{equation*}\]</div>
<p>ist <span class="math notranslate nohighlight">\(X_k\)</span> für <span class="math notranslate nohighlight">\(k\ll r\)</span> numerisch viel effizienter zu bearbeiten als <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Wir wenden nun die SVD auf unser Regressionsproblem von oben an und erhalten Singulärwerte</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">U</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">VT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">VT</span><span class="o">.</span><span class="n">T</span>

<span class="n">s</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1.8869e+01, 2.7241e+00, 3.0411e-15])
</pre></div>
</div>
</div>
</div>
<p>Offensichtlich können wir hier <span class="math notranslate nohighlight">\(k = 2\)</span>, wählen d.h.  zwei Parameter reichen aus.
Betrachten wir die zugehörigen Singulärbasen <span class="math notranslate nohighlight">\(V\)</span></p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">V</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-0.5222,  0.6277, -0.5774],
       [ 0.2825,  0.7661,  0.5774],
       [ 0.8047,  0.1383, -0.5774]])
</pre></div>
</div>
</div>
</div>
<p>dann sehen wir, dass Parameterkombinationen <span class="math notranslate nohighlight">\(w = s(-1,1,-1)^T\)</span>, <span class="math notranslate nohighlight">\(s\in\mathbb{R}\)</span>, vernachlässigbar sind.
Durch Einschränkung auf die ersten beiden Singulärbasen kann man das Modell dann vereinfachen und wir erhalten</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">Xk</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">V</span><span class="p">[:,:</span><span class="n">k</span><span class="p">])</span>

<span class="n">wk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">Xk</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">V</span><span class="p">[:,:</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">wk</span><span class="p">)</span>

<span class="n">evalw</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w = </span><span class="si">{}</span><span class="s2">, l(w) = </span><span class="si">{:f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">l</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w = [-1.0151 -2.532  -1.5169], l(w) = 0.055920
</pre></div>
</div>
<img alt="_images/02_Dimensionsreduktion_46_1.png" src="_images/02_Dimensionsreduktion_46_1.png" />
</div>
</div>
<p>Jetzt betrachten wir zu den selben Daten <span class="math notranslate nohighlight">\(x_i,y_i\)</span> das Regressionsproblem mit Modellfunktion</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
g(x,w) = w_1 + w_2 \, x + w_3 \, x^2 + w_4 \, x^3
\end{equation*}\]</div>
<p>Einfache Regression liefert das folgende Ergebnis</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">npar</span> <span class="o">=</span> <span class="mi">4</span>

<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span>


<span class="n">wX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">npar</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">wX</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">npar</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">evalw</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w = </span><span class="si">{}</span><span class="s2">, l(w) = </span><span class="si">{:f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">l</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w = [ 4.9521 -6.5425  2.0585  0.5842], l(w) = 0.010212
</pre></div>
</div>
<img alt="_images/02_Dimensionsreduktion_48_1.png" src="_images/02_Dimensionsreduktion_48_1.png" />
</div>
</div>
<p>Als Singulärwerte von <span class="math notranslate nohighlight">\(X\)</span> erhalten wir</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">npar</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">wX</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">npar</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span>

<span class="n">U</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">VT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">VT</span><span class="o">.</span><span class="n">T</span>

<span class="n">s</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([12.3789,  4.0561,  0.8277,  0.0902])
</pre></div>
</div>
</div>
</div>
<p>Hier würden wir bei <span class="math notranslate nohighlight">\(k=3\)</span> abschneiden. Die Regression auf den reduzierten Daten liefert dann</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">Xk</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">V</span><span class="p">[:,:</span><span class="n">k</span><span class="p">])</span>

<span class="n">wk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">Xk</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">V</span><span class="p">[:,:</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">wk</span><span class="p">)</span>

<span class="n">evalw</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w = </span><span class="si">{}</span><span class="s2">, l(w) = </span><span class="si">{:f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">l</span><span class="p">(</span><span class="n">w</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w = [ 4.853  -5.3505 -0.8073  2.4439], l(w) = 0.011277
</pre></div>
</div>
<img alt="_images/02_Dimensionsreduktion_52_1.png" src="_images/02_Dimensionsreduktion_52_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="dimensionsreduktion-uber-principle-component-analysis-pca">
<h2>Dimensionsreduktion über Principle Component Analysis (PCA)<a class="headerlink" href="#dimensionsreduktion-uber-principle-component-analysis-pca" title="Link zu dieser Überschrift">¶</a></h2>
<div class="section" id="motivation">
<h3>Motivation<a class="headerlink" href="#motivation" title="Link zu dieser Überschrift">¶</a></h3>
<p>Oft sind die Daten in Form hochdimensionaler Vektoren
<span class="math notranslate nohighlight">\(x_j\in \mathbb{R}^m\)</span>, <span class="math notranslate nohighlight">\(j=1,\ldots,n\)</span>, also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
X = (x_1,\ldots,x_n) \in \mathbb{R}^{m \times n}
\end{equation*}\]</div>
<p>mit <span class="math notranslate nohighlight">\(m\)</span> groß gegeben. Ein typische Beispiel ist die Verarbeitung
von Bildern (<span class="math notranslate nohighlight">\(m\)</span> ist die Anzahl der Pixel multipliziert mit der Anzahl der Farbkanäle).</p>
<p>Die direkte Anwendung von numerischen Algorithmen ist dann meistens mit großem Rechenaufwand verbunden.
Deshalb versucht man, die Dimension der Daten zunächst zu reduzieren
und zwar so, dass der dadurch entstehende Informationsverlust möglichst gering ist.</p>
<p>Wir betrachten den folgenden zweidimensionalen Beispieldatensatz</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>

<span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_features</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">centers</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="n">seed</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/02_Dimensionsreduktion_56_0.png" src="_images/02_Dimensionsreduktion_56_0.png" />
</div>
</div>
<p>Offensichtlich gibt es eine Richtung, in der die Daten
stark variieren. Senkrecht dazu ist die Variation eher gering.</p>
<p>Zur Datenreduktion könnte man nun eine Gerade in die Richtung
der starken Variationen legen und die Projektionen der Daten
auf diese Gerade betrachten.</p>
<p>Die Projektion für einen Datenpunkt <span class="math notranslate nohighlight">\(x_i\)</span> ist ein Skalar, also eine
eindimensionale Größe („Koordinate entlang der Geraden“)</p>
<p>Wenn diese Projektionen für die einzelnen Datenpunkte
sich hinreichend stark unterscheiden, dann kann man z.B. ein
Klassifikationsverfahren statt auf den zweidimensionalen
Originaldaten ohne große Verluste auch auf den eindimensionlen
Projektionen der Daten durchführen.</p>
<p>Um die weiteren Betrachtungen zu vereinfachen, zentrieren
wir den Datensatz. Den allgemeinen Fall behandeln wir im Anschluss.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">VT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:</span> <span class="p">,</span><span class="mi">0</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="o">-</span><span class="n">v</span><span class="p">,</span> <span class="n">v</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span> <span class="s1">&#39;g&#39;</span><span class="p">)</span>

<span class="n">Xi</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">45</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">Xi</span><span class="p">,</span> <span class="s1">&#39;r.&#39;</span><span class="p">)</span>

<span class="n">pXi</span> <span class="o">=</span> <span class="n">Xi</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">*</span> <span class="n">v</span>
<span class="n">eXi</span> <span class="o">=</span> <span class="n">Xi</span> <span class="o">-</span> <span class="n">pXi</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="mi">0</span><span class="o">*</span><span class="n">pXi</span><span class="p">,</span> <span class="n">pXi</span><span class="p">],</span> <span class="s1">&#39;r&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">pXi</span><span class="p">,</span> <span class="n">Xi</span><span class="p">],</span> <span class="s1">&#39;r:&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="mi">0</span><span class="o">*</span><span class="n">pXi</span><span class="p">,</span> <span class="n">Xi</span><span class="p">],</span> <span class="s1">&#39;r:&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/02_Dimensionsreduktion_58_0.png" src="_images/02_Dimensionsreduktion_58_0.png" />
</div>
</div>
</div>
<div class="section" id="mathematische-formulierung">
<h3>Mathematische Formulierung<a class="headerlink" href="#mathematische-formulierung" title="Link zu dieser Überschrift">¶</a></h3>
<p>Gegeben ist also ein zentrierter Datensatz</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
x_j \in \mathbb{R}^m, \quad j = 1,\ldots,n,
\quad
\bar{X} = \frac{1}{n} \sum_{j=1}^n x_j = 0.
\end{equation*}\]</div>
<p>Wir suchen eine Gerade</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
g(s) = s \, u, 
\quad s\in \mathbb{R}, \quad u \in \mathbb{R}^m \quad\text{mit}\quad \|u\|_2 = 1,
\end{equation*}\]</div>
<p>so dass die Varianz</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
V_1(u) 
&amp;= \frac{1}{n-1}\sum_{j=1}^n (x_j,u)_2^2\\
&amp;= \frac{1}{n-1}(X^T u, X^T u)_2\\
%&amp;= \frac{1}{n-1}u^T X X^T u\\
&amp;= \frac{1}{n-1} \|X^T u\|_2^2
\end{align*}\]</div>
<p>der Projektionen von <span class="math notranslate nohighlight">\(x_i\)</span> auf <span class="math notranslate nohighlight">\(u\)</span> maximal wird.
Mit der Singulärwertzerlegung <span class="math notranslate nohighlight">\(X = U\Sigma V^T\)</span> erhalten wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
V_1(u) 
&amp;= \frac{1}{n-1} \|V\Sigma^T U^T u\|_2^2\\
&amp;= \frac{1}{n-1} \|\Sigma^T z\|_2^2\\
&amp;= \frac{1}{n-1} \sum_{i=1}^r (\sigma_i z)^2\\
\end{align*}\]</div>
<p>mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\quad z =  U^T u,
\quad \|z\|_2 =  \|u\|_2.
\end{equation*}\]</div>
<p>Wir müssen also das Problem</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\max_{z} f(z), \quad g(z)= 0
\end{equation*}\]</div>
<p>für</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
f(z) 
&amp;= \sum_{i=1}^r (\sigma_i z_i)^2 = \|\Sigma^T z\|_2^2 = (\Sigma^T z, \Sigma^T z)_2,
\\
g(z) 
&amp;= \| z\|_2^2 - 1 = (z, z)_2 - 1
\end{align*}\]</div>
<p>lösen.
Damit neben der Zielfunktion <span class="math notranslate nohighlight">\(f\)</span> auch die Nebenbedingung <span class="math notranslate nohighlight">\(g\)</span> differenzierbar ist benutzen wir
nicht <span class="math notranslate nohighlight">\(\|z\|_2 = 1\)</span> sondern äquivalent <span class="math notranslate nohighlight">\(\|z\|^2_2 = 1\)</span>.
Beide Funktionen <span class="math notranslate nohighlight">\(f\)</span> und <span class="math notranslate nohighlight">\(g\)</span> sind auch konvex.</p>
<p>Wir stellen nun die Lagrangefunktion</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
L(z,\lambda) = f(z) - \lambda g(z)
\end{equation*}\]</div>
<p>auf und betrachten deren stationäre Punkte.
Für die Ableitungen folgt nach der Produktregel</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
\partial_z L(z, \lambda)(w)
&amp; = (\Sigma^T w, \Sigma^T z)_2 + (\Sigma^T z, \Sigma^T w)_2- \lambda \big((w, z)_2 + (z, w)_2\big)\\
&amp; = 2 \big((\Sigma^T w, \Sigma^T z)_2 - \lambda (w, z)_2 \big)\\
&amp; = 2 \big(w^T\Sigma \Sigma^T z - \lambda w^T z \big)
\end{align*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\partial_z L(z, \lambda)
= 2 \big(\Sigma \Sigma^T z - \lambda z \big)
\end{equation*}\]</div>
<p>bzw.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\partial_\lambda L(z, \lambda)  = \|z\|_2^2 - 1
\end{equation*}\]</div>
<p>Als notwendige Bedingung für eine Maximalstelle
<span class="math notranslate nohighlight">\(\hat{z}\)</span> muss es ein <span class="math notranslate nohighlight">\(\hat{\lambda}\in\mathbb{R}\)</span> geben mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\partial_z L(\hat{z}, \hat{\lambda}) = 0,
\quad
\partial_\lambda L(\hat{z}, \hat{\lambda}) = 0.
\end{equation*}\]</div>
<p>Für die Maximalstelle <span class="math notranslate nohighlight">\(\hat{z}\in\mathbb{R}^m\)</span> muss also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\|\hat{z}\|_2 = 1,
\quad
\Sigma \Sigma^T \hat{z} = \hat{\lambda} \hat{z}
\end{equation*}\]</div>
<p>gelten, d.h. <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span>, <span class="math notranslate nohighlight">\(\hat{z}\)</span> sind Eigenwert und Eigenvektor
der Diagonalmatrix</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\Sigma \Sigma^T = 
\begin{pmatrix}
\sigma_1^2 \\
&amp; \ddots\\
&amp;&amp; \sigma_r^2\\
&amp;&amp;&amp; 0\\
&amp;&amp;&amp;&amp; \ddots
\end{pmatrix},
\end{equation*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\hat{z} = e_i,
\quad
\hat{\lambda}_i = 
\begin{cases}
\sigma_i^2 &amp; i = 1,\ldots,r \\
0 &amp; i&gt;r
\end{cases},
\end{equation*}\]</div>
<p>wobei <span class="math notranslate nohighlight">\(e_i\)</span> der <span class="math notranslate nohighlight">\(i\)</span>-te Einheitsvektor ist.
Als Wert unserer Zielfunktion <span class="math notranslate nohighlight">\(f\)</span> erhalten wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
f(\hat{z}) 
= \|\Sigma^T e_i\|_2^2
= \sigma_i^2
\end{equation*}\]</div>
<p>so dass das gesuchte Maximum für <span class="math notranslate nohighlight">\(i=1\)</span> angenommen wird.</p>
<p>Für unser Ausgangsproblem erhalten wir somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\hat{u} = U e_1= u_1,
\quad
V_1(\hat{u}) 
= \frac{\sigma_1^2}{n-1},
\quad
X = U\Sigma V^T,
\end{equation*}\]</div>
<p>d.h. die Gerade mit maximaler Varianz der Projektionen wird vom ersten
linken Singulärvektor <span class="math notranslate nohighlight">\(u_1\)</span> von <span class="math notranslate nohighlight">\(X\)</span> aufgespannt und die Varianz der Projektion
ist durch den zugehörigen Singulärwert <span class="math notranslate nohighlight">\(\sigma_i\)</span> gegeben,
oder anders ausgedrückt, <span class="math notranslate nohighlight">\(u_1\)</span> ist
der Eigenvektor von <span class="math notranslate nohighlight">\(C = XX^T\)</span>
zum größten Eigenwert
<span class="math notranslate nohighlight">\(\lambda_1 = \sigma_1^2\)</span> und</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
V_1(u_1) 
= \frac{\lambda_1}{n-1}
.
\end{equation*}\]</div>
<p>Projiziert man die Daten <span class="math notranslate nohighlight">\(X\)</span> jetzt auf <span class="math notranslate nohighlight">\(u_1^\bot\)</span>, so reduziert sich die Dimension auf <span class="math notranslate nohighlight">\((m-1)\times n\)</span>. Wendet man die selben Überlegungen erneut an, so erhält man
als zweite Richtung mit der zweitgrößten Varianz</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\hat{u} = u_2,
\quad
V_2(u_2) 
= \frac{\sigma_2^2}{n-1}.
\end{equation*}\]</div>
<p>Setzt man dies nun per Induktion weiter fort, so erhält nach <span class="math notranslate nohighlight">\(r\)</span> Schritten</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
\mathbb{R}^{m\times n} \ni X 
&amp;= U\Sigma V^T 
= U_r \Sigma_r V_r^T 
= U_r \tilde{X}, 
\\ 
\tilde{X} &amp;= \Sigma_r V_r^T = U_r^T X
\in \mathbb{R}^{r\times n}
.
\end{align*}\]</div>
<p>Die <span class="math notranslate nohighlight">\(u_j\)</span> heißen <strong>Hauptkomponenten</strong>
oder auch <strong>Karhunen-Loeve-Richtungen</strong>
zum Datensatz <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Die Daten <span class="math notranslate nohighlight">\(X\in \mathbb{R}^{m\times n}\)</span> werden in ein neues Orthonormalsystem der Dimension
<span class="math notranslate nohighlight">\(r \le m\)</span> transformiert (Spalten von <span class="math notranslate nohighlight">\(U_r\)</span>). Die neuen Koordinaten sind die Projektionen
<span class="math notranslate nohighlight">\(\tilde{X}= U_r^T X\)</span>. Für die Varianzen der einzelnen Koordinaten gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
V(\tilde{X}_{i,\cdot}) = \frac{\sigma_i^2}{n-1}, \quad i = 1,\ldots,r
\end{equation*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
V(\tilde{X}_{1,\cdot}) \ge V(\tilde{X}_{2,\cdot}) \ge \ldots \ge V(\tilde{X}_{r,\cdot}) &gt; 0.
\end{equation*}\]</div>
<p>Außerdem gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
V(X) 
= \sum_{i=1}^m V(X_{i,\cdot})
= \sum_{i=1}^r V(\tilde{X}_{i,\cdot})
= V(\tilde{X}),
\end{equation*}\]</div>
<p>d.h. in <span class="math notranslate nohighlight">\(\tilde{X}\)</span> „stecken alle Variationen von <span class="math notranslate nohighlight">\(X\)</span>“ drin, d.h. bezüglich Unterscheidungsmöglichkeiten
der Daten haben wir keine Information verloren.</p>
<p>Damit liegt nun folgende Vorgehensweise zur Dimensionsreduktion nahe:</p>
<ul>
<li><p>berechne <span class="math notranslate nohighlight">\(V(X)\)</span></p></li>
<li><p>bestimme zu <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span> ein
möglichst kleines <span class="math notranslate nohighlight">\(k\)</span> mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \sum_{i=1}^k V(\tilde{X}_{i,\cdot}) 
   = \frac{\sum_{i=1}^k\sigma_i^2}{n-1}
  \ge V(X) - \varepsilon
  \end{equation*}\]</div>
</li>
<li><p>benutze statt <span class="math notranslate nohighlight">\(X\in \mathbb{R}^{m\times n}\)</span> jetzt <span class="math notranslate nohighlight">\(\tilde{X} = U_k^TX\in \mathbb{R}^{k\times n}\)</span></p></li>
</ul>
<p>Man nimmt dabei einen (geringen) Informationsverlust (<span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>) zugunsten einer
Dimensionsreduktion (oft ist <span class="math notranslate nohighlight">\(k\ll m\)</span>) in Kauf.</p>
<p>Bisher haben wir angenommen, dass die Daten <span class="math notranslate nohighlight">\(x_j\)</span> zentriert sind.
Die Verallgemeinerung ist recht einfach. Sind die Daten <span class="math notranslate nohighlight">\(x_j\)</span> beliebig, so betrachtet
man die zentrierten Größen</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
y_j = x_j - \bar{X}, \quad \bar{X} = \frac{1}{n} \sum_{j=1}^n x_j. 
\end{equation*}\]</div>
<p>Man erhält Hauptrichtungen <span class="math notranslate nohighlight">\(u_j\)</span> und Varianzen <span class="math notranslate nohighlight">\(V(Y_{i,\cdot})\)</span>
über die Eigenvektoren und Eigenwerte von</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
C = YY^T = (X - \bar{X}) (X - \bar{X})^T,
\end{equation*}\]</div>
<p>der <a class="reference external" href="https://de.wikipedia.org/wiki/Kovarianzmatrix">Kovarianzmatrix</a> der Originaldaten <span class="math notranslate nohighlight">\(X\)</span>.
Die neuen Koordinaten <span class="math notranslate nohighlight">\(\tilde{Y} = U_r^T(X - \bar{X})\)</span>
sind die Projektionen auf die Geraden</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\hat{g}_j = \bar{X} + s \, u_j,
\end{equation*}\]</div>
<p>d.h. man projiziert auf affine Unterräume.</p>
</div>
<div class="section" id="beispiel">
<h3>Beispiel<a class="headerlink" href="#beispiel" title="Link zu dieser Überschrift">¶</a></h3>
<p>Wir betrachten den folgenden Datensatz.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#%matplotlib notebook</span>

<span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="n">seed</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>

<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="c1">#plt.axis(&#39;equal&#39;);</span>

<span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

<span class="n">Xbar</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">Xbar</span>
<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">VT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

<span class="k">for</span> <span class="n">col</span><span class="p">,</span><span class="n">u</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">U</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="o">-</span><span class="n">u</span><span class="p">,</span> <span class="n">u</span><span class="p">]</span><span class="o">*</span><span class="mi">15</span> <span class="o">+</span> <span class="n">Xbar</span><span class="p">));</span>

<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">70</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/02_Dimensionsreduktion_70_0.png" src="_images/02_Dimensionsreduktion_70_0.png" />
</div>
</div>
<p>Die Hauptkomponenten sind</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">U</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-0.6248, -0.6964,  0.3531],
       [ 0.2762, -0.6201, -0.7342],
       [ 0.7303, -0.3612,  0.5798]])
</pre></div>
</div>
</div>
</div>
<p>Für die Varianzen erhalten wir</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vk</span> <span class="o">=</span> <span class="n">s</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">vk</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([52.785 ,  4.4244,  1.0172])
</pre></div>
</div>
</div>
</div>
<p>In Scikit-Learn stehen entsprechende Methoden zur Verfügung, mit denen man diese Berechnungen sehr einfach durchführen lassen kann. Wir erhalten</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#%matplotlib notebook</span>

<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="c1">#plt.axis(&#39;equal&#39;);</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="k">for</span> <span class="n">col</span><span class="p">,</span><span class="n">u</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">T</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="o">-</span><span class="n">u</span><span class="p">,</span> <span class="n">u</span><span class="p">]</span><span class="o">*</span><span class="mi">15</span> <span class="o">+</span> <span class="n">Xbar</span><span class="p">));</span>

<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">70</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/02_Dimensionsreduktion_76_0.png" src="_images/02_Dimensionsreduktion_76_0.png" />
</div>
</div>
<p>mit Hauptkomponenten</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-0.6248, -0.6964,  0.3531],
       [ 0.2762, -0.6201, -0.7342],
       [ 0.7303, -0.3612,  0.5798]])
</pre></div>
</div>
</div>
</div>
<p>und Varianzen</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([52.785 ,  4.4244,  1.0172])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="pca-bei-klassifikation-von-ziffern">
<h3>PCA bei Klassifikation von Ziffern<a class="headerlink" href="#pca-bei-klassifikation-von-ziffern" title="Link zu dieser Überschrift">¶</a></h3>
<p>Wir wenden die oben erklärten Methoden auf die Klassifikation
von handschriftlichen Ziffern an.</p>
<p>In Scikit-Learn gibt es einen entsprechenden Datensatz mit
8x8-Pixel Graustufenbildern.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">target</span>

<span class="k">def</span> <span class="nf">digiplot</span><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">mi</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">pi</span> <span class="o">=</span> <span class="mi">8</span><span class="p">):</span>
    <span class="c1"># Anzahl der Subplots</span>
    <span class="k">if</span> <span class="n">imgs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">images</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">imgs</span><span class="o">.</span><span class="n">shape</span><span class="p">]</span>
        <span class="n">ni</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">m</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">n</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="mf">2.0</span><span class="o">*</span><span class="n">m</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="p">[</span><span class="n">ax</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">imgs</span>
        <span class="n">ni</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">m</span> <span class="o">=</span> <span class="p">(</span><span class="n">ni</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span>
        
        <span class="k">if</span> <span class="n">m</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">n</span> <span class="o">=</span> <span class="n">ni</span>
        
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="mf">2.0</span><span class="o">*</span><span class="n">m</span><span class="p">])</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">axk</span> <span class="ow">in</span> <span class="n">ax</span><span class="p">:</span>
            <span class="n">axk</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>        

    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">axk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ax</span><span class="p">[:</span><span class="n">ni</span><span class="p">]):</span>
        <span class="n">axk</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mi</span><span class="p">,</span><span class="n">pi</span><span class="p">),</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">)</span>
        <span class="n">axk</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>

<span class="n">ndigi</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">digiplot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="n">ndigi</span><span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="n">ndigi</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/02_Dimensionsreduktion_83_0.png" src="_images/02_Dimensionsreduktion_83_0.png" />
</div>
</div>
<p>Die 1797 Bilder sind in Form einer Matrix mit Spaltenvektoren der Dimension 64 abgespeichert:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1797, 64)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="klassifikation-mit-k-means-clustering-ohne-pca">
<h3>Klassifikation mit k-Means-Clustering ohne PCA<a class="headerlink" href="#klassifikation-mit-k-means-clustering-ohne-pca" title="Link zu dieser Überschrift">¶</a></h3>
<p>Wir lassen den Datensatz jetzt direkt mit k-Means-Clustering in 10 Klassen einteilen. Die einzelnen Daten <span class="math notranslate nohighlight">\(x_j\)</span> haben Dimension 64.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">km</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="n">seed</span><span class="p">)</span>
<span class="n">yd</span> <span class="o">=</span> <span class="n">km</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">ii</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">yd</span> <span class="o">==</span> <span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()[:</span><span class="mi">10</span><span class="p">]</span>
    <span class="n">digiplot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">ii</span><span class="p">],</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">yd</span><span class="p">[</span><span class="n">ii</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">ii</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/02_Dimensionsreduktion_88_0.png" src="_images/02_Dimensionsreduktion_88_0.png" />
<img alt="_images/02_Dimensionsreduktion_88_1.png" src="_images/02_Dimensionsreduktion_88_1.png" />
<img alt="_images/02_Dimensionsreduktion_88_2.png" src="_images/02_Dimensionsreduktion_88_2.png" />
<img alt="_images/02_Dimensionsreduktion_88_3.png" src="_images/02_Dimensionsreduktion_88_3.png" />
<img alt="_images/02_Dimensionsreduktion_88_4.png" src="_images/02_Dimensionsreduktion_88_4.png" />
<img alt="_images/02_Dimensionsreduktion_88_5.png" src="_images/02_Dimensionsreduktion_88_5.png" />
<img alt="_images/02_Dimensionsreduktion_88_6.png" src="_images/02_Dimensionsreduktion_88_6.png" />
<img alt="_images/02_Dimensionsreduktion_88_7.png" src="_images/02_Dimensionsreduktion_88_7.png" />
<img alt="_images/02_Dimensionsreduktion_88_8.png" src="_images/02_Dimensionsreduktion_88_8.png" />
<img alt="_images/02_Dimensionsreduktion_88_9.png" src="_images/02_Dimensionsreduktion_88_9.png" />
</div>
</div>
<p>Zur besseren Übersicht wandeln wir die Klassennummern in die (vermeintlich) erkannten Ziffern um.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ytrans</span><span class="p">(</span><span class="n">yd</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">ydcl</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">yd</span><span class="p">))</span>
    
    <span class="n">tt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ydcl</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">ydk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ydcl</span><span class="p">):</span>
        <span class="n">ii</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">yd</span> <span class="o">==</span> <span class="n">ydk</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">u</span><span class="p">,</span> <span class="n">uc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">ii</span><span class="p">],</span> <span class="n">return_counts</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">tt</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">u</span><span class="p">[</span><span class="n">uc</span><span class="o">.</span><span class="n">argmax</span><span class="p">()]</span>
    
    <span class="n">ttt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">tt</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">ttt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">ydcl</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;**** Fehler ****&quot;</span><span class="p">)</span>
        <span class="n">tt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">tt</span><span class="p">)</span>

<span class="n">tt</span> <span class="o">=</span> <span class="n">ytrans</span><span class="p">(</span><span class="n">yd</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">ydt</span> <span class="o">=</span> <span class="n">tt</span><span class="p">[</span><span class="n">yd</span><span class="p">]</span>
<span class="n">digiplot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="n">ndigi</span><span class="p">:],</span> <span class="n">ydt</span><span class="p">[</span><span class="o">-</span><span class="n">ndigi</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/02_Dimensionsreduktion_90_0.png" src="_images/02_Dimensionsreduktion_90_0.png" />
</div>
</div>
<p>Die Beurteilung des Ergebnisses erfolgt wieder mit den Standardmethoden von Scikit-Learn</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ydt</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

           0       0.99      0.99      0.99       178
           1       0.60      0.30      0.40       182
           2       0.84      0.83      0.84       177
           3       0.88      0.85      0.86       183
           4       0.98      0.90      0.94       181
           5       0.93      0.75      0.83       182
           6       0.97      0.98      0.98       181
           7       0.83      0.98      0.90       179
           8       0.45      0.57      0.51       174
           9       0.56      0.78      0.66       180

    accuracy                           0.79      1797
   macro avg       0.80      0.79      0.79      1797
weighted avg       0.81      0.79      0.79      1797
</pre></div>
</div>
</div>
</div>
<div class="section" id="klassifikation-mit-pca-und-k-means-clustering">
<h4>Klassifikation mit PCA und k-Means-Clustering<a class="headerlink" href="#klassifikation-mit-pca-und-k-means-clustering" title="Link zu dieser Überschrift">¶</a></h4>
<p>Wir führen zunächst eine (partielle) PCA durch und schauen uns die Varianzen der
Hauptrichtungen an</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">nu</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">nu</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([179.0069, 163.7177, 141.7884, 101.1004,  69.5132,  59.1085,
        51.8845,  44.0151,  40.311 ,  37.0118,  28.519 ,  27.3212,
        21.9015,  21.3244,  17.6367,  16.9469,  15.8514,  15.0045,
        12.2345,  10.8869,  10.6936,   9.5826,   9.2264,   8.6903,
         8.3656,   7.1658,   6.9197,   6.1929,   5.8849,   5.1559,
         4.4902,   4.2463])
</pre></div>
</div>
</div>
</div>
<p>bzw. die relativen Varianzen bezogen auf die Varianz von <span class="math notranslate nohighlight">\(X\)</span></p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.1489, 0.1362, 0.1179, 0.0841, 0.0578, 0.0492, 0.0432, 0.0366,
       0.0335, 0.0308, 0.0237, 0.0227, 0.0182, 0.0177, 0.0147, 0.0141,
       0.0132, 0.0125, 0.0102, 0.0091, 0.0089, 0.008 , 0.0077, 0.0072,
       0.007 , 0.006 , 0.0058, 0.0052, 0.0049, 0.0043, 0.0037, 0.0035])
</pre></div>
</div>
</div>
</div>
<p>Die Varianzen fallen stark ab. Kumuliert erhalten wir</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">varcum</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span>
<span class="n">varcum</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.1489, 0.2851, 0.403 , 0.4871, 0.545 , 0.5941, 0.6373, 0.6739,
       0.7074, 0.7382, 0.762 , 0.7847, 0.8029, 0.8206, 0.8353, 0.8494,
       0.8626, 0.8751, 0.8852, 0.8943, 0.9032, 0.9112, 0.9188, 0.9261,
       0.933 , 0.939 , 0.9447, 0.9499, 0.9548, 0.9591, 0.9628, 0.9664])
</pre></div>
</div>
</div>
</div>
<p>Mehr als 90 Prozent der Varianz von <span class="math notranslate nohighlight">\(X\)</span> erreichen wir schon mit sehr wenigen
Hauptkomponenten</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n90</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">varcum</span> <span class="o">&gt;</span> <span class="mf">0.9</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="n">n90</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>20
</pre></div>
</div>
</div>
</div>
<p>Wir klassifizieren den Datensatz erneut indem wir</p>
<ul class="simple">
<li><p>zunächst mit PCA die Dimension der Daten reduzieren</p></li>
<li><p>dann wieder k-Means anwenden</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">km</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="n">seed</span><span class="p">)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n90</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;PCA&#39;</span><span class="p">,</span> <span class="n">pca</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;k-Means&#39;</span><span class="p">,</span> <span class="n">km</span><span class="p">)])</span>

<span class="n">ydpca</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">ttpca</span> <span class="o">=</span> <span class="n">ytrans</span><span class="p">(</span><span class="n">ydpca</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">ydpcat</span> <span class="o">=</span> <span class="n">ttpca</span><span class="p">[</span><span class="n">ydpca</span><span class="p">]</span>
<span class="n">digiplot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="n">ndigi</span><span class="p">:],</span> <span class="n">ydpcat</span><span class="p">[</span><span class="o">-</span><span class="n">ndigi</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/02_Dimensionsreduktion_103_0.png" src="_images/02_Dimensionsreduktion_103_0.png" />
</div>
</div>
<p>Die Ergebnisse sind nur unwesentlich schlechter als im Fall ohne PCA.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ydpcat</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

           0       0.99      0.99      0.99       178
           1       0.61      0.30      0.40       182
           2       0.84      0.83      0.84       177
           3       0.87      0.85      0.86       183
           4       0.98      0.91      0.94       181
           5       0.92      0.75      0.82       182
           6       0.97      0.98      0.98       181
           7       0.82      0.98      0.89       179
           8       0.45      0.58      0.51       174
           9       0.57      0.77      0.66       180

    accuracy                           0.79      1797
   macro avg       0.80      0.79      0.79      1797
weighted avg       0.80      0.79      0.79      1797
</pre></div>
</div>
</div>
</div>
<p>Die Daten mit denen wir hier gearbeitet haben hatten dabei die
reduzierte Dimension</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1797, 20)
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="zusammenfassung">
<h2>Zusammenfassung<a class="headerlink" href="#zusammenfassung" title="Link zu dieser Überschrift">¶</a></h2>
<ul class="simple">
<li><p>mit SVD können lineare Modelle bzw. hochdimensionale Daten vereinfacht werden</p></li>
<li><p>die Berechnung von Singulärwerten und -vektoren kann mit Methoden der linearen Algebra
oder der konvexen, restringierten Optimierung erfolgen</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="01_Regression.html" title="zurück Seite">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">zurück</p>
            <p class="prev-next-title">Regression</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="03_Regularisierung.html" title="weiter Seite">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">weiter</p>
        <p class="prev-next-title">Regularisierung</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      Durch Martin Reißel<br/>
    
        &copy; Urheberrechte © 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>