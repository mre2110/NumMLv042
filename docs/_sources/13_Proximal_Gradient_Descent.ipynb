{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Überblick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei Subgradient-Descent haben wir gesehen, dass fehlende Differenzierbarkeit von $f$ kein Problem ist, solange Subgradienten existieren.\n",
    "Allerdings reduziert sich bei fehlender Glattheit die Konvergenzgeschwindigkeit.\n",
    "\n",
    "Hat $f$ zusätzlich eine spezielle Struktur, so kann man Gradient-Descent so modifizieren, dass man Konvergenzraten wie im $C^1$-Fall erhält.\n",
    "\n",
    "Wir untersuchen hier nicht restringierte Probleme, restringierte lassen sich bei\n",
    "  Proximal-Gradient-Descent\n",
    "  durch geeignete Wahl der Zielfunktion auf restringierte zurück führen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grundlagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für $\\gamma > 0$ und festes $z$ ist die Funktion\n",
    "  \\begin{equation*} \n",
    "  q(y) = \\frac{1}{2\\gamma} \\|y - z\\|_2^2\n",
    "  \\end{equation*}\n",
    "strikt konvex mit globalem Minimum $y_\\ast=z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für $f$ differenzierbar kann man deshalb einen Gradient-Descent-Schritt\n",
    "  \\begin{equation*} \n",
    "  x_{t+1} = x_t - \\gamma f'_t\n",
    "  \\end{equation*}\n",
    "auch schreiben kann als\n",
    "  \\begin{equation*} \n",
    "  x_{t+1} \n",
    "  = \n",
    "  \\underset{y}{\\mathrm{argmin}} \n",
    "  \\Big(\n",
    "  \\frac{1}{2\\gamma}\n",
    "  \\big\\|\n",
    "  y - (x_t - \\gamma f'_t)\n",
    "  \\big\\|_2^2\n",
    "  \\Big).\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir nehmen nun an, dass $f=g+h$ ist mit\n",
    "\n",
    "- $g$ differenzierbar, $L$-glatt und $\\mu$-konvex\n",
    "  \n",
    "- $h$ \"nur\" konvex\n",
    "  \n",
    "\n",
    "**Beispiel:** Bei Lasso ist\n",
    "  \\begin{equation*} \n",
    "  f(w) = \\underbrace{\\frac{1}{2m}\\|Xw-y\\|_2^2}_{g(w)}\n",
    "         + \\underbrace{\\alpha\\|w\\|_1}_{h(w)}.\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir benutzen jetzt die Darstellung von Gradient-Descent von oben,\n",
    "  behandeln $g$ \"wie üblich\" und hängen $h$ \"unbehandelt\" an\n",
    "  \\begin{align*}\n",
    "  x_{t+1} \n",
    "  &= \n",
    "  \\underset{y}{\\mathrm{argmin}} \n",
    "  \\Big(\n",
    "  \\frac{1}{2\\gamma}\n",
    "  \\big\\|\n",
    "  y - (x_t - \\gamma g'_t)\n",
    "  \\big\\|_2^2\n",
    "  + h(y)\n",
    "  \\Big)\\\\\n",
    "  &= \n",
    "  \\underset{y}{\\mathrm{argmin}} \n",
    "  \\Big(\n",
    "  \\frac{1}{2}\n",
    "  \\big\\|\n",
    "  y - (x_t - \\gamma g'_t)\n",
    "  \\big\\|_2^2\n",
    "  + \\gamma h(y)\n",
    "  \\Big).\n",
    "  \\end{align*}\n",
    "  \n",
    "Das führt uns zu folgender Notation.\n",
    "\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "- für $h$ konvex, $\\gamma > 0$ ist \n",
    "    der *Proximal-Operator* definiert als\n",
    "    \\begin{equation*} \n",
    "    \\mathrm{prox}(x) = \n",
    "      \\underset{y}{\\mathrm{argmin}} \n",
    "      \\big(\n",
    "      \\frac{1}{2} \\|y - x\\|_2^2 + \\gamma h(y)\n",
    "      \\big)\n",
    "    \\end{equation*}\n",
    "    \n",
    "- für $f=g+h$, $g$ differenzierbar, $h$ konvex, ist *Proximal-Gradient-Descent* gegeben durch\n",
    "  \\begin{equation*} \n",
    "  x_{t+1} = \\mathrm{prox}(y_{t+1}),\n",
    "  \\quad\n",
    "  y_{t+1} = x_t - \\gamma g'_t\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bemerkung:**\n",
    "\n",
    "- man kann zeigen, dass für\n",
    "    $h$ konvex, halbstetig von unten, \"proper\" ($h(x)>-\\infty$ $\\forall x$,\n",
    "    $h$ nicht identisch $+\\infty$)\n",
    "    der Proximal-Operator  wohldefiniert ist\n",
    "    (Eindeutigkeit ist klar, da\n",
    "    $\\frac{1}{2} \\|y - x\\|_2^2 + \\gamma h(y)$ strikt konvex ist, die sonstigen\n",
    "    Voraussetzungen an $h$ braucht man zum Nachweis der Existenz)\n",
    "  \n",
    "- Proximal-Gradient-Descent ähnelt Projected-Gradient-Descent, wobei \n",
    "    $\\Pi$ durch $\\mathrm{prox}$ ersetzt wird \n",
    "    (Übung: Projected-Gradient-Descent ist Spezialfall von \n",
    "    Proximal-Gradient-Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konvergenz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Konvergenzbeweise verlaufen ähnlich wie bei Projected-Gradient-Descent. Dazu benötigen wir Informationen über die Abbildungseigenschaften von $\\mathrm{prox}(\\cdot)$. \n",
    "  \n",
    "\n",
    "**Lemma:**\n",
    "\n",
    "  - ist $x_\\ast = \\underset{x}{\\mathrm{argmin}}  f(x)$, dann gilt\n",
    "    $x_\\ast = \\mathrm{prox}(x_\\ast - \\gamma g'_\\ast)$ \n",
    "    (Übung)\n",
    "    \n",
    "  - $\\|\\mathrm{prox}(y) - \\mathrm{prox}(x)\\|_2 \\leq \\|y-x\\|_2 \\quad\\forall x,y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es sei nun $g$ differenzierbar, $L$-glatt und $\\mu$-konvex.\n",
    "Wegen der $L$-Glattheit ist $g'$ Lipschitz-stetig, d.h.\n",
    "  \\begin{equation*} \n",
    "  \\|g'(y) - g'(x)\\|_2 \\leq L \\|y -x\\|_2 .\n",
    "  \\end{equation*}\n",
    "\n",
    "$\\mu$-Konvexität bedeutet\n",
    "  \\begin{equation*} \n",
    "  g(y) \\geq g(x) + g'(x)(y-x) + \\frac{\\mu}{2}\\|y -x\\|_2^2    \n",
    "  \\quad \\forall x,y,\n",
    "  \\end{equation*}\n",
    "also auch\n",
    "  \\begin{equation*} \n",
    "  g(x) \\geq g(y) + g'(y)(x-y) + \\frac{\\mu}{2}\\|x -y\\|_2^2   \n",
    "  \\quad \\forall x,y.\n",
    "  \\end{equation*}  \n",
    "Addition der beiden Ungleichungen liefert\n",
    "  \\begin{equation*} \n",
    "  g(y) + g(x) \\geq g(x) + g(y) + (g'(x)-g'(y))(y-x) + \\mu \\|y -x\\|_2^2 ,\n",
    "  \\end{equation*}\n",
    "so dass \n",
    "  \\begin{equation*} \n",
    "  \\big(g'(y)-g'(x)\\big)(y-x) \n",
    "  \\geq   \\mu \\|y -x\\|_2^2  .\n",
    "  \\end{equation*}\n",
    "  \n",
    "Mit Cauchy-Schwartz folgt\n",
    "  \\begin{equation*} \n",
    "  \\mu \\|y -x\\|_2^2  \\leq \\big(g'(y)-g'(x)\\big)(y-x) \\leq \\|g'(y) - g'(x)\\|_2\\|y -x\\|_2\n",
    "  \\end{equation*}\n",
    "  und schließlich\n",
    "  \\begin{equation*} \n",
    "  \\|g'(y) - g'(x)\\|_2 \\geq \\mu \\|y -x\\|_2 .  \n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für $\\beta\\in\\mathbb{R}$ beliebig erhalten wir\n",
    "  \\begin{equation*} \n",
    "  \\beta\\|g'(y) - g'(x)\\|_2^2 \\leq \\beta L^2 \\|y -x\\|_2^2 \\quad\\text{für}\\quad \\beta\\geq 0 \n",
    "  \\end{equation*}\n",
    "bzw.\n",
    "  \\begin{equation*} \n",
    "  \\beta\\|g'(y) - g'(x)\\|_2^2 \\leq \\beta \\mu^2 \\|y -x\\|_2^2 \\quad\\text{für}\\quad \\beta < 0 \n",
    "  \\end{equation*}  \n",
    "und somit\n",
    "  \\begin{equation*} \n",
    "  \\beta\\|g'(y) - g'(x)\\|_2^2 \n",
    "  \\leq \\max(\\beta L^2, \\beta\\mu^2) \\|y -x\\|_2^2 \n",
    "  \\quad \\forall \\beta \\in \\mathbb{R}.\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Außerdem ist\n",
    "  \\begin{align*}\n",
    "  \\big(g'(y)-g'(x)\\big)(y-x) \n",
    "  &\\geq   \\mu \\|y -x\\|_2^2  \\\\\n",
    "  &= \\frac{L}{L+\\mu} \\mu \\|y -x\\|_2^2 + \\frac{\\mu}{L+\\mu} \\mu \\|y -x\\|_2^2 \\\\\n",
    "  &\\geq \\frac{L}{L+\\mu} \\mu \\|y -x\\|_2^2 \n",
    "      + \\frac{\\mu^2}{L+\\mu} \\frac{1}{L^2}\\|g'(y) - g'(x)\\|_2^2 .\n",
    "  \\end{align*}\n",
    "Wegen $0 < \\mu \\leq L$ folgt \n",
    "  \\begin{equation*} \n",
    "  \\big(g'(y)-g'(x)\\big)(y-x) \n",
    "  \\geq\n",
    "  \\frac{1}{L +\\mu} \\|g'(y) - g'(x)\\|_2^2\n",
    "  +\\frac{L\\mu}{L+\\mu} \\|y -x\\|_2^2. \n",
    "  \\end{equation*}\n",
    "  \n",
    "Damit erhalten wir das folgende Resultat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Satz:** Ist $f=g+h$, $g$ differenzierbar, $L$-glatt und $\\mu$-konvex,\n",
    "  $h$ konvex, $x_\\ast = \\mathrm{argmin}_x f(x)$ und\n",
    "  \\begin{equation*}  \n",
    "  \\ x_{t+1} = \\mathrm{prox}(x_t - \\gamma g'_t),\n",
    "  \\end{equation*}\n",
    "  dann gilt\n",
    "  \\begin{equation*} \n",
    "  \\|x_t - x_\\ast\\|_2 \\leq Q(\\gamma)^t \\|x_0 - x_\\ast\\|_2\n",
    "  \\end{equation*}\n",
    "  mit\n",
    "  \\begin{equation*} \n",
    "  Q(\\gamma) = \\max(|1-\\gamma L|, |1- \\gamma \\mu|).\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beweis:**\n",
    "\n",
    "- es ist\n",
    "   \\begin{align*}\n",
    "   \\|x_{t+1} - x_\\ast\\|_2^2\n",
    "   & = \\|  \\mathrm{prox}(x_t - \\gamma g'_t) \n",
    "         - \\mathrm{prox}(x_\\ast - \\gamma g'_\\ast)\\|_2^2 \\\\\n",
    "   & \\leq \\|x_{t}-\\gamma g_{t}'-(x_\\ast-\\gamma g'_\\ast)\\|_{2}^{2} \\\\\n",
    "   &   =  \\|x_{t}-x_\\ast-\\gamma(g'_{t}-g'_\\ast)\\|_{2}^{2} \\\\\n",
    "   &   =  \\|x_{t}-x_\\ast\\|_{2}^{2}\n",
    "          +\\gamma^{2}\\|g_{t}'-g'_\\ast\\|_{2}^{2} \n",
    "          -2 \\gamma(g'_{t}-g'_\\ast) (x_{t}-x_\\ast)\\\\\n",
    "   & \\leq \\|x_{t}-x_\\ast\\|_{2}^{2}\n",
    "          +\\gamma^{2}\\|g_{t}'-g'_\\ast\\|_{2}^{2} \\\\\n",
    "   & \\quad-2 \\gamma\n",
    "             \\big(\n",
    "             \\frac{1}{L+\\mu}\\|g_{t}'-g_\\ast'\\|_{2}^{2}\n",
    "             +\\frac{L {\\mu}}{L+\\mu}\\|x_{t}-x_{\\ast}\\|_{2}^{2}\n",
    "             \\big) \\\\\n",
    "   & =  \\underbrace{\\Big(1-\\frac{2 \\gamma L \\mu}{L+\\mu}\\Big)\n",
    "        }_{\\alpha}\n",
    "        \\|x_{t}-x_\\ast\\|_{2}^{2}\n",
    "       +\\underbrace{\n",
    "          \\gamma \\Big(\\gamma-\\frac{2}{L+\\mu}\\Big)\n",
    "         }_{\\beta}\n",
    "         \\|g_{t}'-g'_\\ast\\|_{2}^{2}\n",
    "   \\end{align*}\n",
    "  und somit\n",
    "   \\begin{align*}\n",
    "   \\|x_{t+1} - x_\\ast\\|_2^2 \n",
    "   & \\leq \\alpha \\|x_t - x_\\ast\\|_2^2 \n",
    "        + \\max\\big(\\beta L^2, \\beta \\mu^2\\big) \\|x_t - x_\\ast\\|_2^2 \\\\\n",
    "   &  =  \\max\\big(\\alpha + \\beta L^2, \\alpha + \\beta \\mu^2\\big) \\|x_t - x_\\ast\\|_2^2 \n",
    "   \\end{align*}\n",
    "   \n",
    "- mit \n",
    "    \\begin{align*}\n",
    "    \\alpha+\\beta L^{2} \n",
    "    &=1-\\frac{2 \\gamma L{\\mu}}{L+\\mu}+\\gamma\\big(\\gamma-\\frac{2}{L+\\mu}\\big) L^{2} \\\\\n",
    "    &=1+\\gamma^{2} L^{2}-\\frac{1}{L+\\mu}\\big(2 \\gamma L \\mu+2 \\gamma L^{2}\\big) \\\\\n",
    "    &=1+\\gamma^{2} L^{2}-\\frac{2}{L+\\mu} \\gamma L(L+\\mu) \\\\\n",
    "    &=1+\\gamma^{2} L^{2}-2 \\gamma L \\\\\n",
    "    &=(1-\\gamma L)^{2}\n",
    "    \\end{align*}\n",
    "  und\n",
    "    \\begin{align*}\n",
    "    \\alpha+\\beta \\mu^{2} \n",
    "    &=1-\\frac{2 \\gamma L{\\mu}}{L+\\mu}+\\gamma\\big(\\gamma-\\frac{2}{L+\\mu}\\big) \\mu^{2} \\\\\n",
    "    &=1+\\gamma^{2} \\mu^{2}-\\frac{2\\gamma\\mu}{L+\\mu}  (L + \\mu) \\\\\n",
    "    &=(1-\\gamma\\mu)^{2}\n",
    "    \\end{align*}\n",
    "  erhalten wir schließlich\n",
    "    \\begin{equation*} \n",
    "    \\|x_{t+1} - x_\\ast\\|_2^2\n",
    "    \\leq \\max\\big((1-\\gamma L)^{2}, (1-\\gamma\\mu)^{2}\\big)\\ \\|x_t - x_\\ast\\|_2^2\n",
    "    \\end{equation*}\n",
    "    \n",
    "$\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ist $f=g+h$, $g$ differenzierbar, $L$-glatt und $\\mu$-konvex,\n",
    "  $h$ konvex, $x_\\ast = \\mathrm{argmin}_x f(x)$, dann hat\n",
    "  Proximal-Gradient-Descent die selbe asymptotische Konvergenzgeschwindigkeit\n",
    "  wie im Fall dass $f$ differenzierbar, $L$-glatt und $\\mu$-konvex ist.\n",
    "Die fehlende Glattheit von $h$ spielt, anders als bei Subgradient-Descent,\n",
    "  keine Rolle.\n",
    "\n",
    "  \n",
    "Das Berechnen von \n",
    "\n",
    "$$\n",
    "\\mathrm{prox}(x) =\\underset{y}{\\mathrm{argmin}} \n",
    "      \\big(\n",
    "      \\frac{1}{2} \\|y - x\\|_2^2 + \\gamma h(y)\n",
    "      \\big)\n",
    "$$\n",
    "\n",
    "ist in der Regel nicht trivial.\n",
    "Für einige spezielle $h$, die in der Praxis häufig auftreten, \n",
    "  lässt sich $\\mathrm{prox}(x)$ einfach bestimmen (siehe Übung).\n",
    "  \n",
    "Projected-Gradient-Descent kann als Spezialfall von Proximal-Gradient-Descent\n",
    "  interpretiert werden (siehe Übung)."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "fr",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": false,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
