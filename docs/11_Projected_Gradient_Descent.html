
<!DOCTYPE html>

<html lang="de">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Projected Gradient-Descent &#8212; Numerische Algorithmen für Maschinelles Lernen WS21/22 (Version 0.42)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Stichwortverzeichnis" href="genindex.html" />
    <link rel="search" title="Suche" href="search.html" />
    <link rel="next" title="Subgradient Descent" href="12_Subgradient_Descent.html" />
    <link rel="prev" title="Gradient Descent" href="10_Gradient_Descent.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="de">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Numerische Algorithmen für Maschinelles Lernen WS21/22 (Version 0.42)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Dieses Buch durchsuchen ..." aria-label="Dieses Buch durchsuchen ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="00_Vorwort.html">
   Numerische Algorithmen für Maschinelles Lernen (Version 0.422)
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_Regression.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_Dimensionsreduktion.html">
   Dimensionsreduktion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_Regularisierung.html">
   Regularisierung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_Background_Removal_QR.html">
   Background Removal mit TSVD
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_Klassifikation_mit_SVM.html">
   Support-Vector Klassifikation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_Neuronale_Netze.html">
   Neuronale Netze
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_Topic_Extraction.html">
   Topic Extraction, NMF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_Grundlagen_Optimierung.html">
   Grundlagen der Optimierung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_Konvexitaet.html">
   Konvexität
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_Gradient_Descent.html">
   Gradient Descent
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Projected Gradient-Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12_Subgradient_Descent.html">
   Subgradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13_Proximal_Gradient_Descent.html">
   Proximal Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14_Stochastic_Gradient_Descent.html">
   Stochastic Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15_Probabilistische_Lineare_Algebra.html">
   Probabilistische Lineare Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="99_Literatur.html">
   Weiterführende Links
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Navigation umschalten" aria-controls="site-navigation"
                title="Navigation umschalten" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Laden Sie diese Seite herunter"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/11_Projected_Gradient_Descent.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Quelldatei herunterladen" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="In PDF drucken"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Vollbildmodus"
        title="Vollbildmodus"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/mre2110/NumMLv042/master?urlpath=tree/11_Projected_Gradient_Descent.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Starten Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Inhalt
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uberblick">
   Überblick
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#grundlagen">
   Grundlagen
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#konvergenz">
   Konvergenz
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#voruberlegungen">
     Vorüberlegungen
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lipschitz-stetigkeit">
     Lipschitz-Stetigkeit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-glattheit">
     <span class="math notranslate nohighlight">
      \(L\)
     </span>
     -Glattheit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mu-konvexitat">
     <span class="math notranslate nohighlight">
      \(\mu\)
     </span>
     -Konvexität
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#projektionsoperatoren">
   Projektionsoperatoren
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kugel-in-der-2-norm">
     Kugel in der 2-Norm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nicht-negativer-kegel">
     Nicht negativer Kegel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kugel-in-der-1-norm">
     Kugel in der 1-Norm
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#beispiel-tomographie">
   Beispiel Tomographie
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#zusammenfassung">
   Zusammenfassung
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Projected Gradient-Descent</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Inhalt </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uberblick">
   Überblick
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#grundlagen">
   Grundlagen
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#konvergenz">
   Konvergenz
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#voruberlegungen">
     Vorüberlegungen
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lipschitz-stetigkeit">
     Lipschitz-Stetigkeit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#l-glattheit">
     <span class="math notranslate nohighlight">
      \(L\)
     </span>
     -Glattheit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mu-konvexitat">
     <span class="math notranslate nohighlight">
      \(\mu\)
     </span>
     -Konvexität
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#projektionsoperatoren">
   Projektionsoperatoren
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kugel-in-der-2-norm">
     Kugel in der 2-Norm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nicht-negativer-kegel">
     Nicht negativer Kegel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kugel-in-der-1-norm">
     Kugel in der 1-Norm
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#beispiel-tomographie">
   Beispiel Tomographie
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#zusammenfassung">
   Zusammenfassung
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="projected-gradient-descent">
<h1>Projected Gradient-Descent<a class="headerlink" href="#projected-gradient-descent" title="Link zu dieser Überschrift">¶</a></h1>
<div class="section" id="uberblick">
<h2>Überblick<a class="headerlink" href="#uberblick" title="Link zu dieser Überschrift">¶</a></h2>
<p>Bisher haben wir Gradient-Descent für nicht restringierte
Optimierungsprobleme untersucht.
In diesem Abschnitt modifizieren wir Gradient-Descent so, dass wir es auch auf eine
bestimmte Klasse von restringierten Optimierungsproblemen anwenden können. Dazu
kombinieren wir Gradient-Descent mit einer geeigneten Projektion auf
die zulässige Parametermenge <span class="math notranslate nohighlight">\(X\subset\mathbb{R}^d\)</span>.</p>
</div>
<div class="section" id="grundlagen">
<h2>Grundlagen<a class="headerlink" href="#grundlagen" title="Link zu dieser Überschrift">¶</a></h2>
<p>In den Beispielen oben haben wir bereits Projected-Gradient-Descent benutzt, um  für <span class="math notranslate nohighlight">\(f\in C^1(\mathbb{R}^d)\)</span>  restringierte Optimierungsproblem näherungsweise zu lösen:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
  x_\ast  &amp;= \mathrm{argmin}_{x\in X}f(x), \quad X\subset\mathbb{R}^d\\
  y_{t+1} &amp;= x_{t} - \gamma_{t} f'(x_{t})\\
  x_{t+1} &amp;= \mathrm{argmin}_{x\in X}\|x - y_{t+1}\|
  \end{align*}\]</div>
<p>Ist <span class="math notranslate nohighlight">\(x_{t+1}\)</span> überhaupt wohldefiniert (Existenz, Eindeutigkeit)?</p>
<p><strong>Satz:</strong> Ist <span class="math notranslate nohighlight">\(V\)</span> eine Hilbert-Raum, <span class="math notranslate nohighlight">\(\emptyset \neq X\subset V\)</span> konvex und abgeschlossen, dann existiert genau ein <span class="math notranslate nohighlight">\(y\in X\)</span> (die Projektion von <span class="math notranslate nohighlight">\(x\)</span> auf <span class="math notranslate nohighlight">\(X\)</span>) mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|x - y\| = \inf_{z\in X}\|z - y\|. 
  \end{equation*}\]</div>
<p>Für <span class="math notranslate nohighlight">\(x\in V\)</span> ist <span class="math notranslate nohighlight">\(y\in X\)</span> die zugehörige Projektion, genau dann wenn</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  (x-y, z-y) \leq 0 \quad \forall z\in X.
  \end{equation*}\]</div>
<p><strong>Beweis:</strong></p>
<ul>
<li><p>Existenz von <span class="math notranslate nohighlight">\(y\)</span>:</p>
<ul>
<li><p>sei <span class="math notranslate nohighlight">\(y_i\in X\)</span> eine Folge mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \lim_{i\to\infty}\|x - y_i\| = \inf_{z\in X}\|x - z\| =:d
      \end{equation*}\]</div>
</li>
<li><p>wegen <span class="math notranslate nohighlight">\(\|v\|^2 = (v,v)\)</span> gilt die Parallelogrammgleichung</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \|u+v\|^{2} + \|u-v\|^{2} = 2\big(\|u\|^{2}+\|v\|^{2}\big)
      \end{equation*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \|u-v\|^{2}=2\big(\|u\|^{2}+\|v\|^{2}\big)-\|u+v\|^{2}
      \end{equation*}\]</div>
</li>
<li><p>setzen wir nun</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      u = y_i -x, \quad v = y_j - x
      \end{equation*}\]</div>
<p>dann folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \|y_{i}-y_{j}\|^{2}=
      2\big(\|y_{i}-x\|^{2}+\|y_{j}-x\|^{2}\big)-\|y_{i}+y_{j}-2 x\|^{2}
      \end{equation*}\]</div>
</li>
<li><p>wegen</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
        \|y_{i}+y_{j}-2 x\|^{2} 
        =4\big\|\underbrace{\frac{y_{i}+y_{j}}{2}}_{\in X}-x\big\|^2 \geq 4 d^{2}
      \end{equation*}\]</div>
<p>gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
        \|y_{i}-y_{j}\|^{2}  \leq 2\big(\|y_{i}-x\|^{2}+\|y_{j}-x\|^{2}\big)-4 d^{2}
      \end{equation*}\]</div>
</li>
<li><p>andererseits gibt es für alle <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span> ein <span class="math notranslate nohighlight">\(N_\varepsilon \in  \mathbb{N}\)</span> mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \|y_{k}-x\|^{2}  \leq d^2 + \varepsilon  \quad \forall k\geq N_\varepsilon 
      \end{equation*}\]</div>
<p>und somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
        \|y_{i}-y_{j}\|^{2}  
        \leq 2\big(d^2 + \varepsilon + d^2 + \varepsilon \big)-4 d^{2}
        = 4 \varepsilon
        \quad \forall i,j\geq N_\varepsilon 
      \end{equation*}\]</div>
</li>
<li><p>damit ist <span class="math notranslate nohighlight">\(y_i\)</span> eine Cauchy-Folge</p></li>
<li><p>da <span class="math notranslate nohighlight">\(V\)</span> vollständig ist gibt es ein <span class="math notranslate nohighlight">\(y\in V\)</span> mit <span class="math notranslate nohighlight">\(y_i \xrightarrow{i\to\infty}y\)</span></p></li>
<li><p>wegen <span class="math notranslate nohighlight">\(y_i \in X\)</span>, <span class="math notranslate nohighlight">\(X\)</span> abgeschlossen, ist dann auch <span class="math notranslate nohighlight">\(y\in X\)</span></p></li>
</ul>
</li>
<li><p>Eindeutigkeit von <span class="math notranslate nohighlight">\(y\)</span>:</p>
<ul>
<li><p>für <span class="math notranslate nohighlight">\(y, \tilde{y} \in X\)</span> gelte</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      d = \inf_{z\in X}\|z - x\| = \|y - x\| = \|\tilde{y} - x\|
      \end{equation*}\]</div>
</li>
<li><p>da <span class="math notranslate nohighlight">\(X\)</span> konvex ist gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \bar{y} = \frac{y + \tilde{y}}{2} \in X 
      \end{equation*}\]</div>
<p>und mit der Parallelogrammgleichung folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
        \begin{aligned}
        \|\bar{y} - x\|^2 
        &amp;=\big\|\frac{y+\tilde{y}}{2}-x\big\|^{2} \\
        &amp;=\frac{1}{4} \| \underbrace{y-x}_{u}+\underbrace{\tilde{y}-x}_{v} \|^{2} \\
        &amp;=\frac{1}{4}\|u+v\|^{2} \\
        &amp;=\frac{1}{2}(\|u\|^{2}+\|v\|^{2})-\frac{1}{4}\|u-v\|^{2} \\
        &amp;=\frac{1}{2}(\|y-x\|^{2}+\|\tilde{y}-x\|^{2})-\frac{1}{4}\|y-\tilde{y}\|^{2} \\
        &amp;=d^{2}-\frac{1}{4}\|y-\tilde{y}\|^{2}
        \end{aligned}
      \end{equation*}\]</div>
</li>
<li><p>für <span class="math notranslate nohighlight">\(y \neq \tilde{y}\)</span> ist</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \|\bar{y} - x\|^2 &lt; d^2
      \end{equation*}\]</div>
<p>was ein Widerspruch zu <span class="math notranslate nohighlight">\(d = \inf_{z\in X}\|z - x\|\)</span> ist</p>
</li>
</ul>
</li>
<li><p>Ungleichung:</p>
<ul>
<li><p>es sei <span class="math notranslate nohighlight">\(y\in X\)</span> der eindeutige Minimierer und <span class="math notranslate nohighlight">\(d = \|y - x\| = \inf_{z\in X}\|z - x\|\)</span>:</p>
<ul>
<li><p>da <span class="math notranslate nohighlight">\(X\)</span> konvex ist gilt für alle <span class="math notranslate nohighlight">\(z\in X\)</span> und alle <span class="math notranslate nohighlight">\(\lambda \in (0,1)\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
        (1-\lambda)y + \lambda z = y + \lambda(z-y) \in X
        \end{equation*}\]</div>
</li>
<li><p>damit folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
        \|y - x\|^2 = d^2
        &amp; \leq \|y + \lambda(z-y) - x\|^2 \\
        &amp; = \|y - x\|^2 +  \lambda^2\|z - y\|^2 + 2  \lambda(y-x, z-y)
        \end{align*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
        2  \lambda(x-y, z-y) \leq \lambda^2\|z - y\|^2
        \end{equation*}\]</div>
<p>und wegen <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
        (x-y, z-y) \leq \frac{\lambda}{2}\|z - y\|^2
        \end{equation*}\]</div>
</li>
<li><p>durch Grenzübergang <span class="math notranslate nohighlight">\(\lambda \downarrow 0\)</span> erhalten wir schließlich</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
        (x-y, z-y) \leq 0 \quad \forall z\in X
        \end{equation*}\]</div>
</li>
</ul>
</li>
<li><p>es sei nun <span class="math notranslate nohighlight">\(y\in X\)</span> mit <span class="math notranslate nohighlight">\((x-y, z-y) \leq 0 \ \forall z\in X\)</span>:</p>
<ul>
<li><p>für <span class="math notranslate nohighlight">\(z\in X\)</span>, <span class="math notranslate nohighlight">\(z\neq y\)</span>, folgt dann</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
        \|x - z\|^2
        &amp;= \|x - y  +  y - z\|^2\\
        &amp;= \|x - y\|^2  +  \underbrace{\|y - z\|^2}_{&gt; 0} 
        \:\underbrace{- 2(x-y, z-y)}_{\geq 0} \\
        &amp;&gt; \|x - y\|^2
        \end{align*}\]</div>
</li>
<li><p>somit ist <span class="math notranslate nohighlight">\(y\)</span> Minimierer von <span class="math notranslate nohighlight">\(\|x - z\|\)</span>, <span class="math notranslate nohighlight">\(z\in X\)</span></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Mit diesem Ergebnis machen die folgenden Definitionen Sinn.</p>
<p><strong>Definition:</strong> Ist <span class="math notranslate nohighlight">\(V\)</span> eine Hilbert-Raum, <span class="math notranslate nohighlight">\(\emptyset \neq X\subset V\)</span> konvex und abgeschlossen, dann ist</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  d_X:V\to \mathbb{R}, \quad d_X(x) = \inf_{z\in X}\|z - x\|
  \end{equation*}\]</div>
<p>der <em>Abstand</em> von <span class="math notranslate nohighlight">\(x\)</span> zu <span class="math notranslate nohighlight">\(X\)</span> und</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \Pi_X:V\to X, \quad \Pi_X(x) = \mathrm{argmin}_{z\in X}\|z - x\|
  \end{equation*}\]</div>
<p>die <em>Projektion</em> von <span class="math notranslate nohighlight">\(x\)</span> auf <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p><strong>Bemerkung:</strong></p>
<ul class="simple">
<li><p>wir lassen bei <span class="math notranslate nohighlight">\(d,\Pi\)</span> den Index <span class="math notranslate nohighlight">\(X\)</span> weg</p></li>
<li><p><span class="math notranslate nohighlight">\(\Pi\)</span> ist wohldefiniert, wenn <span class="math notranslate nohighlight">\(V\)</span> vollständig und die Norm auf <span class="math notranslate nohighlight">\(V\)</span> strikt konvex ist</p></li>
<li><p>wird die Norm von einem Skalarprodukt induziert, dann ist sie immer strikt konvex</p></li>
<li><p>ist dies nicht der Fall, kann man einfache Gegenbeispiele konstruieren</p></li>
</ul>
<p><strong>Beispiel:</strong></p>
<ul>
<li><p>betrachte</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      V = \mathbb{R}^2, 
      \quad \|x\|_\infty = \max(|x_1|,|x_2|),
      \end{equation*}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\((V, \|\cdot\|_\infty)\)</span> ist ein Banachraum, also ein vollständiger
normierter Vektorraum</p></li>
<li><p>für die konvexe Menge</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      X = [-1,1]\times[-1,1]
      \end{equation*}\]</div>
<p>und den Punkt <span class="math notranslate nohighlight">\(x = (2,0)^T\)</span> erhalten wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      d(x) = \inf_{z\in X}\|z - x\|_\infty = \inf_{z\in X} \max(|z_1 - 2|,|z_2|)
      \end{equation*}\]</div>
</li>
<li><p>wegen <span class="math notranslate nohighlight">\(z_1 \in [-1,1]\)</span> ist für alle <span class="math notranslate nohighlight">\(z\in X\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      |z_1 - 2|\geq 1,
      \end{equation*}\]</div>
<p>s.d. <span class="math notranslate nohighlight">\(d(x) \geq 1\)</span></p>
</li>
<li><p>andererseits gilt für alle <span class="math notranslate nohighlight">\(z\)</span> mit <span class="math notranslate nohighlight">\(z_1=1\)</span> und <span class="math notranslate nohighlight">\(z_2 \in [-1,1]\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \|z - x\|_\infty = \max(|z_1 - 2|,|z_2|) = 1,
      \end{equation*}\]</div>
<p>der minimale Abstand <span class="math notranslate nohighlight">\(d(x)=1\)</span> wird also für alle diese <span class="math notranslate nohighlight">\(z\)</span> angenommen</p>
</li>
<li><p><span class="math notranslate nohighlight">\(d(x)\)</span> ist also nach wie vor wohldefiniert, <span class="math notranslate nohighlight">\(\Pi(x)\)</span> wegen der
fehlenden Eindeutigkeit dagegen nicht</p></li>
</ul>
<p>Jetzt betrachten wir einige wichtige Eigenschaften von <span class="math notranslate nohighlight">\(d\)</span> und <span class="math notranslate nohighlight">\(\Pi\)</span>.</p>
<p><strong>Lemma:</strong> Sind <span class="math notranslate nohighlight">\(V,X,d,\Pi\)</span> wie oben definiert, dann gilt:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(d:V\to \mathbb{R}\)</span> ist konvex</p></li>
<li><p><span class="math notranslate nohighlight">\(\big(y - \Pi(y), x - \Pi(y) \big) \leq 0 \quad \forall x\in X, \ \forall y\in V\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\|\Pi(y) - \Pi(x)\| \leq \|y-x\| \quad \forall x,y \in V\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\|x - \Pi(y)\|^2 + \|y - \Pi(y) \|^2 \leq \|y-x\|^2 \quad \forall x\in X, \ \forall y\in V\)</span></p></li>
</ul>
<p><strong>Beweis:</strong></p>
<ul>
<li><p>Teil 1:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x,y\in V\)</span>, <span class="math notranslate nohighlight">\(\bar{x} = \Pi(x)\)</span>, <span class="math notranslate nohighlight">\(\bar{y} = \Pi(y)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\bar{x}, \bar{y}\in X\)</span> und da <span class="math notranslate nohighlight">\(X\)</span> konvex ist gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \bar{z} = (1-\lambda)\bar{x} + \lambda \bar{y} \in X \quad \forall \lambda \in [0,1]
      \end{equation*}\]</div>
</li>
<li><p>mit <span class="math notranslate nohighlight">\(u=(1-\lambda)x + \lambda y\)</span> folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
      d\big((1-\lambda)x + \lambda y\big)
      &amp; = \inf_{z\in X} \|z - u\| \\
      &amp; \leq \|u - \bar{z}\| \\
      &amp; = \|(1-\lambda)x + \lambda y - (1-\lambda)\bar{x} - \lambda \bar{y}\| \\
      &amp; = \|(1-\lambda)(x-\bar{x}) + \lambda (y-\bar{y})\| \\
      &amp; \leq (1-\lambda)\|x-\bar{x}\| + \lambda \|y-\bar{y}\| \\
      &amp; = (1-\lambda) d(x) + \lambda d(y)
      \end{align*}\]</div>
</li>
</ul>
</li>
<li><p>Teil 2:</p>
<ul>
<li><p>nach dem Satz von oben gilt für <span class="math notranslate nohighlight">\(y\in V\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \bar{y} \in X, \quad \|y-\bar{y}\|=\inf _{x \in X}\|y-x\| 
      \quad \Leftrightarrow \quad
      (y-\bar{y}, x-\bar{y}) \leq 0 \quad \forall x \in X
      \end{equation*}\]</div>
</li>
<li><p>mit <span class="math notranslate nohighlight">\(\bar{y}=\Pi(y)\)</span> folgt die Behauptung</p></li>
</ul>
</li>
<li><p>Teil 3:</p>
<ul>
<li><p>es sei wieder <span class="math notranslate nohighlight">\(x,y\in V\)</span>, <span class="math notranslate nohighlight">\(\bar{x} = \Pi(x)\)</span>, <span class="math notranslate nohighlight">\(\bar{y} = \Pi(y)\)</span></p></li>
<li><p>nach Teil 2 gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      (x-\bar{x}, z-\bar{x}) \leq 0,
      \quad
      (y-\bar{y}, z-\bar{y}) \leq 0 
      \quad 
      \forall z \in X
      \end{equation*}\]</div>
</li>
<li><p>mit <span class="math notranslate nohighlight">\(z = \bar{y}\)</span> folgt aus der ersten Ungleichung</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      (x-\bar{x}, \bar{y}-\bar{x}) \leq 0
      \quad\Rightarrow\quad
      (\bar{x}-x, \bar{x}-\bar{y}) \leq 0
      \end{equation*}\]</div>
<p>und analog mit <span class="math notranslate nohighlight">\(z = \bar{x}\)</span> in der zweiten Ungleichung</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      (y-\bar{y}, \bar{x}-\bar{y}) \leq 0
      \end{equation*}\]</div>
</li>
<li><p>Addition der beiden Ungleichungen liefert</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      (\bar{x}-x + y-\bar{y}, \bar{x}-\bar{y}) \leq 0
      \end{equation*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      -(x-y, \bar{x}-\bar{y}) + \|\bar{x}-\bar{y}\|^2 \leq 0
      \end{equation*}\]</div>
</li>
<li><p>mit Cauchy-Schwarz folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \|\bar{x}-\bar{y}\|^2 
      \leq (x-y, \bar{x}-\bar{y})
      \leq \|x-y\| \ \|\bar{x}-\bar{y}\|
      \end{equation*}\]</div>
<p>und somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \|\bar{x}-\bar{y}\| \leq \|x-y\|
      \end{equation*}\]</div>
</li>
</ul>
</li>
<li><p>Teil 4:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\forall v,w \in V\)</span> gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \|v-w\|^2 = (v-w, v-w) = \|v\|^2 + \|w\|^2 - 2(v,w)
      \end{equation*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \|v\|^2 + \|w\|^2 - \|v-w\|^2 =  2(v,w)
      \end{equation*}\]</div>
</li>
<li><p>mit <span class="math notranslate nohighlight">\(v = x-\bar{y}\)</span>, <span class="math notranslate nohighlight">\(w = y-\bar{y}\)</span> folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \|x-\bar{y}\|^2 + \|y-\bar{y}\|^2 - \|x-y\|^2
      = 2(x-\bar{y}, y-\bar{y})
      \leq 0
      \end{equation*}\]</div>
</li>
</ul>
</li>
</ul>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
</div>
<div class="section" id="konvergenz">
<h2>Konvergenz<a class="headerlink" href="#konvergenz" title="Link zu dieser Überschrift">¶</a></h2>
<p>Wir werden jetzt analog zum einfachen Gradientenverfahren Konvergenzaussagen unter verschiedenen Voraussetzungen an <span class="math notranslate nohighlight">\(f\)</span> beweisen. Dabei werden wir sehen, dass die asymptotischen Aussagen sich nicht verändern, d.h. die Projektion auf abgeschlossene konvexe Mengen hat keinen negativen Einfluss.</p>
<div class="section" id="voruberlegungen">
<h3>Vorüberlegungen<a class="headerlink" href="#voruberlegungen" title="Link zu dieser Überschrift">¶</a></h3>
<p>Für <span class="math notranslate nohighlight">\(f\in C^1(\mathbb{R}^d)\)</span>, <span class="math notranslate nohighlight">\(X\subset\mathbb{R}^d\)</span>
konvex und abgeschlossen,
wenden wir auf das  restringierte Optimierungsproblem</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  x_\ast  = \mathrm{argmin}_{x\in X}f(x)
  \end{equation*}\]</div>
<p>Projected-Gradient-Descent</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  y_{t+1} = x_{t} - \gamma f'(x_{t}),
  \quad
  x_{t+1} = \Pi(y_{t+1})
  \end{equation*}\]</div>
<p>mit konstanter Schrittweite <span class="math notranslate nohighlight">\(\gamma\)</span> an, wobei <span class="math notranslate nohighlight">\(\Pi\)</span> die
Projektion in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> auf <span class="math notranslate nohighlight">\(X\)</span> bezüglich <span class="math notranslate nohighlight">\((\cdot,\cdot)_2\)</span> ist.
Wegen <span class="math notranslate nohighlight">\(y_{t+1} = x_t - \gamma f'_t\)</span>  gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|y_{t+1}-x_{*}\|_{2}^{2} 
  =\|x_{t}-x_{*}\|_{2}^{2}+\gamma^{2}\|f_{t}^{\prime}\|_{2}^{2}-2
  \gamma f_{t}^{\prime}(x_{t}-x_{*})
  \end{equation*}\]</div>
<p>und somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f'_{t}\left(x_{t}-x_{*}\right) 
  =\frac{1}{2 \gamma}\left(\gamma^{2}\left\|f_{t}^{\prime}\right\|_{2}^{2}
  +\left\|x_{t}-x_{*}\right\|_{2}^{2}-\left\|y_{t+1}-x_{*}\right\|_{2}^{2}\right).
  \end{equation*}\]</div>
<p>Der einzige Unterschied zu Gradient-Descent ist <span class="math notranslate nohighlight">\(y_{t+1}\)</span> statt <span class="math notranslate nohighlight">\(x_{t+1}\)</span>
im letzten Term auf der rechten Seite.</p>
<p>Um diesen Term zu bearbeiten, greifen wir auf die Eigenschaften
der Projektion <span class="math notranslate nohighlight">\(\Pi\)</span> zurück.
Für <span class="math notranslate nohighlight">\(\Pi\)</span> gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|\Pi(y) - \Pi(x)\| \leq \|y-x\| \quad \forall x,y \in V
  \end{equation*}\]</div>
<p>bzw.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \Pi(x_\ast) = x_\ast
  \end{equation*}\]</div>
<p>und somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|x_{t+1}-x_{*}\|_{2}
   = \|\Pi(y_{t+1}) - \Pi(x_\ast)\|_2
  \leq  \|y_{t+1}-x_{*}\|_{2},
  \end{equation*}\]</div>
<p>bzw.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f'_{t}\left(x_{t}-x_{*}\right) 
  \leq\frac{1}{2 \gamma}\left(\gamma^{2}\left\|f_{t}^{\prime}\right\|_{2}^{2}
  +\left\|x_{t}-x_{*}\right\|_{2}^{2}-\left\|x_{t+1}-x_{*}\right\|_{2}^{2}\right).
  \end{equation*}\]</div>
<p>Wir erhalten also fast das selbe Ergebnis wie bei Gradient-Descent, nur
dass jetzt das „<span class="math notranslate nohighlight">\(=\)</span>“ durch „<span class="math notranslate nohighlight">\(\leq\)</span>“ ersetzt wurde.</p>
<p>Ist <span class="math notranslate nohighlight">\(f\)</span> konvex, dann gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f(y) \geq f(x) + f'(x)(y-x)
  \end{equation*}\]</div>
<p>und mit <span class="math notranslate nohighlight">\(y=x_\ast\)</span>, <span class="math notranslate nohighlight">\(x=x_t\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_\ast \geq f_t + f'_t (x_\ast-x_t)
  \end{equation*}\]</div>
<p>bzw.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
  0  \leq f_t - f_\ast
  &amp; \leq f'_t(x_t - x_\ast)\\
  &amp; = \frac{1}{2 \gamma}\left(\gamma^{2}\left\|f_{t}^{\prime}\right\|_{2}^{2}
  +\left\|x_{t}-x_{*}\right\|_{2}^{2}-\left\|y_{t+1}-x_{*}\right\|_{2}^{2}\right)\\
  &amp; \leq\frac{1}{2 \gamma}\left(\gamma^{2}\left\|f_{t}^{\prime}\right\|_{2}^{2}
  +\left\|x_{t}-x_{*}\right\|_{2}^{2}-\left\|x_{t+1}-x_{*}\right\|_{2}^{2}\right).
  \end{align*}\]</div>
</div>
<div class="section" id="lipschitz-stetigkeit">
<h3>Lipschitz-Stetigkeit<a class="headerlink" href="#lipschitz-stetigkeit" title="Link zu dieser Überschrift">¶</a></h3>
<p><span class="math notranslate nohighlight">\(f\in C^1(\mathbb{R}^d)\)</span> sei konvex und zusätzlich Lipschitz-Stetigkeit
mit Konstante <span class="math notranslate nohighlight">\(L_f\)</span>, also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|f(y) - f(x)\| \leq L_f \|y - x\| 
  \end{equation*}\]</div>
<p>was äquivalent ist zu</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|f'(x)\| \leq L_f .
  \end{equation*}\]</div>
<p>Wie bei Gradient-Descent folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
  \sum_{t=0}^{T-1} (f_t - f_\ast)
  &amp;\leq 
  \sum_{t=0}^{T-1} f'_t(x_t - x_\ast)\\
  &amp;\leq \frac{\gamma}{2}T L_f^2 
  + \frac{1}{2\gamma} 
  \big( 
  \underbrace{\|x_{0}-x_{*}\|_{2}^{2}}_{e_0^2}
  -
  \underbrace{\|x_{T}-x_{*}\|_{2}^{2}}_{\geq 0}
  \big)\\
  &amp;\leq \frac{\gamma T L_f^2}{2} + \frac{e_0^2}{2\gamma}.
  \end{align*}\]</div>
<p><strong>Satz:</strong> <span class="math notranslate nohighlight">\(f:\mathbb{R}^d\to \mathbb{R}\)</span>, konvex, <span class="math notranslate nohighlight">\(C^1\)</span>, L-stetig mit Konstante <span class="math notranslate nohighlight">\(L_f\)</span>
und es existiere <span class="math notranslate nohighlight">\(x_\ast = \mathrm{argmin}_{x\in\mathbb{R}^d}f(x)\)</span>.
Mit <span class="math notranslate nohighlight">\(\gamma = \frac{c}{T^\omega}\)</span>, <span class="math notranslate nohighlight">\(\omega\in(0,1)\)</span>, gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
  \min_{t=0,\ldots,T-1}(f_t - f_\ast)
  &amp;\leq \frac{1}{T} \sum_{t=0}^{T-1} (f_t - f_\ast)\\
  &amp;= \mathcal{O}\Big(\big(\frac{1}{T}\big)^{\min(\omega,1-\omega)}\Big)
  \quad 
  \text{für}
  \quad
  T\to\infty.
  \end{align*}\]</div>
<p>Die optimale Ordnung ist <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> für <span class="math notranslate nohighlight">\(\omega=\frac{1}{2}\)</span>.
Mit <span class="math notranslate nohighlight">\(e_0 = \|x_0 - x_\ast\|\)</span>, <span class="math notranslate nohighlight">\(\gamma = \frac{e_0}{L_f\sqrt{T}}\)</span> gilt außerdem</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \min_{t=0,\ldots,T-1}(f_t - f_\ast)
  \leq \frac{1}{T} \sum_{t=0}^{T-1} (f_t - f_\ast)
  \leq \frac{L_f e_0}{\sqrt{T}}.
  \end{equation*}\]</div>
</div>
<div class="section" id="l-glattheit">
<h3><span class="math notranslate nohighlight">\(L\)</span>-Glattheit<a class="headerlink" href="#l-glattheit" title="Link zu dieser Überschrift">¶</a></h3>
<p>Wir leiten zuerst eine neue Version des Descent-Lemmas her.
Dazu starten wir mit der <span class="math notranslate nohighlight">\(L\)</span>-Glattheit von <span class="math notranslate nohighlight">\(f\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_{t+1} \leq f_{t}+f_{t}^{\prime}(x_{t+1}-x_{t})+\frac{L}{2}\|x_{t+1}-   x_{t}\|_{2}^{2}
  \end{equation*}\]</div>
<p>benutzen</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  y_{t+1} = x_t - \gamma f'_t
  \end{equation*}\]</div>
<p>bzw.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f'_t = \frac{1}{\gamma}(x_t - y_{t+1})
  \end{equation*}\]</div>
<p>und erhalten</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_{t+1} 
  \leq f_{t}
    - \frac{1}{\gamma}
    (\underbrace{y_{t+1} - x_t}_u)^T
    (\underbrace{x_{t+1} - x_t}_v) 
    + \frac{L}{2}\|x_{t+1}-   x_{t}\|_{2}^{2}.
  \end{equation*}\]</div>
<p>Wegen <span class="math notranslate nohighlight">\(2u^Tv = 2(u,v)_2 = \|u\|_2^2 + \|v\|_2^2 - \|u-v\|_2^2\)</span> folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
  f_{t+1} 
  &amp; \leq f_{t}
    - \frac{1}{2 \gamma}
    \big(
    \|y_{t+1}-x_{t}\|_{2}^{2}
    + \|x_{t+1}-x_{t}\|_{2}^{2}
    - \|y_{t+1}-x_{t+1}\|_{2}^{2}
    \big)      
    +\frac{L}{2}\|x_{t+1}-x_{t}\|_{2}^{2} \\
  &amp;= f_{t}
      -\frac{1}{2 \gamma}
      \big(
      \gamma^{2}\|f_{t}^{\prime}\|_{2}^{2}
      +\|x_{t+1}-x_{t}\|_{2}^{2}
      -\|y_{t+1}-x_{t+1}\|_{2}^{2}
      \big) 
      +\frac{L}{2}\|x_{t+1}-x_{t}\|_{2}^{2} \\
  &amp;= f_{t}
      -\frac{\gamma}{2}\|f'_{t}\|_{2}^{2} 
      +\frac{1}{2 \gamma}\|y_{t+1}-x_{t+1}\|_{2}^{2} 
      +\frac{1}{2}\big(L-\frac{1}{\gamma}\big)\|x_{t+1}-x_{t}\|_{2}^{2}.
  \end{align*}\]</div>
<p>Für <span class="math notranslate nohighlight">\(0&lt;\gamma \leq \frac{1}{L}\)</span> ist <span class="math notranslate nohighlight">\(L-\frac{1}{\gamma}\leq 0\)</span> und wir erhalten das folgende Ergebnis.</p>
<p><strong>Descent-Lemma:</strong> Ist <span class="math notranslate nohighlight">\(f:\mathbb{R}^d\to \mathbb{R}\)</span> <span class="math notranslate nohighlight">\(L\)</span>-glatt und
<span class="math notranslate nohighlight">\(0 &lt; \gamma \leq \frac{1}{L}\)</span>, dann gilt für Projected-Gradient-Descent</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_{t+1} 
  \leq
  f_{t}
      -\frac{\gamma}{2}\|f'_{t}\|_{2}^{2} 
      +\frac{1}{2 \gamma}\|y_{t+1}-x_{t+1}\|_{2}^{2} .
  \end{equation*}\]</div>
<p><strong>Bemerkung:</strong></p>
<ul>
<li><p>bei Gradient-Descent war</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f_{t+1} 
    \leq f_{t}-\beta\|f_{t}^{\prime}\|_2^{2},
    \quad
    \beta = \gamma\big(1-\frac{\gamma L}{2}\big),
    \end{equation*}\]</div>
<p>so dass die <span class="math notranslate nohighlight">\(f_t\)</span> bei <span class="math notranslate nohighlight">\(0&lt;\gamma &lt; 2/L\)</span> monoton fallend waren</p>
</li>
<li><p>bei Projected-Gradient-Descent haben wir auf der rechten Seite den
nicht-negativen Zusatzterm</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    \frac{1}{2 \gamma}\|y_{t+1}-x_{t+1}\|_{2}^{2}, 
    \end{equation*}\]</div>
<p>so dass wir zunächst nichts über die Monotonie der <span class="math notranslate nohighlight">\(f_t\)</span> aussagen können</p>
</li>
<li><p>man kann aber trotzdem zeigen, dass die <span class="math notranslate nohighlight">\(f_t\)</span> auch hier
(wenn <span class="math notranslate nohighlight">\(\gamma\)</span> kurz genug ist) monoton fallen (Übung)</p></li>
</ul>
<p>Aus den Vorüberlegungen wissen wir (<span class="math notranslate nohighlight">\(f\)</span> konvex)</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
  0  \leq f_t - f_\ast
  &amp; \leq f'_t(x_t - x_\ast)\\
  &amp; = \frac{1}{2 \gamma}(\gamma^{2}\|f_{t}^{\prime}\|_{2}^{2}
  +\|x_{t}-x_{*}\|_{2}^{2}-\|y_{t+1}-x_{*}\|_{2}^{2}).
  \end{align*}\]</div>
<p>Laut Descent-Lemma ist</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|f'_{t}\|_{2}^{2} 
  \leq \frac{2}{\gamma}
  \big(
  f_{t} - f_{t+1} 
      +\frac{1}{2 \gamma}\|y_{t+1}-x_{t+1}\|_{2}^{2}
  \big)
  \end{equation*}\]</div>
<p>und oben eingesetzt folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_t - f_\ast
  \leq 
  f_{t} - f_{t+1} 
  +\frac{1}{2 \gamma}
  \big(
  \|y_{t+1}-x_{t+1}\|_{2}^{2} + \|x_{t}-x_{*}\|_{2}^{2}-\|y_{t+1}-x_{*}\|_{2}^{2}
  \big).
  \end{equation*}\]</div>
<p>Wegen der Eigenschaft</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|x - \Pi(y)\|^2 + \|y - \Pi(y) \|^2 \leq \|y-x\|^2 
  \quad \forall x\in X, \ \forall y\in V\end{equation*}\]</div>
<p>der Projektion <span class="math notranslate nohighlight">\(\Pi\)</span> und <span class="math notranslate nohighlight">\(x_\ast \in X\)</span> folgt
mit <span class="math notranslate nohighlight">\(y = y_{t+1}\)</span>, <span class="math notranslate nohighlight">\(x=x_\ast\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
  \|y_{t+1}-x_{t+1}\|_{2}^2
  &amp;= \|y_{t+1} - \Pi(y_{t+1})\|_2^2 \\
  &amp;\leq \|y_{t+1} - x_\ast\|_2^2  - \|x_\ast - \Pi(y_{t+1})\|_2^2 \\
  &amp;  =  \|y_{t+1} - x_\ast\|_2^2  - \|x_{t+1} - x_\ast\|_2^2 
  \end{align*}\]</div>
<p>so dass</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_t - f_\ast
  \leq 
  f_{t} - f_{t+1} 
  +\frac{1}{2 \gamma}
  \big(
  \|x_{t}-x_{*}\|_{2}^{2}-\|x_{t+1}-x_{*}\|_{2}^{2}
  \big).
  \end{equation*}\]</div>
<p>Durch Summation erhalten wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \sum_{t=0}^{T-1}(f_t - f_\ast)
  \leq
  f_0 - f_T   
  +\frac{1}{2 \gamma}
  \big(
  \|x_{0}-x_{*}\|_{2}^{2}-\|x_{T}-x_{*}\|_{2}^{2}
  \big).
  \end{equation*}\]</div>
<p>Benutzen wir jetzt noch</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_0 - f_T = (f_0 - f_\ast) - (f_T - f_\ast),
  \quad
  \|x_{T}-x_{*}\|_{2}^{2} \geq 0,
  \end{equation*}\]</div>
<p>dann ist</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \sum_{t=1}^{T}(f_t - f_\ast)
  \leq \frac{1}{2 \gamma} \|x_{0}-x_{*}\|_{2}^{2}.
  \end{equation*}\]</div>
<p>Nach dem Descent-Lemma und der daran anschließenden Bemerkung gilt
<span class="math notranslate nohighlight">\(f_{t+1} \leq f_t\)</span> und somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_T - f_\ast
  \leq\frac{1}{T}\sum_{t=1}^{T}(f_t - f_\ast)
  \leq \frac{1}{2 \gamma T} \|x_{0}-x_{*}\|_{2}^{2}.
  \end{equation*}\]</div>
<p><strong>Satz:</strong> <span class="math notranslate nohighlight">\(f:\mathbb{R}^d\to \mathbb{R}\)</span>, konvex, <span class="math notranslate nohighlight">\(L\)</span>-glatt mit Konstante <span class="math notranslate nohighlight">\(L\)</span>
und es existiere <span class="math notranslate nohighlight">\(x_\ast = \mathrm{argmin}_{x\in X}f(x)\)</span>.
Ist  <span class="math notranslate nohighlight">\(0 &lt; \gamma \leq \frac{1}{L}\)</span> dann gilt für Projected-Gradient-Descent</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_{T}-f_\ast 
  \leq \frac{1}{2 \gamma T} \|x_{0}-x_{*}\|_{2}^{2}
  = \mathcal{O}\big( \frac{1}{T} \big)
  \quad \text{für}\quad  T\to\infty .
  \end{equation*}\]</div>
</div>
<div class="section" id="mu-konvexitat">
<h3><span class="math notranslate nohighlight">\(\mu\)</span>-Konvexität<a class="headerlink" href="#mu-konvexitat" title="Link zu dieser Überschrift">¶</a></h3>
<p>Ist <span class="math notranslate nohighlight">\(f\)</span> <span class="math notranslate nohighlight">\(\mu\)</span>-konvex mit <span class="math notranslate nohighlight">\(\mu\geq 0\)</span>, dann gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_\ast \geq f_t + f'_t(x_\ast-x_t) + \frac{\mu}{2} \|x_\ast-x_t\|_2^2
  \end{equation*}\]</div>
<p>bzw.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_t - f_\ast \leq f'_t(x_t - x_\ast)   - \frac{\mu}{2} \|x_t - x_\ast\|_2^2.
  \end{equation*}\]</div>
<p>Aus den Vorüberlegungen wissen wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f'_{t}\left(x_{t}-x_{*}\right) 
  =\frac{1}{2 \gamma}\left(\gamma^{2}\left\|f_{t}^{\prime}\right\|_{2}^{2}
  +\left\|x_{t}-x_{*}\right\|_{2}^{2}-\left\|y_{t+1}-x_{*}\right\|_{2}^{2}\right).
  \end{equation*}\]</div>
<p>Wegen</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|x - \Pi(y)\|^2 + \|y - \Pi(y)\|^2 \leq \|x - y\|^2
  \quad
  \forall x\in X
  \quad 
  \forall y\in V
  \end{equation*}\]</div>
<p>folgt mit <span class="math notranslate nohighlight">\(y = y_{t+1}\)</span>, <span class="math notranslate nohighlight">\(x=x_\ast\)</span>, <span class="math notranslate nohighlight">\(\Pi(y_{t+1})=x_{t+1}\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|x_\ast - x_{t+1}\|_{2}^2 + \|y_{t+1} - x_{t+1}\|_{2}^2
  \leq  \|x_\ast -  y_{t+1}\|_{2}^2,
  \end{equation*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  - \|y_{t+1} - x_\ast\|_{2}^2 
  \leq - \|x_{t+1} - x_\ast\|_{2}^2 - \|y_{t+1} - x_{t+1}\|_{2}^2.
  \end{equation*}\]</div>
<p>Oben eingesetzt erhalten wir nun</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
  f_t - f_\ast 
  &amp;\leq f'_t(x_t - x_\ast) - \frac{\mu}{2} \|x_t - x_\ast\|_2^2\\
  &amp;\leq \frac{1}{2 \gamma}
    \big(
    \gamma^{2} \|f'_{t}\|_{2}^{2}  
    + \|x_{t}-x_{*}\|_{2}^{2}
    - \|x_{t+1}-x_{*}\|_{2}^{2}
    - \|y_{t+1} - x_{t+1}\|_{2}^2
    \big)
    \\
    &amp;\quad - \frac{\mu}{2} \|x_t - x_\ast\|_2^2,
  \end{align*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
  2\gamma (f_t - f_\ast)
  &amp; \leq
  \gamma^{2} \|f'_{t}\|_{2}^{2}  
    + (1-\gamma\mu)\|x_{t}-x_{*}\|_{2}^{2}
    \\
    &amp;\quad
    - \|x_{t+1}-x_{*}\|_{2}^{2}
    - \|y_{t+1} - x_{t+1}\|_{2}^2
  \end{align*}\]</div>
<p>bzw.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
  \|x_{t+1}-x_{*}\|_{2}^{2}
  &amp;\leq
  2\gamma (f_\ast - f_t)
  + \gamma^{2} \|f'_{t}\|_{2}^{2}
  - \|y_{t+1} - x_{t+1}\|_{2}^2
  \\
  &amp;\quad
  + (1-\gamma\mu)\|x_{t}-x_{*}\|_{2}^{2}.
  \end{align*}\]</div>
<p>Die ersten drei Terme auf der rechten Seite schätzen wir wieder mit dem Descent-Lemma ab.
Wegen</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_{t+1} 
  \leq
  f_{t}
      -\frac{\gamma}{2}\|f'_{t}\|_{2}^{2} 
      +\frac{1}{2 \gamma}\|y_{t+1}-x_{t+1}\|_{2}^{2} 
  \end{equation*}\]</div>
<p>ist</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_\ast - f_t
  \leq
  f_{t+1} - f_t
  \leq
  -\frac{\gamma}{2}\|f'_{t}\|_{2}^{2} 
      +\frac{1}{2 \gamma}\|y_{t+1}-x_{t+1}\|_{2}^{2} 
  \end{equation*}\]</div>
<p>und somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|x_{t+1}-x_{*}\|_{2}^{2}
  \leq (1-\gamma\mu)\|x_{t}-x_{*}\|_{2}^{2}.
  \end{equation*}\]</div>
<p>Analog zu Gradient-Descent haben wir damit folgendes Ergebnis gezeigt.</p>
<p><strong>Satz:</strong> <span class="math notranslate nohighlight">\(f:\mathbb{R}^d\to \mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(\mu\)</span>-konvex mit <span class="math notranslate nohighlight">\(\mu&gt;0\)</span>, <span class="math notranslate nohighlight">\(L\)</span>-glatt
mit Konstante <span class="math notranslate nohighlight">\(L\)</span> und es existiere <span class="math notranslate nohighlight">\(x_\ast = \mathrm{argmin}_{x\in X}f(x)\)</span>.
Für  <span class="math notranslate nohighlight">\(0 &lt; \gamma \leq \frac{1}{L}\)</span> folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|x_{t+1}-x_{*}\|_{2}^2 \leq \rho \|x_{t}-x_{*}\|_{2}^2
  \end{equation*}\]</div>
<p>und</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  f_T - f_\ast \leq \frac{L}{2} \rho^T \|x_{0}-x_{*}\|_{2}^2
  \end{equation*}\]</div>
<p>mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \rho = 1-\gamma\mu \in [0,1) .
  \end{equation*}\]</div>
</div>
</div>
<div class="section" id="projektionsoperatoren">
<h2>Projektionsoperatoren<a class="headerlink" href="#projektionsoperatoren" title="Link zu dieser Überschrift">¶</a></h2>
<p>In den vorherigen Abschnitten haben wir gesehen, dass wir für Projected-Gradient-Descent
exakt die selben asymptotischen Resultate erhalten wie bei gewöhnlichem Gradient-Descent.
Das sieht zunächst sehr angenehm aus,
für den praktischen Einsatz muss man allerdings bedenken, dass in jedem
Schritt eine Projektion</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  x_{t+1} = \Pi_X(y_{t+1}),
  \quad \Pi_X(x) = \mathrm{argmin}_{z\in X}\|z - x\|
  \end{equation*}\]</div>
<p>auf die abgeschlossene, konvexe Menge <span class="math notranslate nohighlight">\(X\)</span> zu berechnen ist.</p>
<p>Das Berechnen von <span class="math notranslate nohighlight">\(\Pi(x)\)</span> ist also selbst ein restringiertes konvexes
Optimierungsproblem mit einer  „einfachen“ konvexen Zielfunktion
<span class="math notranslate nohighlight">\(x\to\|z - x\|\)</span>.
Für allgemeine abgeschlossene und konvexe Menge <span class="math notranslate nohighlight">\(X\)</span> kann die Lösung
dieses Problems extrem schwierig werden, für spezielle <span class="math notranslate nohighlight">\(X\)</span>, die in der Praxis häufig auftauchen, kann
man <span class="math notranslate nohighlight">\(\Pi_X\)</span> dagegen explizit angeben.</p>
<p>Wir untersuchen einige <span class="math notranslate nohighlight">\(X\subset \mathbb{R}^d\)</span> nun genauer.
Dabei betrachten wir jeweils Projektionen bezüglich des euklidischen
Skalarprodukts <span class="math notranslate nohighlight">\((\cdot,\cdot)_2\)</span>.</p>
<div class="section" id="kugel-in-der-2-norm">
<h3>Kugel in der 2-Norm<a class="headerlink" href="#kugel-in-der-2-norm" title="Link zu dieser Überschrift">¶</a></h3>
<ul class="simple">
<li><p>wir betrachten</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
X 
= \bar{B}_r(0)_{\|\cdot\|_2}
= \big\{ x \ \big| \ x\in\mathbb{R}^d,\ \|x\|_2\leq r \big\}
\end{equation*}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X\)</span> ist offensichtlich konvex und abgeschlossen</p></li>
<li><p>die Projektion auf <span class="math notranslate nohighlight">\(X\)</span> ist</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\Pi_X(x) =
\begin{cases}
x &amp; \|x\|_2 \leq  r \\
\frac{x}{\|x\|_2} r &amp; \|x\|_2 &gt; r 
\end{cases}
\end{equation*}\]</div>
<ul class="simple">
<li><p>mit <span class="math notranslate nohighlight">\(\bar{x} = \Pi_X(x)\)</span> ist zu zeigen, dass</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
(x - \bar{x}, z - \bar{x})_2 \leq 0 \quad \forall z \in X
\end{equation*}\]</div>
<ul class="simple">
<li><p>für <span class="math notranslate nohighlight">\(x\in X\)</span> ist <span class="math notranslate nohighlight">\(\bar{x}=x\)</span>, also</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
(x - \bar{x}, z - \bar{x})_2 
= (x - x, z - x)_2 
= 0 \quad \forall z \in X
\end{equation*}\]</div>
<ul class="simple">
<li><p>für <span class="math notranslate nohighlight">\(x\notin X\)</span>, also <span class="math notranslate nohighlight">\(\|x\|_2&gt;r\)</span>, ist <span class="math notranslate nohighlight">\(\bar{x}=\frac{x}{\|x\|_2} r\)</span> und somit</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(x - \bar{x}, z - \bar{x})_2 
&amp;   =  \big( x - \frac{x}{\|x\|_2} r, z - \frac{x}{\|x\|_2} r \big)_2 \\
&amp;   =  (x,z)_2 + r^2 - \big( x , \frac{x}{\|x\|_2} r \big)_2 - \big(\frac{x}{\|x\|_2} r , z\big)_2 \\
&amp;   =  (x,z)_2 + r^2 - \|x\|_2 r - \frac{r}{\|x\|_2} (x, z)_2 \\
&amp;   =  \big(\underbrace{1 - \frac{r}{\|x\|_2}}_{&gt;0}\big)(x,z)_2 + r^2 - r \|x\|_2 \\
&amp; \leq \big(1 - \frac{r}{\|x\|_2}\big)\|x\|_2 \underbrace{\|z\|_2}_{\leq r} + r^2 - r \|x\|_2 \\
&amp; \leq r \|x\|_2 -  r^2 + r^2 - r \|x\|_2\\
&amp;   =  0
\end{align*}\]</div>
</div>
<div class="section" id="nicht-negativer-kegel">
<h3>Nicht negativer Kegel<a class="headerlink" href="#nicht-negativer-kegel" title="Link zu dieser Überschrift">¶</a></h3>
<ul>
<li><p>der Kegel</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    X
    = \big\{ x \ \big| \ x\in\mathbb{R}^d,\ x_i \geq 0, \  i=1,\ldots,d \big\}
    \end{equation*}\]</div>
<p>ist konvex und abgeschlossen</p>
</li>
<li><p>die Projektion auf <span class="math notranslate nohighlight">\(X\)</span> ist definiert durch</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    \bar{x} = \Pi_X(x),
    \quad
    \bar{x}_i
    =
    \begin{cases}
    x_i &amp; x_i \geq 0 \\
     0  &amp; x_i &lt; 0
    \end{cases}
    \end{equation*}\]</div>
</li>
<li><p>für <span class="math notranslate nohighlight">\(z\in X\)</span> folgt dann</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    (x - \bar{x}, z - \bar{x})_2 
    &amp;   =  \sum_{i=1}^d (x_i - \bar{x}_i) (z_i - \bar{x}_i) \\
    &amp;   =  \sum_{x_i&lt;0} \underbrace{x_i}_{&lt;0}\ \underbrace{z_i}_{\geq 0} \\
    &amp; \leq  0
    \end{align*}\]</div>
</li>
</ul>
</div>
<div class="section" id="kugel-in-der-1-norm">
<h3>Kugel in der 1-Norm<a class="headerlink" href="#kugel-in-der-1-norm" title="Link zu dieser Überschrift">¶</a></h3>
<ul>
<li><p>wir betrachten</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  X_r 
  = B_r(0)_{\|\cdot\|_1}
  = \big\{ x \ \big| \ x\in\mathbb{R}^d,\ \|x\|_1 = \sum_{i=1}^d |x_i|\leq r \big\}
  \end{equation*}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(X_r\)</span> ist damit konvex und abgeschlossen und die Projektion <span class="math notranslate nohighlight">\(\Pi\)</span> auf <span class="math notranslate nohighlight">\(X_r\)</span>
bezüglich der Norm <span class="math notranslate nohighlight">\(\|\cdot\|_2\)</span> ist wohldefiniert:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \Pi(v) = \mathrm{argmin}_{\|y\|_1\leq r} \|v - y\|_2
  \end{equation*}\]</div>
</li>
</ul>
<ul>
<li><p>wegen <span class="math notranslate nohighlight">\(\Pi(v) = v\)</span> für <span class="math notranslate nohighlight">\(\|v\|_1\leq r\)</span> reicht es, wenn wir ab jetzt Vektoren</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  v\in\mathbb{R}^d,
  \quad
  \|v\|_1 &gt; r
  \end{equation*}\]</div>
<p>betrachten</p>
</li>
</ul>
<ul>
<li><p>für diese gilt <span class="math notranslate nohighlight">\(\|\Pi(v)\|_1 = r\)</span>, d.h. die Projektion liegt auf dem Rand von <span class="math notranslate nohighlight">\(X_r\)</span>:</p>
<ul>
<li><p>Annahme: <span class="math notranslate nohighlight">\(\|\Pi(v)\|_1 &lt; r\)</span></p></li>
<li><p>wegen <span class="math notranslate nohighlight">\(\|v\|_1 &gt; r &gt; \|\Pi(v)\|_1\)</span> gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \lambda = \frac{r - \|\Pi(v)\|_1}{\|v\|_1 -\|\Pi(v)\|_1} \in (0,1)
      \end{equation*}\]</div>
</li>
<li><p>für</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      x = (1-\lambda) \Pi(v) + \lambda v 
      \end{equation*}\]</div>
<p>erhalten wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
      \|x\|_1
      &amp;\leq (1-\lambda) \|\Pi(v)\|_1 + \lambda \|v\|_1 \\
      &amp; = \|\Pi(v)\|_1 + \lambda \big(\|v\|_1 -\|\Pi(v)\|_1\big)\\
      &amp; = \|\Pi(v)\|_1 + \frac{r - \|\Pi(v)\|_1}{\|v\|_1 -\|\Pi(v)\|_1} \big(\|v\|_1 -\|\Pi(v)\|_1\big)\\
      &amp; = r
      \end{align*}\]</div>
</li>
<li><p>andererseits gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      x - v = (1-\lambda)\big(\Pi(v)-v\big), \quad \lambda\in(0,1) 
      \end{equation*}\]</div>
<p>und somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \|x - v\|_2 = (1-\lambda) \|\Pi(v)-v\|_2 &lt; \|\Pi(v)-v\|_2,
      \end{equation*}\]</div>
<p>was ein Widerspruch zu</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      \Pi(v) = \mathrm{argmin}_{\|y\|_1\leq r} \|v - y\|_2
      \end{equation*}\]</div>
<p>ist</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>wir werden jetzt die Menge der zu betrachtenden Vektoren <span class="math notranslate nohighlight">\(v\)</span> weiter einschränken</p></li>
<li><p>ist <span class="math notranslate nohighlight">\(L:\mathbb{R}^d\to\mathbb{R}^d\)</span> eine lineare Abbildung mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|Lw\|_1 = \|w\|_1,
  \quad
  \|Lw\|_2 = \|w\|_2
  \quad \forall w\in \mathbb{R}^d
  \end{equation*}\]</div>
<p>dann ist <span class="math notranslate nohighlight">\(L\)</span> umkehrbar und es gilt für <span class="math notranslate nohighlight">\(x = \Pi(w)\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  Lx = \Pi(Lw)
  \end{equation*}\]</div>
<p>bzw.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  x = L^{-1}\Pi(Lw)
  \end{equation*}\]</div>
<p>denn</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
  \| Lw - Lx\|_2
  &amp;= \| L(w - x)\|_2 \\
  &amp;= \| w - x\|_2 \\
  &amp;= \min_{\|y\|_1\leq r} \|w - y\|_2 \\
  &amp;= \min_{\|y\|_1\leq r} \|Lw - Ly\|_2 \\
  &amp;= \min_{\|L^{-1}z\|_1\leq r} \|Lw - z\|_2 \\
  &amp;= \min_{\|z\|_1\leq r} \|Lw - z\|_2 \\
  \end{align*}\]</div>
</li>
</ul>
<ul>
<li><p>die Abbildung</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  Sw = (s_1 w_1,\ldots,s_d w_d)^T,
  \quad
  s_i = \pm 1
  \end{equation*}\]</div>
<p>besitzt diese Eigenschaften:</p>
<ul>
<li><p>für <span class="math notranslate nohighlight">\(v\in\mathbb{R}^d\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    s_i = 
    \begin{cases}
    1  &amp; v_i \geq 0\\
    -1 &amp; v_i&lt;0
    \end{cases},
    \end{equation*}\]</div>
<p>folgt dann
<span class="math notranslate nohighlight">\(Sv = (|v_1|,\ldots,|v_d|)^T=|v|\)</span> (komponentenweise) und</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    x = \Pi(v) = S^{-1}\Pi(Sv) = S^{-1}\Pi(|v|),
    \end{equation*}\]</div>
<p>so dass es genügt, die Projektion für Vektoren <span class="math notranslate nohighlight">\(v\)</span> mit <span class="math notranslate nohighlight">\(v_i\geq 0\)</span> zu betrachten</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>die Abbildung</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  Pw = (w_{\pi(1)},\ldots,w_{\pi(d)})^T
  \end{equation*}\]</div>
<p>wobei <span class="math notranslate nohighlight">\(\pi\)</span> eine beliebige Permutation der Indizes
<span class="math notranslate nohighlight">\(1,\ldots,d\)</span> ist, besitzt ebenfalls die oben beschriebenen Eigenschaften:</p>
<ul>
<li><p>für <span class="math notranslate nohighlight">\(v\in\mathbb{R}^d\)</span> betrachten wir jetzt eine Permutation <span class="math notranslate nohighlight">\(\pi\)</span>,
die die Komponenten von <span class="math notranslate nohighlight">\(v\)</span> absteigend sortiert, also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    v_{\pi(1)} \geq v_{\pi(2)} \geq \ldots \geq v_{\pi(d)}
    \end{equation*}\]</div>
<p>und erhalten</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    x = \Pi(v) = P^{-1}\Pi(Pv),
    \end{equation*}\]</div>
<p>so dass es genügt, die Projektion für Vektoren <span class="math notranslate nohighlight">\(v\)</span> zu betrachten,
deren Komponenten absteigend sortiert sind</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>wir müssen jetzt also nur noch Vektoren <span class="math notranslate nohighlight">\(v\in \mathbb{R}^d\)</span> betrachten mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    \|v\|_1 &gt; r,
    \quad
    v_1 \geq v_2 \geq \ldots \geq v_d \geq 0
    \end{equation*}\]</div>
<p>und wissen bereits, dass</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    \|\Pi(v)\|_1 = r
    \end{equation*}\]</div>
<p>ist</p>
</li>
</ul>
<ul>
<li><p>unter diesen Voraussetzungen gilt für <span class="math notranslate nohighlight">\(x = \Pi(v)\)</span> auch <span class="math notranslate nohighlight">\(x_i\geq 0\)</span>:</p>
<ul>
<li><p>Annahme: es existiere ein <span class="math notranslate nohighlight">\(x_k&lt;0\)</span></p></li>
<li><p>mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      y = (x_1, \ldots, x_{k-1}, -x_k, x_{k+1},\ldots, x_d)^T
      \end{equation*}\]</div>
<p>ist <span class="math notranslate nohighlight">\(\|y\|_1 = \|x\|_1 = r\)</span> und wegen <span class="math notranslate nohighlight">\(x_k &lt; 0 \leq v_k\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
      \|v - y\|_2^2
      &amp;= \sum_{i=1}^d (v_i - y_i)^2 \\
      &amp;= (v_k + x_k)^2 + \sum_{i\neq k} (v_i - x_i)^2 \\
      &amp;&lt; (v_k - x_k)^2 + \sum_{i\neq k} (v_i - x_i)^2 \\
      &amp;= \|v - x\|_2^2
      \end{align*}\]</div>
<p>was ein Widerspruch zu <span class="math notranslate nohighlight">\(x=\Pi(v)\)</span> ist</p>
</li>
</ul>
</li>
<li><p>somit folgt unter diesen Voraussetzungen an <span class="math notranslate nohighlight">\(v\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
  \Pi(v) 
  &amp; = \mathrm{argmin}_{x\in Y_r}\|v - x\|_2,
  \\
  Y_r &amp;= \Big\{x \ \big| \ x\in\mathbb{R}^d, \ x_i\geq 0, \ i=1,\ldots,d,\ \sum_{i=1}^d x_i = r \Big\},
  \end{align*}\]</div>
<p>wobei <span class="math notranslate nohighlight">\(Y_r\)</span> wieder konvex ist</p>
</li>
</ul>
<ul>
<li><p>wir zeigen jetzt, dass ein eindeutiger Index <span class="math notranslate nohighlight">\(p \in \{1,\ldots,d\}\)</span> existieren muss
mit <span class="math notranslate nohighlight">\(x_i&gt;0\)</span> für <span class="math notranslate nohighlight">\(i\leq p\)</span> und <span class="math notranslate nohighlight">\(x_i=0\)</span> für <span class="math notranslate nohighlight">\(i&gt;p\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x = \Pi(v)\)</span> minimiert die konvexe differenzierbare Funktion <span class="math notranslate nohighlight">\(f(y) = \frac{1}{2}\|y - v\|_2^2\)</span>
über der konvexen Menge <span class="math notranslate nohighlight">\(Y_r\)</span>, weshalb</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    0 \leq f'(x)(y-x) = (x - v)^T(y-x) \quad \forall y\in Y_r
    \end{equation*}\]</div>
<p>gelten muss, also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    0 \leq \sum_{i=1}^d (x_i - v_i)(y_i-x_i)\quad \forall y\in Y_r
    \end{equation*}\]</div>
</li>
<li><p>nach oben wissen wir, dass <span class="math notranslate nohighlight">\(\|x\|_1 = r &gt; 0\)</span>, <span class="math notranslate nohighlight">\(x_i \geq 0\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots,d\)</span>, ist,
d.h. es existiert ein Index <span class="math notranslate nohighlight">\(k\)</span> mit <span class="math notranslate nohighlight">\(x_k&gt;0\)</span></p></li>
<li><p>nehmen wir nun an, dass ein Index <span class="math notranslate nohighlight">\(j\)</span> existiert mit <span class="math notranslate nohighlight">\(x_j=0\)</span> und <span class="math notranslate nohighlight">\(x_{j+1} &gt; 0\)</span></p></li>
<li><p>definieren wir für <span class="math notranslate nohighlight">\(\varepsilon&gt;0\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    y = (x_1, \ldots, x_{j-1}, \varepsilon, x_{j+1} - \varepsilon, x_{j+2}, \ldots, x_d)^T
    \end{equation*}\]</div>
<p>dann ist <span class="math notranslate nohighlight">\(y_i \geq 0\)</span>, <span class="math notranslate nohighlight">\(i = 1,\ldots,d\)</span>, <span class="math notranslate nohighlight">\(\|y\|_1 = \|x\|_1 = r\)</span>, also <span class="math notranslate nohighlight">\(y\in Y_r\)</span>, und</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \sum_{i=1}^d (x_i - v_i)(y_i-x_i)
    &amp;= (x_j - v_j)(y_j - x_j) + (x_{j+1} - v_{j+1})(y_{j+1} - x_{j+1}) \\
    &amp;= - v_j\varepsilon - (x_{j+1} - v_{j+1})\varepsilon \\
    &amp;= \varepsilon (\underbrace{v_{j+1} - v_j}_{\leq 0} - \underbrace{x_{j+1}}_{&gt;0}) \\
    &amp; &lt; 0
    \end{align*}\]</div>
<p>was ein Widerspruch zu</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    0 \leq \sum_{i=1}^d (x_i - v_i)(y_i-x_i)\quad \forall y\in Y_r
    \end{equation*}\]</div>
<p>ist</p>
</li>
<li><p>damit kann der Fall <span class="math notranslate nohighlight">\(x_j=0\)</span>, <span class="math notranslate nohighlight">\(x_{j+1}&gt;0\)</span> nicht auftreten</p></li>
<li><p>wegen <span class="math notranslate nohighlight">\(\|x\|_1 = r &gt; 0\)</span> muss <span class="math notranslate nohighlight">\(x\neq 0\)</span> sein, weshalb es einen Index <span class="math notranslate nohighlight">\(p\)</span> gibt mit <span class="math notranslate nohighlight">\(x_i&gt;0\)</span> für <span class="math notranslate nohighlight">\(i\leq p\)</span> und <span class="math notranslate nohighlight">\(x_i = 0\)</span> für <span class="math notranslate nohighlight">\(i &gt; p\)</span></p></li>
</ul>
</li>
</ul>
<ul>
<li><p>für <span class="math notranslate nohighlight">\(x = x(p) =\Pi(v)\)</span> gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
  x_i 
  &amp; = 
  \begin{cases}
  v_i - c_p &gt; 0 &amp; i=1,\ldots,p \\
  0 &amp; i=p+1,\ldots,d
  \end{cases},
  \\
  c_p &amp;= \frac{1}{p}\big(\sum_{i=1}^p v_i  - r \big)
  \end{align*}\]</div>
<ul>
<li><p>Annahme: es existiere ein Index <span class="math notranslate nohighlight">\(j&lt;p\)</span> mit <span class="math notranslate nohighlight">\(x_j - v_j \neq x_{j+1}-v_{j+1}\)</span></p>
<ul class="simple">
<li><p>ohne Einschränkung können wir <span class="math notranslate nohighlight">\(x_j - v_j &lt; x_{j+1}-v_{j+1}\)</span> annehmen</p></li>
<li><p>wegen <span class="math notranslate nohighlight">\(j&lt;p\)</span> ist <span class="math notranslate nohighlight">\(x_j&gt;0\)</span>, so dass für <span class="math notranslate nohighlight">\(0&lt;\varepsilon\leq x_j\)</span></p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    y = (x_1, \ldots, x_{j-1}, x_j - \varepsilon, 
         x_{j+1} + \varepsilon, x_{j+2}, \ldots, x_d)^T 
    \in Y_r
    \end{equation*}\]</div>
<ul class="simple">
<li><p>analog zum letzten Abschnitt erhalten wir</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    (x - v)^T(y - x)
    &amp;= (x_j - v_j)(y_j - x_j) + (x_{j+1} - v_{j+1})(y_{j+1} - x_{j+1}) \\
    &amp;= (x_j - v_j)\varepsilon - (x_{j+1} - v_{j+1})\varepsilon \\
    &amp; &lt; 0
    \end{align*}\]</div>
<p>was erneut ein Widerspruch zu</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    0 \leq (x - v)^T(y - x)\quad \forall y\in Y_r
    \end{equation*}\]</div>
<p>ist</p>
</li>
<li><p>damit ist also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    x_i = 
    \begin{cases}
    v_i - c_p &gt; 0 &amp; i=1,\ldots,p \\
    0 &amp; i=p+1,\ldots,d
    \end{cases}
    \end{equation*}\]</div>
</li>
<li><p>wegen <span class="math notranslate nohighlight">\(x\in Y_r\)</span> gilt</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
   r 
   &amp;= \|x\|_1 
   = \sum_{i=1}^d |x_i| 
   = \sum_{i=1}^p x_i
   \\
   &amp;= \sum_{i=1}^p (v_i - c_p)
   \\
   &amp;= \big(\sum_{i=1}^p v_i\big) - p c_p
   \end{align*}\]</div>
<p>und somit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   c_p = \frac{1}{p}\big(\sum_{i=1}^p v_i  - r\big)
   \end{equation*}\]</div>
</li>
</ul>
<ul>
<li><p>zum Schluss muss jetzt noch der Index <span class="math notranslate nohighlight">\(p\)</span> bestimmt werden</p></li>
<li><p>es gilt <span class="math notranslate nohighlight">\(\Pi(v)=x(p_\ast)\)</span> für</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  p_\ast = \mathrm{argmax}_{p\in\{1,\ldots,d\}}\{v_p - c_p &gt; 0\}
  \end{equation*}\]</div>
<ul>
<li><p>für jedes <span class="math notranslate nohighlight">\(p\in\{1,\ldots,d\}\)</span> ist</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
    x_i 
    &amp; = 
    \begin{cases}
    v_i - c_p &gt; 0 &amp; i=1,\ldots,p \\
    0 &amp; i=p+1,\ldots,d
    \end{cases},
    \\
    c_p 
    &amp; = 
    \frac{1}{p}\big(\sum_{i=1}^p v_i  - r \big)
    \end{align*}\]</div>
<p>eindeutige Lösung von</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    \mathrm{argmin}_{z\in Z_p}\|z - v\|_2^2
    \end{equation*}\]</div>
<p>mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    Z_p = \big\{ z \ \big | \ z\in\mathbb{R}^d, z_i\geq 0, \ z_i = 0 \ \forall i&gt;p, 
                \ \|z\|_1 = \sum_{i=1}^p z_i = r \big\} \subset Y_r,
    \end{equation*}\]</div>
<p>denn <span class="math notranslate nohighlight">\(Z_p\)</span> ist abgeschlossen und konvex, <span class="math notranslate nohighlight">\(f(z) =  \|z - v\|_2^2\)</span> ist konvex
und für alle
<span class="math notranslate nohighlight">\(z \in Z_p\)</span> gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
    f'(x)(z - x)
    &amp; = (x - v)^T(z - x)
    \\
    &amp; = \sum_{i=1}^p (x_i - v_i)(z_i - x_i)
    \\
    &amp;= -c_p \sum_{i=1}^p (z_i - x_i)
    \\
    &amp;= 0
    \end{align*}\]</div>
</li>
<li><p>Annahme: <span class="math notranslate nohighlight">\(p &lt; p_\ast\)</span></p>
<ul class="simple">
<li><p>dann ist</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
     Z_p \ni x(p) 
       = \Pi(v)
       &amp; = \mathrm{argmin}_{y\in Y_r}\|y - v\|_2^2 
       \\
       &amp;= \mathrm{argmin}_{z\in Z_p}\|z - v\|_2^2
     \end{align*}\]</div>
<p>bzw.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
     Z_{p_\ast} \ni x(p_\ast) 
       = \mathrm{argmin}_{z\in Z_{p_\ast}}\|z - v\|_2^2
     \end{equation*}\]</div>
<ul>
<li><p>nach Definition ist <span class="math notranslate nohighlight">\(Z_p \subset Z_{p_\ast} \subset Y_r\)</span>
und alle Mengen sind konvex und abgeschlossen, so dass
alle Minima eindeutig sind</p></li>
<li><p>somit ist <span class="math notranslate nohighlight">\(x(p) = x(p_\ast)\)</span></p></li>
<li><p>nach Konstruktion von <span class="math notranslate nohighlight">\(x(p), x(p_\ast)\)</span> gilt wegen <span class="math notranslate nohighlight">\(p &lt; p_\ast\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      x(p)_{p+1} = 0,
      \quad
      x(p_\ast)_{p+1} \ne 0,
      \end{equation*}\]</div>
<p>was zu einem Widerspruch führt</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>zusammengefasst erhalten wir damit folgenden Algorithmus zur Berechnung von <span class="math notranslate nohighlight">\(x = \Pi(v)\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(v\in\mathbb{R}^d\)</span> gegeben</p></li>
<li><p>ist <span class="math notranslate nohighlight">\(\|v\|_1 \leq r\)</span> dann ist <span class="math notranslate nohighlight">\(x = v\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\|v\|_1 &gt; r\)</span>:</p>
<ul>
<li><p>setze</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      w = PSv,
      \end{equation*}\]</div>
<p>d.h. <span class="math notranslate nohighlight">\(w\)</span> enthält die Beträge der Komponenten von <span class="math notranslate nohighlight">\(v\)</span> absteigend sortiert</p>
</li>
<li><p>bestimme</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
      p_\ast = \mathrm{argmax}_{p\in\{1,\ldots,d\}}
      \big\{
      w_p - \frac{1}{p}\big(\sum_{i=1}^p w_i  - r\big) &gt; 0
      \big\}
      \end{align*}\]</div>
</li>
<li><p>berechne <span class="math notranslate nohighlight">\(y\)</span> als</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
      y_i 
      &amp;= 
      \begin{cases}
      w_i - c_{p_\ast} &gt; 0 &amp; i=1,\ldots,p_\ast \\
      0 &amp; i=p_\ast+1,\ldots,d
      \end{cases},
      \\
      c_p 
      &amp;= \frac{1}{p_\ast}\big(\sum_{i=1}^{p_\ast} w_i  - r \big)
      \end{align*}\]</div>
</li>
<li><p>damit folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
      x = S^{-1}P^{-1}y
      \end{equation*}\]</div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>eine Anwendung in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> bzw. <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span> liefert folgende Resultate</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">l1pro</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># Projektion auf L1-Kugel mit Radius r bzgl. eukl. Skalarpr.</span>
    
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">r</span><span class="p">:</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># S, Vorzeichen merken fuer S_inv</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="n">v</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="c1"># PS anwenden</span>
        <span class="n">w</span>  <span class="o">=</span> <span class="n">s</span> <span class="o">*</span> <span class="n">v</span>
        <span class="n">ii</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">w</span>  <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span>

        <span class="c1"># Projektion</span>
        <span class="n">p</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">cp</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span> <span class="o">-</span> <span class="n">r</span><span class="p">)</span><span class="o">/</span><span class="n">p</span>
        <span class="n">u</span>  <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">cp</span>
        <span class="n">jj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">u</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

        <span class="n">ps</span>  <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">jj</span><span class="p">]</span>
        <span class="n">cps</span> <span class="o">=</span> <span class="n">cp</span><span class="p">[</span><span class="n">jj</span><span class="p">]</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">y</span><span class="p">[:</span><span class="n">ps</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:</span><span class="n">ps</span><span class="p">]</span> <span class="o">-</span> <span class="n">cps</span>

        <span class="c1"># P_inv und S_inv anwenden</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="c1"># P_inv</span>
        <span class="n">x</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
        <span class="c1"># S_inv = S</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">s</span><span class="o">*</span><span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">v</span>
        
    <span class="k">return</span> <span class="n">x</span>


<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">l</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">r</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="n">r</span><span class="p">)</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="n">l1pro</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">w</span><span class="p">,</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">markersize</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">],</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;r.&#39;</span><span class="p">,</span> <span class="n">markersize</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/11_Projected_Gradient_Descent_50_0.png" src="_images/11_Projected_Gradient_Descent_50_0.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#%matplotlib notebook</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">l</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">r</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">l</span> <span class="o">+</span> <span class="n">r</span><span class="p">)</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="n">l1pro</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">w</span><span class="p">,</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">markersize</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">],</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;r.&#39;</span><span class="p">,</span> <span class="n">markersize</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1">#plt.axis(&#39;equal&#39;)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">l</span> <span class="o">+</span> <span class="n">r</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="n">lr</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="n">lr</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="o">-</span><span class="n">lr</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zticks</span><span class="p">([]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/11_Projected_Gradient_Descent_51_0.png" src="_images/11_Projected_Gradient_Descent_51_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="beispiel-tomographie">
<h2>Beispiel Tomographie<a class="headerlink" href="#beispiel-tomographie" title="Link zu dieser Überschrift">¶</a></h2>
<p>Bei den Regularisierungsverfahren hatten wir für unser Tomographieproblem unter anderem die Lasso-Methode kennen gelernt.
Dort bestimmt man</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\mathrm{argmin}_{w\in\mathbb{R}^n}\frac{1}{2m}\| X w - y\|_2^2 + \alpha \|w\|_1,
\end{equation*}\]</div>
<p>wobei <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span> ein vom Benutzer zu wählender Regularisierungsparameter ist.</p>
<p>Dieses nicht restringierte Problem kann in das
restringierte Problem</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\mathrm{argmin}_{\|w\|_1 \leq r}\frac{1}{2m}\| X w - y\|_2^2 
\end{equation*}\]</div>
<p>überführt werden, wobei die Rolle des Regularisierungsparameters
<span class="math notranslate nohighlight">\(\alpha\)</span> jetzt von <span class="math notranslate nohighlight">\(r\)</span> übernommen wird:</p>
<ul class="simple">
<li><p>für <span class="math notranslate nohighlight">\(r\to\infty\)</span> ist <span class="math notranslate nohighlight">\(\|w\|_1 \leq r\)</span> immer weniger
einschränkend, man „regularisiert“ immer weniger,
was <span class="math notranslate nohighlight">\(\alpha\to 0\)</span> entspricht</p></li>
<li><p>für <span class="math notranslate nohighlight">\(r\to 0\)</span> bleibt nur <span class="math notranslate nohighlight">\(w = 0\)</span> als Lösung, d.h.
man „regularisiert“ maximal,
was <span class="math notranslate nohighlight">\(\alpha\to \infty\)</span> entspricht</p></li>
</ul>
<p>Auf das restringierte Problem wenden wir Projected-Gradient-Descent
an und benutzen dabei die Projektion auf <span class="math notranslate nohighlight">\(B_r(0)_{\|\cdot\|_1}\)</span>
aus dem letzten Abschnitt.</p>
<p>Zum Vergleich berechnen wir
zunächst eine Rekonstruktion ohne Regularisierung.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">DatenNotebooks.xrtomo12</span> <span class="kn">import</span> <span class="o">*</span>

<span class="k">def</span> <span class="nf">plotReko</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">pcolor</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>    
<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tomo</span><span class="p">(</span><span class="n">delta</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">)</span>

<span class="n">modell</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">modell</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">modell</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">plotReko</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/11_Projected_Gradient_Descent_55_0.png" src="_images/11_Projected_Gradient_Descent_55_0.png" />
</div>
</div>
<p>Die Lasso-Implementierung aus Scikit-Learn liefert das folgende Ergebnis.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lassocv</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LassoCV</span><span class="p">(</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">lassocv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">lassocv</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">plotReko</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;alpha = </span><span class="si">{:e}</span><span class="s1">,   ||w||_1 = </span><span class="si">{:f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lassocv</span><span class="o">.</span><span class="n">alpha_</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>alpha = 9.867860e-06,   ||w||_1 = 20.221146
</pre></div>
</div>
<img alt="_images/11_Projected_Gradient_Descent_57_1.png" src="_images/11_Projected_Gradient_Descent_57_1.png" />
</div>
</div>
<p>Mit Projected Gradient erhalten wir ein sehr ähnliches Resultat.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">l</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    
<span class="k">def</span> <span class="nf">l1</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">GDp</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">pro</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">nit</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">w</span>  <span class="o">=</span> <span class="n">w0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">pro</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    
    <span class="n">ww</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nit</span><span class="p">):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">pro</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">ww</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">ww</span>


<span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">r</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">pro1</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span> <span class="p">:</span> <span class="n">l1pro</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>

<span class="n">ww</span> <span class="o">=</span> <span class="n">GDp</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">pro</span> <span class="o">=</span> <span class="n">pro1</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">nit</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">ww</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$k$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$||Xw-y||_2^2$&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">ww</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plotReko</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;r = </span><span class="si">{:f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>r = 20.000000
</pre></div>
</div>
<img alt="_images/11_Projected_Gradient_Descent_59_1.png" src="_images/11_Projected_Gradient_Descent_59_1.png" />
<img alt="_images/11_Projected_Gradient_Descent_59_2.png" src="_images/11_Projected_Gradient_Descent_59_2.png" />
</div>
</div>
</div>
<div class="section" id="zusammenfassung">
<h2>Zusammenfassung<a class="headerlink" href="#zusammenfassung" title="Link zu dieser Überschrift">¶</a></h2>
<p>Für Projected-Gradient-Descent bei restringierten Optimierungsproblemen mit
<span class="math notranslate nohighlight">\(X\in\mathbb{R}^d\)</span> abgeschlossen und konvex haben wir das selbe
Konvergenzverhalten wie im nicht restringierten Fall nachgewiesen:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(f\)</span> konvex und Lipschitz-stetig, <span class="math notranslate nohighlight">\(\gamma = \frac{c}{\sqrt{T}}\)</span>, <span class="math notranslate nohighlight">\(c&gt;0\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    \min_{t=0,\ldots,T-1}(f_t - f_\ast) \leq \varepsilon
    \quad \Rightarrow\quad
    T = \mathcal{O}\big(\frac{1}{\varepsilon^2}\big)
    \end{equation*}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(f\)</span> konvex und <span class="math notranslate nohighlight">\(L\)</span>-glatt, <span class="math notranslate nohighlight">\(0 &lt; \gamma &lt; \frac{2}{L}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f_{T}-f_\ast \leq \varepsilon
    \quad \Rightarrow\quad
    T =\mathcal{O}\big( \frac{1}{\varepsilon} \big)
    \end{equation*}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(f\)</span> <span class="math notranslate nohighlight">\(\mu\)</span>-konvex mit <span class="math notranslate nohighlight">\(\mu&gt;0\)</span> und <span class="math notranslate nohighlight">\(L\)</span>-glatt, <span class="math notranslate nohighlight">\(0 &lt; \gamma \leq \frac{1}{L}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    f_{T}-f_\ast \leq \varepsilon
    \quad \Rightarrow\quad
    T = \mathcal{O}\Big( \log\big(\frac{1}{\varepsilon}\big) \Big)
    \end{equation*}\]</div>
</li>
</ul>
<p>Das Berechnen der Projektion <span class="math notranslate nohighlight">\(\Pi_X\)</span> auf <span class="math notranslate nohighlight">\(X\in\mathbb{R}^d\)</span> abgeschlossen und konvex
kann extrem aufwendig sein.
Für einige spezielle Mengen <span class="math notranslate nohighlight">\(X\)</span> die in der Praxis häufig auftreten
lässt sich <span class="math notranslate nohighlight">\(\Pi_X\)</span> einfach berechnen.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="10_Gradient_Descent.html" title="zurück Seite">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">zurück</p>
            <p class="prev-next-title">Gradient Descent</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="12_Subgradient_Descent.html" title="weiter Seite">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">weiter</p>
        <p class="prev-next-title">Subgradient Descent</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      Durch Martin Reißel<br/>
    
        &copy; Urheberrechte © 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>