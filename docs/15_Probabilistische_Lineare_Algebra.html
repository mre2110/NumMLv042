
<!DOCTYPE html>

<html lang="de">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Probabilistische Lineare Algebra &#8212; Numerische Algorithmen für Maschinelles Lernen WS21/22 (Version 0.42)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/translations.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Stichwortverzeichnis" href="genindex.html" />
    <link rel="search" title="Suche" href="search.html" />
    <link rel="next" title="Weiterführende Links" href="99_Literatur.html" />
    <link rel="prev" title="Stochastic Gradient Descent" href="14_Stochastic_Gradient_Descent.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Numerische Algorithmen für Maschinelles Lernen WS21/22 (Version 0.42)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Dieses Buch durchsuchen ..." aria-label="Dieses Buch durchsuchen ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="00_Vorwort.html">
   Numerische Algorithmen für Maschinelles Lernen (Version 0.42)
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_Regression.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_Dimensionsreduktion.html">
   Dimensionsreduktion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_Regularisierung.html">
   Regularisierung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_Background_Removal_QR.html">
   Background Removal mit TSVD
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_Klassifikation_mit_SVM.html">
   Support-Vector Klassifikation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_Neuronale_Netze.html">
   Neuronale Netze
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_Topic_Extraction.html">
   Topic Extraction, NMF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_Grundlagen_Optimierung.html">
   Grundlagen der Optimierung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_Konvexitaet.html">
   Konvexität
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_Gradient_Descent.html">
   Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_Projected_Gradient_Descent.html">
   Projected Gradient-Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12_Subgradient_Descent.html">
   Subgradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13_Proximal_Gradient_Descent.html">
   Proximal Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14_Stochastic_Gradient_Descent.html">
   Stochastic Gradient Descent
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Probabilistische Lineare Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="99_Literatur.html">
   Weiterführende Links
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Navigation umschalten" aria-controls="site-navigation"
                title="Navigation umschalten" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Laden Sie diese Seite herunter"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/15_Probabilistische_Lineare_Algebra.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Quelldatei herunterladen" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="In PDF drucken"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Vollbildmodus"
        title="Vollbildmodus"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/mre2110/NumMLv042/master?urlpath=tree/15_Probabilistische_Lineare_Algebra.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Starten Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Inhalt
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#grundlagen">
   Grundlagen
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probabilistische-verfahren">
   Probabilistische Verfahren
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fixed-rank">
   Fixed-Rank
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#randomized-svd">
   Randomized SVD
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fixed-precision">
   Fixed-Precision
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#zusammenfassung">
   Zusammenfassung
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="probabilistische-lineare-algebra">
<h1>Probabilistische Lineare Algebra<a class="headerlink" href="#probabilistische-lineare-algebra" title="Link zu dieser Überschrift">¶</a></h1>
<div class="section" id="grundlagen">
<h2>Grundlagen<a class="headerlink" href="#grundlagen" title="Link zu dieser Überschrift">¶</a></h2>
<p>Aus der linearen Algebra sind viele Matrix-Zerlegungen bekannt, z.B. LU, QR, SVD.
Wir betrachten hier die SVD, also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  A = U\Sigma V^T
  \end{equation*}\]</div>
<p>mit <span class="math notranslate nohighlight">\(U,V\)</span> orthonormal und <span class="math notranslate nohighlight">\(\Sigma = \mathrm{diag}_i(\sigma_i)\)</span>.</p>
<p>Hat <span class="math notranslate nohighlight">\(A\)</span> den Rang <span class="math notranslate nohighlight">\(r\)</span>, dann gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
   \sigma_i &amp;&gt; 0 \quad i=1,\ldots,r \\
   \sigma_i &amp;= 0 \quad i=r+1,\ldots
  \end{align*}\]</div>
<p>und man erhält</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \underbrace{A}_{\mathbb{R}^{m \times n}}=
  \underbrace{U_r}_{\mathbb{R}^{m \times r}}
  \ \underbrace{\Sigma_r}_{\mathbb{R}^{r \times r}}
  \ \underbrace{V_r^T}_{\mathbb{R}^{r \times n}}
  \end{equation*}\]</div>
<p>mit <span class="math notranslate nohighlight">\(r \leq \min(m,n)\)</span>.
Ist <span class="math notranslate nohighlight">\(r \ll \min(m,n)\)</span>, dann können Operationen mit <span class="math notranslate nohighlight">\(A\)</span>, wie z.B.
Matrix-Vektor-Produkte, über die Zerlegung mit deutlich weniger
Aufwand berechnet werden.</p>
<p>Die Spalten von <span class="math notranslate nohighlight">\(U_r\)</span> bilden eine Orthonormalbasis des Bildraums
<span class="math notranslate nohighlight">\(R(A)\)</span>, d.h. sie enthalten die komplette Information über  <span class="math notranslate nohighlight">\(R(A)\)</span>.
Gilt nun</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \sigma_1 \geq \ldots \geq \sigma_k \geq \sigma_{k+1}\geq \ldots \geq \sigma_r,
  \quad
  \sigma_1 \gg \sigma_{k+1}
  \end{equation*}\]</div>
<p>dann hatten wir bei TSVD einfach <span class="math notranslate nohighlight">\(\sigma_{k+1} = \ldots = \sigma_r =0\)</span> gesetzt
und damit eine Matrix-Approximation durchgeführt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  A = U\Sigma V^T \approx U_k \Sigma_k V_k^T.
  \end{equation*}\]</div>
<p>Die Spalten von <span class="math notranslate nohighlight">\(U_k\)</span> sind eine Orthonormalbasis eines Unterraums von <span class="math notranslate nohighlight">\(R(A)\)</span>.
Wegen <span class="math notranslate nohighlight">\(\sigma_1 \gg \sigma_{k+1}\)</span> „hoffen“ wir, das wir damit den „wesentlichen Teil“
von <span class="math notranslate nohighlight">\(R(A)\)</span> erwischt haben und damit <span class="math notranslate nohighlight">\(U_k \Sigma_k V_k^T\)</span> eine gute Approximation von
<span class="math notranslate nohighlight">\(A\)</span> ist.</p>
<p>Diese Vorgehensweise lässt sich verallgemeinern:</p>
<ul>
<li><p>für <span class="math notranslate nohighlight">\(A\in \mathbb{R}^{m\times n}\)</span> sucht man eine Rang-<span class="math notranslate nohighlight">\(k\)</span>-Approximation</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  A \approx QC, \quad Q \in \mathbb{R}^{m\times k}, \quad C \in \mathbb{R}^{k\times n}
  \end{equation*}\]</div>
</li>
<li><p>dazu konstruiert man zunächst einen Unterraum von <span class="math notranslate nohighlight">\(R(A)\)</span> der möglichst kleine
Dimension <span class="math notranslate nohighlight">\(k\)</span> haben sollte, aber gleichzeitig die wesentlichen Abbildungseigenschaften
von <span class="math notranslate nohighlight">\(A\)</span> wiedergeben soll</p></li>
<li><p>eine Orthonormalbasis dieses Unterraums legen wir als Spalten in der Matrix</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  Q\in\mathbb{R}^{m\times k}
  \end{equation*}\]</div>
<p>ab und approximieren <span class="math notranslate nohighlight">\(A\)</span> durch Orthogonalprojektion in diesen Unterraum, d.h.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  A \approx QQ^T A =  QC = B, \quad C = Q^T A \in \mathbb{R}^{k\times n}
  \end{equation*}\]</div>
</li>
<li><p>mit <span class="math notranslate nohighlight">\(Q\)</span> erzeugen wir also eine niedrig dimensionale
Matrix <span class="math notranslate nohighlight">\(C = Q^TA\)</span> so dass <span class="math notranslate nohighlight">\(QC\)</span> die Ausgangsmatrix <span class="math notranslate nohighlight">\(A\)</span>
approximiert</p></li>
<li><p>auf <span class="math notranslate nohighlight">\(C\)</span> wenden wir dann Standard-Zerlegungen (LU, QR, SVD)
an, was aufgrund der niedrigen Dimension wenig Aufwand verursacht,
und konstruieren daraus approximative Zerlegungen für
die Ausgangs-Matrix <span class="math notranslate nohighlight">\(A\)</span></p></li>
</ul>
<p>Wir gehen also zweistufig vor:</p>
<ul class="simple">
<li><p>bestimme <span class="math notranslate nohighlight">\(Q\)</span></p></li>
<li><p>berechne die Zerlegung von <span class="math notranslate nohighlight">\(C = Q^T A\)</span> und darüber
eine Zerlegung von <span class="math notranslate nohighlight">\(B = QC\)</span>, was dann eine
approximative Zerlegung von <span class="math notranslate nohighlight">\(A\)</span> darstellt</p></li>
</ul>
<p>Für den ersten Teil werden wir probabilistische Methoden benutzen.</p>
<p><strong>Beispiel:</strong> Approximative SVD für <span class="math notranslate nohighlight">\(A\in \mathbb{R}^{m \times n}\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(Q_k\in \mathbb{R}^{m \times k}\)</span> orthonormal sei gegeben, <span class="math notranslate nohighlight">\(k\leq\min(m,n)\)</span></p></li>
<li><p>ist <span class="math notranslate nohighlight">\(C_k = Q_k^TA \in \mathbb{R}^{k \times n}\)</span>, dann ist</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  B_k = Q_kC_k = Q_k Q_k^TA \approx A
  \end{equation*}\]</div>
<p>eine Rang-<span class="math notranslate nohighlight">\(k\)</span>-Approximation von <span class="math notranslate nohighlight">\(A\)</span></p>
</li>
<li><p>wir berechnen die SVD von <span class="math notranslate nohighlight">\(C_k\)</span> und erhalten</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  C_k = \tilde{U}_k \Sigma_k V_k^T,
  \quad
  \tilde{U}_k\in \mathbb{R}^{k \times k},
  \quad
  \Sigma_k\in \mathbb{R}^{k \times k},
  \quad
  V_k^T \in \mathbb{R}^{k \times n}
  \end{equation*}\]</div>
</li>
<li><p>mit <span class="math notranslate nohighlight">\(A\approx B_k = Q_k C_k\)</span> folgt dann</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  A\approx Q_k C_k  = \underbrace{Q_k \tilde{U}_k}_{U_k} \Sigma_k V_k^T,
  \end{equation*}\]</div>
<p>mit <span class="math notranslate nohighlight">\(U_k\in \mathbb{R}^{m \times k}\)</span> orthonormal</p>
</li>
</ul>
<p>Die offene Frage ist jetzt, wie wir geeignete Matrizen <span class="math notranslate nohighlight">\(Q_k\)</span> finden können.</p>
</div>
<div class="section" id="probabilistische-verfahren">
<h2>Probabilistische Verfahren<a class="headerlink" href="#probabilistische-verfahren" title="Link zu dieser Überschrift">¶</a></h2>
<p>In diesem Abschnitt betrachten wir die Konstruktion von <span class="math notranslate nohighlight">\(Q_k\)</span>.</p>
<p>Ab jetzt lassen wir den Index <span class="math notranslate nohighlight">\(k\)</span> weg.
Wir müssen folgendes Problem lösen:</p>
<ul>
<li><p>gegeben ist <span class="math notranslate nohighlight">\(A\in \mathbb{R}^{m \times n}\)</span>, <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span></p></li>
<li><p>finde eine orthonormale Matrix <span class="math notranslate nohighlight">\(Q\in \mathbb{R}^{m \times k}\)</span>,
<span class="math notranslate nohighlight">\(k = k(\varepsilon) \leq \min(m,n)\)</span>, mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    \|A -QQ^TA\| \leq \varepsilon,
    \end{equation*}\]</div>
<p>d.h.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    \|A - X\| \leq \varepsilon
    \end{equation*}\]</div>
<p>mit <span class="math notranslate nohighlight">\(X=QQ^TA\)</span> mit <span class="math notranslate nohighlight">\(\mathrm{rang}(X) \leq k\)</span></p>
</li>
</ul>
<p><span class="math notranslate nohighlight">\(k(\varepsilon)\)</span> sollte so klein wie möglich sein.
Als Matrix-Norm benutzen wir in der Regel</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
  \|A\|_2 
  &amp;= \sqrt{\rho(A^TA)},
  \\
  \|A\|_F 
  &amp;= \sqrt{\mathrm{tr}(A^TA)} = \sqrt{\sum_{i=1}^m\sum_{j=1}^n a_{ij}^2}.
  \end{align*}\]</div>
<p>Für orthonormal-invariante Matrix-Normen, d.h.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|A\| = \|QA\| = \|A\tilde{Q}\|
  \quad
  \forall Q,\tilde{Q} \ \text{orthonormal}
  \end{equation*}\]</div>
<p>kennt man eine Optimal-Lösung dieses Problems.</p>
<p><strong>Satz (Eckart, Young, Mirsky):</strong> Ist <span class="math notranslate nohighlight">\(A\in \mathbb{R}^{m \times n}\)</span>
mit SVD <span class="math notranslate nohighlight">\(A=U\Sigma V^T\)</span> und die Matrixnorm <span class="math notranslate nohighlight">\(\|\cdot\|\)</span> orthonormal invariant, dann
gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \min_{X\in \mathbb{R}^{m \times n}, \mathrm{rang}(X)\leq k} \|A-X\| = \sigma_{k+1}.
  \end{equation*}\]</div>
<p>Das Minimum wird für</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  X_{\min} = U_k \Sigma_k V_k^T,
  \end{equation*}\]</div>
<p>also TSVD, angenommen.</p>
<p>Die Normen <span class="math notranslate nohighlight">\(\|\cdot\|_2\)</span> und <span class="math notranslate nohighlight">\(\|\cdot\|_F\)</span> sind orthonormal
invariant (Übung), somit können wir das Ergebnis hier in anwenden.
In unserem Set-Up bedeutet das:</p>
<ul>
<li><p>in <span class="math notranslate nohighlight">\(X_{\min}\)</span> kommen nur die Spalten <span class="math notranslate nohighlight">\(u_1,\ldots,u_k\)</span> von <span class="math notranslate nohighlight">\(U\)</span> vor, d.h.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    Q = U_k = (u_1,\ldots,u_k) \in \mathbb{R}^{m\times k}
    \end{equation*}\]</div>
</li>
<li><p>somit ist</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
  QQ^TA 
  &amp;= U_k U_k^T U\Sigma V^T \\
  &amp;= U_k (I_k,0)\Sigma V^T \\
  &amp;= U_k (\Sigma_k,0) V^T \\
  &amp;= U_k \Sigma_k V_k^T 
  \end{align*}\]</div>
<p>und wir könnten wie folgt vorgehen:</p>
<ul class="simple">
<li><p>gebe <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span> vor</p></li>
<li><p>suche <span class="math notranslate nohighlight">\(k\)</span> mit <span class="math notranslate nohighlight">\(\sigma_{k+1}&lt;\varepsilon\)</span></p></li>
<li><p>benutze <span class="math notranslate nohighlight">\(Q=U_k\)</span> und</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    B = QQ^TA = U_k \Sigma_k V_k^T 
    \end{equation*}\]</div>
<p>als Rang-<span class="math notranslate nohighlight">\(k\)</span>-Approximation</p>
</li>
</ul>
<p>Das Problem dabei ist, dass man dazu eine komplette SVD von <span class="math notranslate nohighlight">\(A\)</span> benötigt.</p>
<p>Wir müssen also überlegen, wie wir „billiger“ an <span class="math notranslate nohighlight">\(Q\)</span> kommen können.
Dazu betrachten wir für <span class="math notranslate nohighlight">\(A\in \mathbb{R}^{m \times n}\)</span> zwei Fälle:</p>
<ul>
<li><p>fixed-presicion:
gegeben <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>,
suche <span class="math notranslate nohighlight">\(Q\)</span> orthonormal mit
<span class="math notranslate nohighlight">\(\mathrm{rang}(Q)\leq k(\varepsilon)\)</span>,<br />
wobei
<span class="math notranslate nohighlight">\(k(\varepsilon)\)</span> möglichst klein sein soll,
und</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|A -QQ^TA\| \leq \varepsilon
  \end{equation*}\]</div>
</li>
<li><p>fixed-rank: <span class="math notranslate nohighlight">\(k,p\)</span> ist gegeben, suche <span class="math notranslate nohighlight">\(Q \in \mathbb{R}^{m\times (k+p)}\)</span> orthonormal mit
<span class="math notranslate nohighlight">\(\mathrm{rang}(Q)\leq k\)</span>
und</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \|A -QQ^TA\| 
  \approx
  \min_{X\in \mathbb{R}^{m \times n}, \mathrm{rang}(X)\leq k} \|A-X\| 
  = 
  \sigma_{k+1}
  \end{equation*}\]</div>
</li>
</ul>
<p>Bei fixed-rank lassen wir bei <span class="math notranslate nohighlight">\(Q\)</span> mehr Spalten zu als für eine Rang-<span class="math notranslate nohighlight">\(k\)</span>-Approximation
unbedingt nötig wären (<span class="math notranslate nohighlight">\(p\)</span>).
Dieser Freiheitsgrad wird später eine wichtige Rolle bei der Genauigkeit
probabilistischer Methoden spielen.</p>
<p>Wir untersuchen zunächst fixed-rank und knacken damit dann später auch fixed-precision.</p>
</div>
<div class="section" id="fixed-rank">
<h2>Fixed-Rank<a class="headerlink" href="#fixed-rank" title="Link zu dieser Überschrift">¶</a></h2>
<p>Wie kann der „Zufall“ bei der Konstruktion von <span class="math notranslate nohighlight">\(Q\)</span> helfen?</p>
<p>Wir betrachten dazu zunächst <span class="math notranslate nohighlight">\(A\in \mathbb{R}^{m \times n}\)</span> mit <span class="math notranslate nohighlight">\(\mathrm{rang}(A)=k\)</span>:</p>
<ul>
<li><p>ziehe <span class="math notranslate nohighlight">\(k\)</span> Zufallsvektoren <span class="math notranslate nohighlight">\(\omega^{(i)}\)</span> (Verteilung zunächst uninteressant)
und berechne</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  y^{(i)} = A \omega^{(i)}, \quad i=1,\ldots,k
  \end{equation*}\]</div>
</li>
<li><p>mit hoher Wahrscheinlichkeit gilt:</p>
<ul class="simple">
<li><p>die <span class="math notranslate nohighlight">\(\omega^{(i)}\)</span> sind linear unabhängig</p></li>
<li><p><span class="math notranslate nohighlight">\(\omega^{(i)} \not\in N(A)\)</span></p></li>
</ul>
</li>
<li><p>damit sind dann auch die <span class="math notranslate nohighlight">\(y^{(i)}\)</span> mit hoher Wahrscheinlichkeit linear unabhängig</p></li>
<li><p>wegen <span class="math notranslate nohighlight">\(\mathrm{rang}(A)=k\)</span> gilt dann</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  R(A)=\mathrm{span}\big(y^{(1)},\ldots,y^{(k)}\big),
  \end{equation*}\]</div>
<p>d.h. <span class="math notranslate nohighlight">\(Y = \big(y^{(1)},\ldots,y^{(k)}\big)\)</span> spannt <span class="math notranslate nohighlight">\(R(A)\)</span> auf</p>
</li>
<li><p>orthonormalisieren wir jetzt <span class="math notranslate nohighlight">\(Y\)</span> (modifizierter Gram-Schmidt, QR, etc.), so erhalten wir
<span class="math notranslate nohighlight">\(Q\in \mathbb{R}^{m \times k}\)</span></p></li>
</ul>
<p>Jetzt sei <span class="math notranslate nohighlight">\(A = A_0 + E\)</span>, <span class="math notranslate nohighlight">\(\mathrm{rang}(A_0)=k\)</span>, wobei <span class="math notranslate nohighlight">\(E\)</span> sei eine „kleine“ Störung ist.</p>
<ul>
<li><p>wir suchen einen Unterraum in <span class="math notranslate nohighlight">\(R(A)\)</span>, der einen möglichst „großen Teil“ von
<span class="math notranslate nohighlight">\(R(A_0)\)</span> abdeckt, aber nicht unedbingt die kleinstmögliche Dimension <span class="math notranslate nohighlight">\(k\)</span> hat
sondern Dimension <span class="math notranslate nohighlight">\(k+p\)</span></p></li>
<li><p>wir betrachten wieder</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    y^{(i)} = A \omega^{(i)} = A_0 \omega^{(i)} + E  \omega^{(i)}
    \end{equation*}\]</div>
</li>
<li><p>der „Störanteil“ <span class="math notranslate nohighlight">\(E  \omega^{(i)}\)</span> „schiebt“ <span class="math notranslate nohighlight">\(A_0 \omega^{(i)}\)</span> eventuell
aus <span class="math notranslate nohighlight">\(R(A_0)\)</span> hinaus</p></li>
<li><p>wenn wir genau <span class="math notranslate nohighlight">\(k\)</span> verschiedene <span class="math notranslate nohighlight">\(y^{(i)}\)</span> benutzen, dann ist die Wahrscheinlichkeit
hoch, dass wir „wichtige Teile“ von <span class="math notranslate nohighlight">\(R(A_0)\)</span> nicht erfassen</p></li>
<li><p>deshalb benutzen wir <span class="math notranslate nohighlight">\(k+p\)</span> Vektoren <span class="math notranslate nohighlight">\(y^{(i)}\)</span>, <span class="math notranslate nohighlight">\(p\in\mathbb{N}\)</span></p></li>
</ul>
<p>Fassen wir diese Schritte zusammen, so erhalten wir das <strong>Fixed-Rank-Verfahren</strong>:</p>
<ul class="simple">
<li><p>gegeben ist <span class="math notranslate nohighlight">\(A\in \mathbb{R}^{m \times n}\)</span>, <span class="math notranslate nohighlight">\(k,p\in \mathbb{N}\)</span></p></li>
<li><p>bestimme eine „zufällige“ Matrix <span class="math notranslate nohighlight">\(\Omega \in \mathbb{R}^{n \times (k+p)}\)</span></p></li>
<li><p>berechne damit</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  Y = A\Omega \in \mathbb{R}^{m \times (k+p)}
  \end{equation*}\]</div>
<ul class="simple">
<li><p>orthonormiere die Spalten von <span class="math notranslate nohighlight">\(Y\)</span> und erzeuge damit die orthonormale Matrix</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  Q \in \mathbb{R}^{m \times (k+p)}
  \end{equation*}\]</div>
<p>Dabei sind folgende Fragen noch offen:</p>
<ul class="simple">
<li><p>welches <span class="math notranslate nohighlight">\(\Omega\)</span> und welches <span class="math notranslate nohighlight">\(p\)</span> soll benutzt werden?</p></li>
<li><p>wie orthonormalisiert man <span class="math notranslate nohighlight">\(Y\)</span>, das in der Regel schlechte Kondition hat
(Spalten sind „fast“ linear abhängig)?</p></li>
<li><p>wie ist der Aufwand, die Genauigkeit?</p></li>
<li><p>wie geht fixed-prescision?</p></li>
</ul>
<p><strong>Satz:</strong> <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span>, <span class="math notranslate nohighlight">\(k\geq 2\)</span>, <span class="math notranslate nohighlight">\(p\geq 2\)</span>, <span class="math notranslate nohighlight">\(k+p \leq \min(m,n)\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\Omega = \big(\omega_{ij}\big)_{i,j} \in  \mathbb{R}^{n \times (k+p)},
\quad
\omega_{ij} \sim \mathcal{N}(0,1) \ \text{iid},  
\end{equation*}\]</div>
<p>dann gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
\mathbb{E}_\Omega\big(\|A - QQ^TA\|_2\big) 
&amp;\leq (1+\delta)\sigma_{k+1},
\\
\delta 
&amp;= \frac{4\sqrt{k+p}}{p-1}\sqrt{\min(m,n)}
\end{align*}\]</div>
<p>bzw.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
\mathbb{P}\big(\|A - QQ^TA\|_2 
\leq (1+\tilde{\delta})\sigma_{k+1}\big)
&amp;\geq
1 - 3p^{-p},
\\
\tilde{\delta} 
&amp;= 9 \sqrt{k+p} \sqrt{\min(m,n)}.
\end{align*}\]</div>
<p><strong>Bemerkung:</strong></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\min_{X\in \mathbb{R}^{m \times n}, \mathrm{rang}(X)\leq k} \|A-X\|_2 = \sigma_{k+1}\)</span>
so dass <span class="math notranslate nohighlight">\(\delta\)</span>, <span class="math notranslate nohighlight">\(\tilde{\delta}\)</span> die Abweichung vom Optimum angibt</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta\)</span>, <span class="math notranslate nohighlight">\(\tilde{\delta}\)</span> wächst nur schwach in <span class="math notranslate nohighlight">\(k,m,n\)</span></p></li>
<li><p>es ist (unabhängig von <span class="math notranslate nohighlight">\(k,m,n\)</span>)</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  3 p^{-p}
  \approx
  \begin{cases}
  10^{-3} &amp; p=5 \\
  3\cdot 10^{-10} &amp; p=10 
  \end{cases},
  \end{equation*}\]</div>
<p>so dass für recht kleine <span class="math notranslate nohighlight">\(p\)</span>-Werte
<span class="math notranslate nohighlight">\(\mathbb{P}\big(\|A - QQ^TA\|_2 \leq (1+\tilde{\delta})\sigma_{k+1}\big)\approx 1\)</span></p>
</li>
<li><p>fallen die Singulärwerte schnell ab, so dass <span class="math notranslate nohighlight">\(\sigma_{k+1}\)</span> klein ist, dann
sind die Approximationen sehr gut</p></li>
</ul>
</div>
<div class="section" id="randomized-svd">
<h2>Randomized SVD<a class="headerlink" href="#randomized-svd" title="Link zu dieser Überschrift">¶</a></h2>
<p>Wir wenden jetzt die Methoden des letzten Abschnitts, um eine approximative SVD
von <span class="math notranslate nohighlight">\(A\)</span> zu berechnen.</p>
<p>Eine direkte Anwendung liefert oft schlechte Ergebnisse, insbesondere wenn die
Singulärwerte von <span class="math notranslate nohighlight">\(A\)</span> nur langsam abfallen.
Statt <span class="math notranslate nohighlight">\(Y=A\Omega\)</span> benutzt man dann die Power-Methode</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  Y = (AA^T)^q A\Omega,
  \end{equation*}\]</div>
<p>wobei in der Praxis meist <span class="math notranslate nohighlight">\(q=1\)</span> oder <span class="math notranslate nohighlight">\(q=2\)</span> ausreicht.</p>
<p>Es gilt <span class="math notranslate nohighlight">\(R\big((AA^T)^q A\big)\subset R(A)\)</span> und mit <span class="math notranslate nohighlight">\(A=U\Sigma V^T\)</span> folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  AA^T = U\Sigma\Sigma^T U^T
  \end{equation*}\]</div>
<p>also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
  (AA^T)^q A 
  &amp;= (U\Sigma\Sigma^T U^T)^q U\Sigma V^T\\
  &amp;= U(\Sigma\Sigma^T)^q\Sigma V^T\\
  &amp;= U\Sigma_{q+1} V^T
  \end{align*}\]</div>
<p>mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \quad
  \Sigma_{q+1} = \mathrm{diag}(\sigma_i^{q+1}),
  \end{equation*}\]</div>
<p>wobei <span class="math notranslate nohighlight">\(\sigma_i^{q+1}\)</span> für <span class="math notranslate nohighlight">\(q&gt;0\)</span> schneller abfällt als <span class="math notranslate nohighlight">\(\sigma_i\)</span>.</p>
<p>Damit erhalten wir schließlich folgenden Algorithmus für RSVD:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span>, <span class="math notranslate nohighlight">\(k\in\mathbb{N}\)</span>, <span class="math notranslate nohighlight">\(p,q\in\mathbb{N}_0\)</span></p></li>
<li><p>Schritt 1:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\Omega = \big(\omega_{ij}\big)_{i,j} \in \mathbb{R}^{n \times (k+p)}\)</span>,
<span class="math notranslate nohighlight">\(\omega_{ij} \sim \mathcal{N}(0,1)\)</span> iid</p></li>
<li><p><span class="math notranslate nohighlight">\(Y = (AA^T)^q A\Omega \in \mathbb{R}^{m \times (k+p)}\)</span></p></li>
<li><p>orthonormalisiere die Spalten von <span class="math notranslate nohighlight">\(Y\)</span> und lege das Ergebnis in
<span class="math notranslate nohighlight">\(Q\in \mathbb{R}^{m \times (k+p)}\)</span> ab</p></li>
</ul>
</li>
<li><p>Schritt 2:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(C = Q^T A\in \mathbb{R}^{(k+p) \times n}\)</span></p></li>
<li><p>berechne SVD von <span class="math notranslate nohighlight">\(C\)</span>, <span class="math notranslate nohighlight">\(C = \tilde{U}\Sigma V^T\)</span></p></li>
<li><p>setze <span class="math notranslate nohighlight">\(U = Q\tilde{U}\)</span></p></li>
</ul>
</li>
<li><p>insgesamt ist dann <span class="math notranslate nohighlight">\(A \approx U\Sigma V^T\)</span></p></li>
</ul>
<p><strong>Bemerkung:</strong></p>
<ul class="simple">
<li><p>man berechnet eine Rang-<span class="math notranslate nohighlight">\((k+p)\)</span>-Approximation um eine (gute) Näherung für die
ersten <span class="math notranslate nohighlight">\(k\)</span> Singulärwerte und -vektoren zu erhalten</p></li>
<li><p>die Produkte <span class="math notranslate nohighlight">\(AA^TA\Omega\)</span> sind anfällig gegen Rundungsfehler, weshalb man nach
jedem Schritt orthonormalisieren sollte</p></li>
</ul>
<p><strong>Satz:</strong> <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span>, <span class="math notranslate nohighlight">\(q\in\mathbb{N}_0\)</span>,
<span class="math notranslate nohighlight">\(2 \leq k \leq \frac{1}{2}\min(m,n)\)</span>, dann gilt für <span class="math notranslate nohighlight">\(U,\Sigma,V\)</span> aus der RSVD
mit <span class="math notranslate nohighlight">\(p=k\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
  \mathbb{E}_\Omega\big(\|A - U\Sigma V^T\|_2\big) 
  &amp;\leq 
  (1+\delta)^{\frac{1}{2q+1}}\sigma_{k+1},
  \\
  \delta &amp;= 4 \sqrt{\frac{2\min(m,n)}{k-1}}
  \end{align*}\]</div>
<p>Da wir nur an den ersten <span class="math notranslate nohighlight">\(k\)</span> Approximationen der Singulärwerte und -vektoren
interessiert sind, ist es naheliegend, die <span class="math notranslate nohighlight">\(\sigma_i\)</span> für <span class="math notranslate nohighlight">\(i&gt;k\)</span> abzuschneiden.
Wir ersetzen oben <span class="math notranslate nohighlight">\(\Sigma\)</span> durch</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\Sigma_k =  \mathrm{diag}(\sigma_1,\ldots,\sigma_k,0,\ldots,0),
\end{equation*}\]</div>
<p>d.h. wir benutzen
eine TSVD von <span class="math notranslate nohighlight">\(C\)</span> in Schritt 2.
In diesem Fall ändert sich die Fehleranschätzung des letzten Satzes geringfügig.</p>
<p><strong>Satz:</strong> <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span>, <span class="math notranslate nohighlight">\(q\in\mathbb{N}_0\)</span>,
<span class="math notranslate nohighlight">\(2 \leq k \leq \frac{1}{2}\min(m,n)\)</span>, dann gilt für <span class="math notranslate nohighlight">\(U,\Sigma_k,V\)</span> aus der RSVD
mit <span class="math notranslate nohighlight">\(p=k\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
\mathbb{E}_\Omega\big(\|A - U\Sigma_k V^T\|_2\big) 
&amp;\leq 
(1+\delta)^{\frac{1}{2q+1}}\sigma_{k+1} + \sigma_{k+1},
\\
\delta &amp;= 4 \sqrt{\frac{2\min(m,n)}{k-1}}
\end{align*}\]</div>
<p>Damit ist die fixed-rank Variante der RSVD soweit erledigt.</p>
</div>
<div class="section" id="fixed-precision">
<h2>Fixed-Precision<a class="headerlink" href="#fixed-precision" title="Link zu dieser Überschrift">¶</a></h2>
<p>Das fixed-precision Problem könnte wie folgt auf die fixed-rank Variante
zurückgeführt werden:</p>
<ul class="simple">
<li><p>starte mit fixed-rank mit kleinem <span class="math notranslate nohighlight">\(k\)</span></p></li>
<li><p>teste, ob <span class="math notranslate nohighlight">\(\|A - QQ^TA\|_2 \leq \varepsilon\)</span></p></li>
<li><p>wenn nicht, dann erhöhe <span class="math notranslate nohighlight">\(k\)</span> und wiederhole den vorherigen Schritt</p></li>
</ul>
<p>Das Berechnen der Matrixnorm <span class="math notranslate nohighlight">\(\|A - QQ^TA\|_2\)</span> ist in der Regel zu aufwendig,
deshalb benötigt man dafür einen einfachen a-posteriori Fehlerindikator.
Auch hier hilft uns wieder der „Zufall“ weiter.</p>
<p><strong>Lemma:</strong> Sei <span class="math notranslate nohighlight">\(B\in\mathbb{R}^{m\times n}\)</span>, <span class="math notranslate nohighlight">\(\omega^{(j)}\in\mathbb{R}^n\)</span>,
<span class="math notranslate nohighlight">\(j=1,\ldots,r\)</span>,
<span class="math notranslate nohighlight">\(\omega_i^{(j)} \sim \mathcal{N}(0,1)\)</span>  iid und <span class="math notranslate nohighlight">\(\alpha &gt; 1\)</span>.
Dann gilt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\mathbb{P}
\big(
\|B\|_2 
\leq 
\alpha\sqrt{\frac{2}{\pi}} \ \max_{j=1,\ldots,r}\|B\omega^{(j)}\|_2
\big)
\geq
1 - \alpha^{-r}.
\end{equation*}\]</div>
<p>Für die Praxis bedeutet das:</p>
<ul>
<li><p>erzeuge <span class="math notranslate nohighlight">\(\omega^{(j)}\)</span>,  <span class="math notranslate nohighlight">\(j=1,\ldots,r\)</span> für <span class="math notranslate nohighlight">\(r\)</span> klein</p></li>
<li><p>mit <span class="math notranslate nohighlight">\(m_r = \max_{j=1,\ldots,r}\|(A-QQ^TA)\omega^{(j)}\|_2\)</span> und <span class="math notranslate nohighlight">\(\alpha=10\)</span> folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \mathbb{P}
  \big(
  \|A-QQ^TA\|_2 
  \leq 
  10\sqrt{\frac{2}{\pi}} \ m_r
  \big)
  \geq
  1 - 10^{-r},
  \end{equation*}\]</div>
<p>so dass <span class="math notranslate nohighlight">\(r\leq 10\)</span> ausreichend ist</p>
</li>
</ul>
<p>Prinzipiell wenden wir beim diesem Fehlerschätzer die gleichen Operationen
wie bei der Gaussian-Projection</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
Y = A\Omega,
\quad
\Omega = \big(\omega_{ij}\big)_{i,j} \in  \mathbb{R}^{n \times (k+p)},
\quad
\omega_{ij} \sim \mathcal{N}(0,1) \ \text{iid}
\end{equation*}\]</div>
<p>an, nur dass wir statt der Matrix <span class="math notranslate nohighlight">\(\Omega\)</span> einzelne Vektoren <span class="math notranslate nohighlight">\(\omega^{(j)}\)</span>
benutzen.</p>
<p>Ordnen wir die Berechnungsschritte etwas um, dann können wir den Fehlerschätzer
praktisch ohne Mehraufwand in unser fixed-rank-Verfahren integrieren:</p>
<ul>
<li><p>gegeben sind <span class="math notranslate nohighlight">\(A\in\mathbb{R}^{m\times n}\)</span>, <span class="math notranslate nohighlight">\(r\in\mathbb{N}\)</span>, <span class="math notranslate nohighlight">\(\varepsilon&gt;0\)</span>:</p>
<ul class="simple">
<li><p>erzeuge <span class="math notranslate nohighlight">\(\omega^{(j)}\in\mathbb{R}^n\)</span>, <span class="math notranslate nohighlight">\(j=1,\ldots,r\)</span>,
<span class="math notranslate nohighlight">\(\omega_i^{(j)} \sim \mathcal{N}(0,1)\)</span>  iid</p></li>
<li><p><span class="math notranslate nohighlight">\(y^{(j)} = A \omega^{(j)}\)</span>, <span class="math notranslate nohighlight">\(j=1,\ldots,r\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(k=0\)</span>, <span class="math notranslate nohighlight">\(Q^{(0)} = [\ ]\)</span> (leer)</p></li>
<li><p>wiederhole bis
<span class="math notranslate nohighlight">\(\displaystyle 
\max_{j=1,\ldots,r}\|y^{(k+j)}\|_2 \leq \frac{\varepsilon}{10\sqrt{\frac{2}{\pi}}}\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(k := k+1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y^{(k)} := \big(I - Q^{(k-1)}{Q^{(k-1)}}^T\big) y^{(k)}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(q^{(k)} := \frac{y^{(k)}}{\|y^{(k)}\|_2}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Q^{(k)} = [Q^{(k-1)}, q^{(k)}]\)</span></p></li>
<li><p>erzeuge <span class="math notranslate nohighlight">\(\omega^{(k+r)} \in \mathbb{R}^n\)</span>,
<span class="math notranslate nohighlight">\(\omega^{(k+r)} \sim \mathcal{N}(0,1)\)</span> iid</p></li>
<li><p><span class="math notranslate nohighlight">\(y^{(k+r)} := \big(I - Q^{(k)}{Q^{(k)}}^T\big) A \omega^{(k+r)}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y^{(l)} := y^{(l)} - {q^{(k)}}^T y^{(l)} {q^{(k)}}\)</span> für <span class="math notranslate nohighlight">\(l = k+1,k+2,\ldots,k+r-1\)</span></p></li>
</ul>
</li>
<li><p>mit <span class="math notranslate nohighlight">\(Q = Q^{(j)}\)</span> gilt dann</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \mathbb{P}\big(\|A-QQ^TA\|_2 \leq \varepsilon\big) \geq 1- 10^{-r}\min(m,n)
  \end{equation*}\]</div>
</li>
</ul>
<p>Damit ist <span class="math notranslate nohighlight">\(Q\)</span> bestimmt.
Der Rest der RSVD ist identisch zum fixed-rank-Fall, insbesondere
kann dabei auch wieder die Power-Methode benutzt werden.</p>
</div>
<div class="section" id="zusammenfassung">
<h2>Zusammenfassung<a class="headerlink" href="#zusammenfassung" title="Link zu dieser Überschrift">¶</a></h2>
<p>Wir haben probabilistische Methoden für approximative Berechnung von TSVD
betrachtet und Algortihmen für fixed-rank und fixed-precision Aufgabenstellungen
vorgestellt.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="14_Stochastic_Gradient_Descent.html" title="zurück Seite">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">zurück</p>
                <p class="prevnext-title">Stochastic Gradient Descent</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="99_Literatur.html" title="weiter Seite">
            <div class="prevnext-info">
                <p class="prevnext-label">weiter</p>
                <p class="prevnext-title">Weiterführende Links</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          Martin Reißel<br/>
        
            2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>