
<!DOCTYPE html>

<html lang="de">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Neuronale Netze &#8212; Numerische Algorithmen für Maschinelles Lernen WS21/22 (Version 0.42)</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Stichwortverzeichnis" href="genindex.html" />
    <link rel="search" title="Suche" href="search.html" />
    <link rel="next" title="Topic Extraction, NMF" href="07_Topic_Extraction.html" />
    <link rel="prev" title="Support-Vector Klassifikation" href="05_Klassifikation_mit_SVM.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="de">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Numerische Algorithmen für Maschinelles Lernen WS21/22 (Version 0.42)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00_Vorwort.html">
                    Numerische Algorithmen für Maschinelles Lernen (Version 0.42)
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_Regression.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_Dimensionsreduktion.html">
   Dimensionsreduktion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_Regularisierung.html">
   Regularisierung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_Background_Removal_QR.html">
   Background Removal mit TSVD
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_Klassifikation_mit_SVM.html">
   Support-Vector Klassifikation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neuronale Netze
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_Topic_Extraction.html">
   Topic Extraction, NMF
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_Grundlagen_Optimierung.html">
   Grundlagen der Optimierung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_Konvexitaet.html">
   Konvexität
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_Gradient_Descent.html">
   Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_Projected_Gradient_Descent.html">
   Projected Gradient-Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12_Subgradient_Descent.html">
   Subgradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13_Proximal_Gradient_Descent.html">
   Proximal Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14_Stochastic_Gradient_Descent.html">
   Stochastic Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15_Probabilistische_Lineare_Algebra.html">
   Probabilistische Lineare Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="99_Literatur.html">
   Weiterführende Links
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/mre2110/NumMLv042/master?urlpath=tree/06_Neuronale_Netze.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/06_Neuronale_Netze.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uberblick">
   Überblick
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-layer-perceptron-mlp">
   Multi-Layer Perceptron (MLP)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modellproblem-1-neuron-mlp">
   Modellproblem 1-Neuron-MLP
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scikit-learn">
     Scikit-Learn
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#keras-tensorflow">
     Keras-Tensorflow
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pytorch">
     Pytorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sub-gradient-descent">
   (Sub)Gradient-Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#accelerated-gradient-descent-nesterov">
   Accelerated Gradient-Descent (Nesterov)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stochastic-sub-gradient-descent">
   Stochastic (Sub)Gradient-Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#backpropagation">
   Backpropagation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#zusammenfassung">
   Zusammenfassung
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Neuronale Netze</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uberblick">
   Überblick
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-layer-perceptron-mlp">
   Multi-Layer Perceptron (MLP)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modellproblem-1-neuron-mlp">
   Modellproblem 1-Neuron-MLP
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scikit-learn">
     Scikit-Learn
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#keras-tensorflow">
     Keras-Tensorflow
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pytorch">
     Pytorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sub-gradient-descent">
   (Sub)Gradient-Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#accelerated-gradient-descent-nesterov">
   Accelerated Gradient-Descent (Nesterov)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stochastic-sub-gradient-descent">
   Stochastic (Sub)Gradient-Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#backpropagation">
   Backpropagation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#zusammenfassung">
   Zusammenfassung
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="neuronale-netze">
<h1>Neuronale Netze<a class="headerlink" href="#neuronale-netze" title="Link zu dieser Überschrift">#</a></h1>
<section id="uberblick">
<h2>Überblick<a class="headerlink" href="#uberblick" title="Link zu dieser Überschrift">#</a></h2>
<p>Neuronale Netze sind relativ einfach strukturierte Modellfunktionen mit sehr vielen (teilweise auch redundanten) Parametern. Die Anpassung dieser Parameter führt zu
hochdimensionalen, nicht-konvexen Optimierungsproblemen.</p>
<p>Anhand einfacher Beispiele wird das Verhalten von konventionellen bzw. stochastischen Gradienten-Verfahren untersucht. Außerdem werden beschleunigte Varianten betrachtet.</p>
</section>
<section id="multi-layer-perceptron-mlp">
<h2>Multi-Layer Perceptron (MLP)<a class="headerlink" href="#multi-layer-perceptron-mlp" title="Link zu dieser Überschrift">#</a></h2>
<p>Ein einfaches neuronales Netz das zur Bearbeitung von
Klassifikations- und Regressionsproblemen eingesetzt werden kann
ist das <strong>Multi-Layer Perceptron</strong> (MLP).
Bei beiden Problemklassen wird versucht mit Hilfe
eines Trainigsdatensatzes eine parameterabhängige Modellfunktion</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
g:\mathbb{R}^n \times \mathbb{R}^p \to \mathbb{R}^o,
\quad
x,w \mapsto g(x,w)
\end{equation*}\]</div>
<p>anzupassen,
die die Inputs möglichst genau auf die Outputs abbildet.</p>
<p>Bei der linearen Regression haben wir
einen linear affinen Ansatz der Form</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
g(x, w) = G(x) w + c(x)
\end{equation*}\]</div>
<p>benutzt und dann die Parameter <span class="math notranslate nohighlight">\(w\)</span>
mit Hilfe der Trainingsdaten möglichst gut (bezüglich des benutzten Loss)
angepasst. Die dabei entstehenden Optimierungsprobleme
waren relativ einfach beherrschbar.</p>
<p>Beim MLP benutzt man einen anderen, allgemeineren
Ansatz für <span class="math notranslate nohighlight">\(g\)</span>. Wir betrachten hier den Fall, dass der Output
skalar ist, d.h. <span class="math notranslate nohighlight">\(g:\mathbb{R}^n\times \mathbb{R}^p  \to \mathbb{R}\)</span>.</p>
<p><img alt="MLP, \url{http://scikit-learn.org/stable/modules/neural_networks_supervised.html}" src="_images/mlp.png" /></p>
<p>Ein MLP hat dann folgende Struktur (Graphik von http://scikit-learn.org/stable/modules/neural_networks_supervised.html):</p>
<ul>
<li><p>der Input <span class="math notranslate nohighlight">\(X\)</span> besteht aus den Komponenten
<span class="math notranslate nohighlight">\(X=(x_1,\ldots,x_n)\)</span>, sie bilden die <strong>Input-Layer</strong></p></li>
<li><p>Linearkombinationen der Form</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  u_i = w_{i0}^{(1)} +  w_{i1}^{(1)}x_1 + \cdots w_{in}^{(1)}x_n, \quad i = 1,\ldots, k
  \end{equation*}\]</div>
<p>werden als Input für die „Neuronen“
<span class="math notranslate nohighlight">\(a_1,\ldots, a_k\)</span> der <strong>Hidden-Layer</strong> benutzt</p>
</li>
<li><p>jedes Neuron <span class="math notranslate nohighlight">\(a_i\)</span> wendet dann auf seinen skalaren Input <span class="math notranslate nohighlight">\(u_i\)</span>
eine skalare Funktion <span class="math notranslate nohighlight">\(a:\mathbb{R}\to\mathbb{R}\)</span>,
die <strong>Aktivierungsfunktion</strong>, an, d.h. wir erhalten
als Output der „Neuronen“ <span class="math notranslate nohighlight">\(a_i\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  v_i = a(u_i), \quad  i = 1,\ldots,k
  \end{equation*}\]</div>
</li>
<li><p>der Output <span class="math notranslate nohighlight">\(g(x,w)\)</span> wird schließlich als Linearkombination</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  g(x, w) = w_0^{(2)} +  w_{1}^{(2)} v_1 + \cdots w_{k}^{(2)} v_k
  \end{equation*}\]</div>
<p>berechnet</p>
</li>
</ul>
<p><span class="math notranslate nohighlight">\(g\)</span> ist also durch</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\big(w_{ij}^{(1)} \big)_{\substack{i = 1,\ldots, k\\ j = 0,\ldots, n}},
\quad
\big(w_{j}^{(2)} \big)_{j = 0,\ldots, k}
\end{equation*}\]</div>
<p>parametriert, d.h. die Anzahl der Parameter ist hoch.
Da <span class="math notranslate nohighlight">\(a\)</span> nichtlinear ist, ist <span class="math notranslate nohighlight">\(g\)</span> auch nichtlinear.</p>
<p>Der Ansatz kann problemlos auf mehrere Hidden-Layers
sowie vektorwertige Zielfunktionen verallgemeinert werden.</p>
<p>Mit Hilfe des Trainingsdatensatzes werden die Parameter <span class="math notranslate nohighlight">\(w\)</span>
von <span class="math notranslate nohighlight">\(g\)</span> so bestimmt, dass der Loss minimiert wird.
Die dabei auftretenden Optimierungsprobleme sind i.d.R.
nicht konvex und werden üblicherweise mit Varianten
des <a class="reference external" href="https://de.wikipedia.org/wiki/Gradientenverfahren">Gradientenverfahrens</a>
näherungsweise gelöst.</p>
<p>Für die Berechnung der dabei benötigten Ableitungen nach den Parametern <span class="math notranslate nohighlight">\(w\)</span> gibt es effiziente Methoden (<a class="reference external" href="https://en.wikipedia.org/wiki/Backpropagation">Backpropagation</a>).</p>
</section>
<section id="modellproblem-1-neuron-mlp">
<h2>Modellproblem 1-Neuron-MLP<a class="headerlink" href="#modellproblem-1-neuron-mlp" title="Link zu dieser Überschrift">#</a></h2>
<p>Um ein Gefühl für das Trainingsverhalten eines Netzes zu bekommen, betrachten
wir ein triviales Netz mit einem skalaren Input, einem skalaren Output,
einer Schicht mit einem Neuron und Aktivierungsfunktion
<span class="math notranslate nohighlight">\(r(x) = \max(0, x)\)</span>
(<strong>RELU</strong>, <strong>Re</strong>ctified <strong>L</strong>inear <strong>U</strong>nit), d.h.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
g(x, w) = r(w_1 \, x + w_2)\,w_3 + w_4,
\quad
r(x) = \max(0, x).
\end{equation*}\]</div>
<p>Als Trainingsdatensatz wird</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
x_i = y_i = \frac{i}{n}, \quad i = 0,\ldots,n, \quad n=10
\end{equation*}\]</div>
<p>benutzt.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">copy</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">123</span>

<span class="o">%</span><span class="k">matplotlib</span> inline


<span class="c1">## Parameter festlegen</span>

<span class="c1"># Anzahl Neuronen</span>
<span class="n">nn</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Anzahl Trainingssample</span>
<span class="n">ntrain</span> <span class="o">=</span> <span class="mi">11</span>

<span class="n">a</span> <span class="o">=</span>  <span class="mi">0</span>
<span class="n">b</span> <span class="o">=</span>  <span class="mi">1</span>

<span class="c1"># Anzahl Plotpunkte</span>
<span class="n">nplot</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">Xplot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">b</span><span class="o">+</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">nplot</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>


<span class="c1">## Daten erzeugen</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1">#g = lambda x: np.sin(10*x*x)</span>
<span class="n">g</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span>

<span class="n">Xtrain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">ntrain</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="c1">#Xtrain = (np.random.rand(ntrain) * (b - a) + a).reshape(-1,1)</span>
<span class="n">Xtrain</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">ytrain</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">)</span><span class="c1">#.ravel()</span>
<span class="c1">#ytrain = ytrain + 0.1 * np.random.randn(*ytrain.shape)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06_Neuronale_Netze_10_0.png" src="_images/06_Neuronale_Netze_10_0.png" />
</div>
</div>
<p>Die Parameter <span class="math notranslate nohighlight">\(w\)</span> sollen so bestimmt werden, dass</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
    l(w) = \frac{1}{n} \sum_{i=1}^n \big(g(x_i, w) -y_i\big)^2
\end{equation*}\]</div>
<p>minimiert wird.
Da <span class="math notranslate nohighlight">\(g\)</span> nichtlinear in <span class="math notranslate nohighlight">\(w\)</span> ist, ist auch <span class="math notranslate nohighlight">\(f\)</span> nichtlinear.</p>
<p>Für die Parameter <span class="math notranslate nohighlight">\(\hat{w} = (1, 0, 1, 0)^T\)</span> erhalten wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
g(x,\hat{w}) = r(x) = \max(0,x),
\end{equation*}\]</div>
<p>d.h. <span class="math notranslate nohighlight">\(g(x,\hat{w})\)</span> interpoliert die Daten <span class="math notranslate nohighlight">\(x_i, y_i\)</span> exakt.
Damit ist</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
l(\hat{w}) = 0
\end{equation*}\]</div>
<p>und <span class="math notranslate nohighlight">\(\hat{w}\)</span> globales Minimum von <span class="math notranslate nohighlight">\(l\)</span>.</p>
<p>Andererseits gilt für <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
r(\alpha x) = \max(0, \alpha x) = \alpha \max(0, x) = \alpha r(x)
\end{equation*}\]</div>
<p>dass auch</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\hat{w}_\alpha = \big(\alpha, 0, \frac{1}{\alpha}, 0\big), \quad \forall \alpha &gt; 0
\end{equation*}\]</div>
<p>ein globales Minimum von <span class="math notranslate nohighlight">\(l\)</span> ist und analog auch</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\hat{w}_\beta = \big(1, \beta, 1, -\beta \big), \quad \forall \beta &gt; 0.
\end{equation*}\]</div>
<p>Damit kann <span class="math notranslate nohighlight">\(l\)</span> nicht strikt konvex sein. Wie das folgende Beispiel zeigt ist <span class="math notranslate nohighlight">\(l\)</span> nicht einmal konvex.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#%matplotlib notebook </span>
<span class="o">%</span><span class="k">matplotlib</span> inline 

<span class="n">relu</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">l</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w3</span><span class="p">):</span>
    <span class="n">fw</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">):</span>
        <span class="n">fw</span> <span class="o">+=</span> <span class="p">(</span><span class="n">relu</span><span class="p">(</span><span class="n">w1</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">w3</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">fw</span><span class="o">/</span><span class="n">Xtrain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">l</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>


<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

<span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">w3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1">#w1 = np.array([ 0, 1])</span>
<span class="c1">#w3 = np.array([-1, 0])</span>

<span class="n">g1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">w1</span><span class="p">)</span>
<span class="n">g3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">w3</span><span class="p">)</span>

<span class="n">ww3</span><span class="p">,</span> <span class="n">ww1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">g3</span><span class="p">,</span> <span class="n">g1</span><span class="p">)</span>
<span class="n">ww1</span> <span class="o">=</span> <span class="n">ww1</span><span class="o">.</span><span class="n">T</span>
<span class="n">ww3</span> <span class="o">=</span> <span class="n">ww3</span><span class="o">.</span><span class="n">T</span>
<span class="n">ff</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">ww1</span><span class="p">,</span> <span class="n">ww3</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">ww1</span><span class="p">,</span> <span class="n">ww3</span><span class="p">,</span> <span class="n">ff</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">jet</span><span class="p">)</span>
<span class="n">cc</span> <span class="o">=</span> <span class="n">ff</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="c1">#cc = 10*np.linspace(0,1)**5</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">ww1</span><span class="p">,</span> <span class="n">ww3</span><span class="p">,</span> <span class="n">ff</span><span class="p">,</span> <span class="n">cc</span><span class="p">)</span>

<span class="n">zoff</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">ww1</span><span class="p">,</span> <span class="n">ww3</span><span class="p">,</span> <span class="n">ff</span><span class="p">,</span> <span class="n">cc</span><span class="p">,</span> <span class="n">zdir</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">zoff</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">jet</span><span class="p">)</span>
<span class="c1">#plt.contour(ww1, ww3, f(ww1, ww3), zdir=&#39;x&#39;, cmap = plt.cm.jet);</span>

<span class="n">l1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">w1</span><span class="p">)</span>
<span class="n">l3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">w3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot3D</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l3</span><span class="p">,</span> <span class="n">l</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l3</span><span class="p">),</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>

<span class="n">l1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">w1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">l3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">w3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot3D</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l3</span><span class="p">,</span> <span class="n">zoff</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">l1</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="n">zoff</span><span class="p">,</span> <span class="n">ff</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$w_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$w_3$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;$f$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">150</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06_Neuronale_Netze_13_0.png" src="_images/06_Neuronale_Netze_13_0.png" />
</div>
</div>
<p>Für <span class="math notranslate nohighlight">\(w_2 = w_4 = 0\)</span> sind die Funktionswerte entlang der
Strecke <span class="math notranslate nohighlight">\((w_1,w_3) = (0,-1)\)</span> nach <span class="math notranslate nohighlight">\((w_1,w_3) = (2,1)\)</span> dargestellt.</p>
<p>Die fehlende Konvexität wird uns beim Anpassen der Parameter <span class="math notranslate nohighlight">\(w\)</span> noch viel „Freude“ bereiten.</p>
<p>Diese Anpassung werden wir nun mit 3 der gängigsten Software Tools vornehmen.</p>
<section id="scikit-learn">
<h3>Scikit-Learn<a class="headerlink" href="#scikit-learn" title="Link zu dieser Überschrift">#</a></h3>
<p>Wir passen einen <code class="docutils literal notranslate"><span class="pre">MLPRegressor</span></code> an und benutzen die Default-Einstellungen.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neural_network</span>

<span class="n">mlp</span> <span class="o">=</span> <span class="n">neural_network</span><span class="o">.</span><span class="n">MLPRegressor</span><span class="p">(</span><span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="p">],</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="n">seed</span><span class="p">)</span>
<span class="n">mlp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="o">.</span><span class="n">flat</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">ev</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="s1">&#39;b.&#39;</span><span class="p">);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xplot</span><span class="p">,</span> <span class="n">mlp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xplot</span><span class="p">),</span> <span class="n">c</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">);</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;solver = </span><span class="si">{}</span><span class="s2">, score = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">solver</span><span class="p">,</span> <span class="n">mlp</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)))</span>

<span class="n">ev</span><span class="p">(</span><span class="n">mlp</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>solver = adam, score = -0.19949628496033456
</pre></div>
</div>
<img alt="_images/06_Neuronale_Netze_18_1.png" src="_images/06_Neuronale_Netze_18_1.png" />
</div>
</div>
<p>Das Ergebnis ist unbrauchbar.</p>
<p>Der Startwert für den iterativen Löser wird zufällig gewählt und kann
über den Parameter <code class="docutils literal notranslate"><span class="pre">random_state</span></code> beeinflusst werden</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">neural_network</span><span class="o">.</span><span class="n">MLPRegressor</span><span class="p">(</span><span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="p">],</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">234</span><span class="p">)</span>
<span class="n">mlp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="o">.</span><span class="n">flat</span><span class="p">)</span>
<span class="n">ev</span><span class="p">(</span><span class="n">mlp</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>solver = adam, score = -0.5531367823099405
</pre></div>
</div>
<img alt="_images/06_Neuronale_Netze_20_1.png" src="_images/06_Neuronale_Netze_20_1.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">neural_network</span><span class="o">.</span><span class="n">MLPRegressor</span><span class="p">(</span><span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="p">],</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">314159</span><span class="p">)</span>
<span class="n">mlp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="o">.</span><span class="n">flat</span><span class="p">)</span>
<span class="n">ev</span><span class="p">(</span><span class="n">mlp</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>solver = adam, score = 0.8664752096370867
</pre></div>
</div>
<img alt="_images/06_Neuronale_Netze_21_1.png" src="_images/06_Neuronale_Netze_21_1.png" />
</div>
</div>
<p>Die Ergebnisse hängen offensichtlich extrem stark vom Startwert ab. Die Qualität ist insgesamt sehr dürftig.</p>
</section>
<section id="keras-tensorflow">
<h3>Keras-Tensorflow<a class="headerlink" href="#keras-tensorflow" title="Link zu dieser Überschrift">#</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Activation</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
<span class="p">[</span>
 <span class="n">Dense</span><span class="p">(</span><span class="n">units</span> <span class="o">=</span> <span class="n">nn</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">),</span>
 <span class="c1">#Dense(units = nn, input_dim = 1, kernel_initializer = keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=seed)),</span>
 <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
 <span class="c1">#Activation(&#39;tanh&#39;),</span>
 <span class="n">Dense</span><span class="p">(</span><span class="n">units</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 1)                 2         
                                                                 
 activation (Activation)     (None, 1)                 0         
                                                                 
 dense_1 (Dense)             (None, 1)                 2         
                                                                 
=================================================================
Total params: 4
Trainable params: 4
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;nadam&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">0</span><span class="p">);</span>

<span class="k">def</span> <span class="nf">kev</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">g</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">),</span> <span class="s1">&#39;b.&#39;</span><span class="p">);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xplot</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xplot</span><span class="p">),</span> <span class="n">c</span><span class="p">);</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loss = </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)[</span><span class="mi">0</span><span class="p">]))</span>
    
<span class="n">kev</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>    
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - 0s 65ms/step - loss: 2.4682 - accuracy: 0.0909
loss = 2.4682343006134033
</pre></div>
</div>
<img alt="_images/06_Neuronale_Netze_25_1.png" src="_images/06_Neuronale_Netze_25_1.png" />
</div>
</div>
</section>
<section id="pytorch">
<h3>Pytorch<a class="headerlink" href="#pytorch" title="Link zu dieser Überschrift">#</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">tnn</span>

<span class="n">xt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">yt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">ytrain</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>

<span class="n">dst</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">)</span>
<span class="n">dlt</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Percep</span><span class="p">(</span><span class="n">tnn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">tnn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">nn</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act1</span>    <span class="o">=</span> <span class="n">tnn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">tnn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Percep</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Percep(
  (linear1): Linear(in_features=1, out_features=1, bias=True)
  (act1): ReLU()
  (linear2): Linear(in_features=1, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span>

<span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">opt</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="ow">in</span> <span class="n">dlt</span><span class="p">:</span>
            <span class="c1"># Generate predictions</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
            <span class="c1"># Perform gradient descent</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training loss: &#39;</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xt</span><span class="p">),</span> <span class="n">yt</span><span class="p">))</span>

<span class="n">fit</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">pev</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">g</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">),</span> <span class="s1">&#39;b.&#39;</span><span class="p">);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xt</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">model</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;r&#39;</span><span class="p">);</span>

<span class="n">pev</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training loss:  tensor(0.1828, grad_fn=&lt;MseLossBackward0&gt;)
</pre></div>
</div>
<img alt="_images/06_Neuronale_Netze_28_1.png" src="_images/06_Neuronale_Netze_28_1.png" />
</div>
</div>
</section>
</section>
<section id="sub-gradient-descent">
<h2>(Sub)Gradient-Descent<a class="headerlink" href="#sub-gradient-descent" title="Link zu dieser Überschrift">#</a></h2>
<p>Wir betrachten wieder unser triviales Netz von oben</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
g(x, w) = r(w_1 \, x + w_2)\,w_3 + w_4,
\quad
r(x) = \max(0, x),
\end{equation*}\]</div>
<p>mit Trainingsdatensatz</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
x_i = y_i = \frac{i}{n}, \quad i = 0,\ldots,n, \quad n=10.
\end{equation*}\]</div>
<p>Da <span class="math notranslate nohighlight">\(r\)</span> bei <span class="math notranslate nohighlight">\(0\)</span> nicht differenzierbar ist, ist eine direkte Anwendung des Gradientenverfahrens zunächst nicht möglich.</p>
<p>Man kann dies durch zwei Strategien reparieren:</p>
<ul class="simple">
<li><p>man ersetzt <span class="math notranslate nohighlight">\(r\)</span> durch eine differenzierbare Approximation <span class="math notranslate nohighlight">\(\tilde{r}\)</span></p></li>
<li><p>man benutzt statt dem Gradienten den Subgradienten</p></li>
</ul>
<p>Wir benutzen die zweite Variante. Als Subgradient von <span class="math notranslate nohighlight">\(r\)</span> erhalten wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\partial r(x)=
\begin{cases}
0 &amp; x &lt; 0\\
[0,1] &amp; x = 0\\
1 &amp; 0&lt; x
\end{cases},
\end{equation*}\]</div>
<p>d.h. bei <span class="math notranslate nohighlight">\(x=0\)</span> müssen wir uns für einen Wert in <span class="math notranslate nohighlight">\([0,1]\)</span> entscheiden.
Wie wir später sehen werden, haben wir hier „freie Auswahl“.
Der Einfachheit halber benutzen wir den Wert <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">autograd.numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">grad</span>

<span class="n">relu</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="n">relu1</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">relu</span><span class="p">)</span>
<span class="n">relu1</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">relu1</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span> <span class="n">relu1</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.0, 0.5, 1.0)
</pre></div>
</div>
</div>
</div>
<p>Für verschieden Anfangswerte erhalten wir</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span><span class="n">w</span> <span class="p">:</span> <span class="n">relu</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">l</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="n">fw</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">):</span>
        <span class="n">fw</span> <span class="o">+=</span> <span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">fw</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">Xtrain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">l1</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">GD</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="n">nit</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">ww</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nit</span><span class="p">):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">l1</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">ww</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ww</span>

<span class="k">def</span> <span class="nf">ev</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="s1">&#39;b.&#39;</span><span class="p">);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xplot</span><span class="p">,</span> <span class="n">g</span><span class="p">(</span><span class="n">Xplot</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">c</span><span class="p">);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">w</span><span class="p">)),</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
    <span class="c1">#plt.ylabel(&#39;$l$&#39;,rotation=0)</span>

<span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">ev</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06_Neuronale_Netze_33_0.png" src="_images/06_Neuronale_Netze_33_0.png" />
<img alt="_images/06_Neuronale_Netze_33_1.png" src="_images/06_Neuronale_Netze_33_1.png" />
</div>
</div>
<p>bzw.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">ev</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06_Neuronale_Netze_35_0.png" src="_images/06_Neuronale_Netze_35_0.png" />
<img alt="_images/06_Neuronale_Netze_35_1.png" src="_images/06_Neuronale_Netze_35_1.png" />
</div>
</div>
</section>
<section id="accelerated-gradient-descent-nesterov">
<h2>Accelerated Gradient-Descent (Nesterov)<a class="headerlink" href="#accelerated-gradient-descent-nesterov" title="Link zu dieser Überschrift">#</a></h2>
<p>Durch eine einfache Modifikation kann man das (Sub)Gradientenverfahren
beschleunigen.
Man bestimmt die neue Suchrichtung als Kombination aus dem aktuellen
negativen Gradienten und der vorherigen Suchrichtung (ähnlich wie beim
CG-Verfahren).</p>
<p>Die bekannteste Variante stammt von <a class="reference external" href="https://uclouvain.be/fr/repertoires/yurii.nesterov">Nesterov</a>, der auch nachgewiesen hat, dass diese Verfahren in einem
gewissen Sinn optimal sind.
Die Iterationsvorschrift sieht wie folgt aus:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
 w^{(-1)} &amp;= w^{(0)} \text {gegeben}\\
 k = 1&amp;,2,...\\
     &amp; v^{(k)} = w^{(k-1)} + \frac{k-2}{k+1} \big( w^{(k-1)} - w^{(k-2)} \big) \\
     &amp; w^{(k)} = v^{(k)} - \alpha^{(k)} l'(v^{(k)})
\end{align*}\]</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">Nes</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">maxit</span> <span class="o">=</span> <span class="mi">30</span><span class="p">):</span>
    <span class="c1"># Variante von Tibshirani</span>
    <span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">w0</span><span class="p">,</span> <span class="n">w0</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">maxit</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">vk</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">wk</span> <span class="o">=</span> <span class="n">vk</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">l1</span><span class="p">(</span><span class="n">vk</span><span class="p">)</span>

        <span class="n">w</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wk</span><span class="p">)</span>

    <span class="k">return</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

<span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">wnes</span> <span class="o">=</span> <span class="n">Nes</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">ev</span><span class="p">(</span><span class="n">wnes</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Nesterov&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">w</span><span class="p">)),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SubGD&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06_Neuronale_Netze_38_0.png" src="_images/06_Neuronale_Netze_38_0.png" />
<img alt="_images/06_Neuronale_Netze_38_1.png" src="_images/06_Neuronale_Netze_38_1.png" />
</div>
</div>
</section>
<section id="stochastic-sub-gradient-descent">
<h2>Stochastic (Sub)Gradient-Descent<a class="headerlink" href="#stochastic-sub-gradient-descent" title="Link zu dieser Überschrift">#</a></h2>
<p>Wir betrachten noch einmal unsere Zielfunktion <span class="math notranslate nohighlight">\(l\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} 
l(w) 
&amp;= \frac{1}{n} \sum_{i=1}^n \big(g(x_i, w) -y_i\big)^2
= \frac{1}{n} \sum_{i=1}^n l_i(w),
\\
l_i(w) &amp;= \big(g(x_i, w) -y_i\big)^2.
\end{align*}\]</div>
<p>In jedem Schritt des (Sub)Gradienten-Verfahren muss</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\partial l(w) = \frac{1}{n} \sum_{i=1}^n \partial l_i(w)
\end{equation*}\]</div>
<p>berechnet werden, d.h. der Aufwand skaliert mit der Anzahl
<span class="math notranslate nohighlight">\(n\)</span> an Trainingsdaten, die zur Bestimmung der Parameter <span class="math notranslate nohighlight">\(w\)</span>
benutzt werden.</p>
<p>Andererseits ist <span class="math notranslate nohighlight">\(\partial l(w)\)</span> offensichtlich der Mittelwert der einzelnen <span class="math notranslate nohighlight">\(\partial l_i(w)\)</span>, so dass es naheliegend ist, diesen Mittelwert durch eine weniger aufwendige Approximation zu nähern, z.B.:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\partial l(w) \approx \partial l_{\hat{i}}(w)\)</span> für <em>ein</em> <span class="math notranslate nohighlight">\(\hat{i}\in \{1,\ldots,n\}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\partial l(w) \approx \frac{1}{n_B} \sum_{i\in B} \partial l_i(w)\)</span> für eine <span class="math notranslate nohighlight">\(n_B\)</span>-elementige Teilmenge <span class="math notranslate nohighlight">\(B \subset \{1,\ldots,n\}\)</span> mit <span class="math notranslate nohighlight">\(n_B \le n\)</span></p></li>
</ul>
<p>Den Index <span class="math notranslate nohighlight">\(\hat{i}\)</span> bzw. die Teilmenge <span class="math notranslate nohighlight">\(B\)</span> wird in jedem Schritt des Gradienten-Verfahrens zufällig neu bestimmt. Das resultierende Verfahren heißt  Stochastic Gradient-Descent- bzw.
Minibatch Stochastic Gradient-Descent-Verfahren.</p>
<p>Angewandt auf unser Modellproblem erhalten wir mit <span class="math notranslate nohighlight">\(n_B = 1\)</span></p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">li</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="p">:</span> <span class="p">((</span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">li1</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">li</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">SGD</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">li1</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="n">nit</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">bs</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">ww</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nit</span><span class="p">):</span>
        <span class="n">g</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])[:</span><span class="n">bs</span><span class="p">]:</span>
            <span class="n">g</span> <span class="o">+=</span> <span class="n">li1</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">g</span> <span class="o">/=</span> <span class="n">bs</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">g</span>
        <span class="n">ww</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ww</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">li1</span><span class="p">,</span> <span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">ev</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06_Neuronale_Netze_44_0.png" src="_images/06_Neuronale_Netze_44_0.png" />
<img alt="_images/06_Neuronale_Netze_44_1.png" src="_images/06_Neuronale_Netze_44_1.png" />
</div>
</div>
<p>Der Abfall der Loss-Funktion ist ähnlich schnell wie beim Standard-Gradienten-Verfahren, aber nicht monoton („Rauschen“)</p>
<p>Bei <span class="math notranslate nohighlight">\(n\)</span> Training-Samples <span class="math notranslate nohighlight">\(x_i,y_i\)</span> ist der Aufwand bei SGD pro Iteration
um einen Faktor <span class="math notranslate nohighlight">\(n\)</span> kleiner.</p>
<p>Für <span class="math notranslate nohighlight">\(n_B = 3\)</span> folgt</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">wb</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">li1</span><span class="p">,</span> <span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="n">bs</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ev</span><span class="p">(</span><span class="n">wb</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06_Neuronale_Netze_47_0.png" src="_images/06_Neuronale_Netze_47_0.png" />
<img alt="_images/06_Neuronale_Netze_47_1.png" src="_images/06_Neuronale_Netze_47_1.png" />
</div>
</div>
<p>Hier ist der Verlauf der Abfall der Loss-Werte etwas weniger „zitterig“ als im Fall <span class="math notranslate nohighlight">\(n_B=1\)</span>, allerdings ist der Aufwand pro Iteration auch wesentlich höher.</p>
<p>Analog kann man auch für das Nesterov-Verfahren eine stochastische Variante
aufbauen.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">SNes</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">li1</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">maxit</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span> <span class="n">bs</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="c1"># Variante von Tibshirani</span>
    <span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">w0</span><span class="p">,</span> <span class="n">w0</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">maxit</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">vk</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>

        <span class="n">gk</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])[:</span><span class="n">bs</span><span class="p">]:</span>
            <span class="n">gk</span> <span class="o">+=</span> <span class="n">li1</span><span class="p">(</span><span class="n">vk</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        
        <span class="n">wk</span> <span class="o">=</span> <span class="n">vk</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">gk</span>

        <span class="n">w</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wk</span><span class="p">)</span>

    <span class="k">return</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">wnes</span> <span class="o">=</span> <span class="n">SNes</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">li1</span><span class="p">,</span> <span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">ev</span><span class="p">(</span><span class="n">wnes</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Nesterov&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">w</span><span class="p">)),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SubGD&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06_Neuronale_Netze_50_0.png" src="_images/06_Neuronale_Netze_50_0.png" />
<img alt="_images/06_Neuronale_Netze_50_1.png" src="_images/06_Neuronale_Netze_50_1.png" />
</div>
</div>
</section>
<section id="backpropagation">
<h2>Backpropagation<a class="headerlink" href="#backpropagation" title="Link zu dieser Überschrift">#</a></h2>
<p>Zuletzt muss noch überlegt werden, wie die (Sub)Gradienten</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\partial l_i(w),
\quad 
l_i(w) = \big(g(x_i, w) -y_i\big)^2
\end{equation*}\]</div>
<p>möglichst effizient berechnet werden können.
Dadurch dass beim MLP die Parameter <span class="math notranslate nohighlight">\(w\)</span> sehr komplex
in <span class="math notranslate nohighlight">\(g\)</span> eingehen, ist dies nicht trivial.</p>
<p>Wir betrachten zunächst den trivialen Fall eines einzelnen skalaren Neurons.
Zur Vereinfachung der Notation wird der Index <span class="math notranslate nohighlight">\(i\)</span> weg gelassen.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
x \rightarrow w_1 x =\colon i_1  \rightarrow a(i_1) =\colon o_1 
\end{equation*}\]</div>
<p>mit differenzierbarem Loss <span class="math notranslate nohighlight">\(l\)</span>.
Für den Gradienten von <span class="math notranslate nohighlight">\(l\)</span> nach <span class="math notranslate nohighlight">\(w_1\)</span> erhalten wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\partial_{w_1} l(o_1)
= l'(o_1)\partial_{w_1} o_1
= l'(o_1)a'(i_1)\partial_{w_1} i_1
= l'(o_1)a'(i_1)  x
\end{equation*}\]</div>
<p>Hat man <span class="math notranslate nohighlight">\(o_1\)</span> berechnet, so kennt man auch <span class="math notranslate nohighlight">\(i_1\)</span> und <span class="math notranslate nohighlight">\(\partial_{w_1} l(o_1)\)</span> ist direkt bestimmbar.</p>
<p>Betrachten wir nun die analoge Konstellation für zwei Neuronen</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
x 
\rightarrow w_1 x =\colon i_1  \rightarrow a(i_1) =\colon o_1 
\rightarrow w_2 o_1 =\colon i_2  \rightarrow a(i_2) =\colon o_2. 
\end{equation*}\]</div>
<p>Für die Ableitung von <span class="math notranslate nohighlight">\(l(o_2)\)</span> nach <span class="math notranslate nohighlight">\(w_k\)</span> erhalten wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\partial_{w_k} l(o_2) 
= l'(o_2)\partial_{w_k} o_2 
= l'(o_2)a'(i_2)\partial_{w_k} i_2
= l'(o_2)a'(i_2)\partial_{w_k} (w_2 o_1) 
\end{equation*}\]</div>
<p>Für <span class="math notranslate nohighlight">\(w_2\)</span> gilt dann</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\partial_{w_2} l(o_2)
= l'(o_2)a'(i_2)\partial_{w_2} (w_2 o_1) 
= l'(o_2)a'(i_2) \big(o_1 + w_2 \partial_{w_2} o_1 \big)
\end{equation*}\]</div>
<p>und da <span class="math notranslate nohighlight">\(o_1\)</span> nicht von <span class="math notranslate nohighlight">\(w_2\)</span> abhängt folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\partial_{w_2} l(o_2) = l'(o_2)a'(i_2)  o_1,
\end{equation*}\]</div>
<p>d.h. <span class="math notranslate nohighlight">\(\partial_{w_2} l(o_2)\)</span> kann einfach bestimmt werden.</p>
<p>Für  <span class="math notranslate nohighlight">\(\partial_{w_1} l(o_2)\)</span> erhalten wir</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\partial_{w_1} l(o_2)
= l'(o_2)a'(i_2)\partial_{w_1} (w_2 o_1) 
= l'(o_2)a'(i_2)  w_2 \partial_{w_1} o_1
\end{equation*}\]</div>
<p>und mit <span class="math notranslate nohighlight">\(\partial_{w_1} o_1 = a'(i_1)\partial_{w_1} i_1 = a'(i_1) x\)</span> folgt</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\partial_{w_1} l(o_2) = l'(o_2)a'(i_2)  w_2 a'(i_1) x
\end{equation*}\]</div>
<p>Analog kann man auch bei komplexeren Netzen beginnend von der Output-Seite hin zur Input-Seite Schritt für Schritt die Ableitungen nach von <span class="math notranslate nohighlight">\(l_i\)</span> nach den Parametern der jeweiligen
Schicht generieren. Deshalb heißt dieser Zugang <em>Backpropagation</em>.</p>
</section>
<section id="zusammenfassung">
<h2>Zusammenfassung<a class="headerlink" href="#zusammenfassung" title="Link zu dieser Überschrift">#</a></h2>
<p>Die Parameteranpassung bei neuronalen Netzen ist schwierig, da
die Zielfunktion oft nicht differenzierbar (z.B. RELU Aktivierung <span class="math notranslate nohighlight">\(a(x)=\max(0,x)\)</span>)
bzw. nicht konvex ist, so dass die Ergebnisse von gradientenartigen Verfahren sehr stark von der Wahl des Anfangswertes abhängen (Nebenminima).</p>
<p>Besonders populär sind stochastische Gradienten-Verfahren, die auch bei großen Trainings-Datensätzen sehr effizient sind. Die benötigten Ableitungen werden dabei in der Regel mit
Backpropagation berechnet.</p>
<p>Man beachte den Unterschied zwischen SGD und Coordinate-Descent. Mit beiden Verfahren minimiert man die Zielfunktion</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
l(w) = \frac{1}{n} \sum_{i=1}^n l_i(w),
\quad
l_i(w) = \big(g(x_i, w) -y_i\big)^2
\end{equation*}\]</div>
<p>durch approximative Gradienten-Updates</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
w^{(k+1)} = w^{(k)} - \alpha^{(k)} g^{(k)}
\end{equation*}\]</div>
<p>mit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
g_{SGD}^{(k)} = \partial_w l_{\hat{i}}(w)
\end{equation*}\]</div>
<p>bzw.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
g_{CD}^{(k)} = \partial_{w_{\hat{i}}} l(w).
\end{equation*}\]</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="05_Klassifikation_mit_SVM.html" title="zurück Seite">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">zurück</p>
            <p class="prev-next-title">Support-Vector Klassifikation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="07_Topic_Extraction.html" title="weiter Seite">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">weiter</p>
        <p class="prev-next-title">Topic Extraction, NMF</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Martin Reißel<br/>
  
      &copy; Copyright 2021.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>